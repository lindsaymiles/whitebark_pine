{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "from __future__ import division\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1) randomly assign samps to pop\n",
    "#2) create SNPSFILE\n",
    "#3) creat qsub files for each SNPSFILE\n",
    "#4) create MATRIXFILE by qsubbing each SNPSFILE\n",
    "#5) pull ENVIRONFILE from BayEnv2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home_dir  = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputedData = pd.read_csv('/home/lindb/eckertlab/wbp/bayenv2/imputedDataHEADERIDX.txt',header=0,index_col=0,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputedData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stpDF = pd.read_csv('/home/lindb/eckertlab/wbp/hierfstat/sampsTOpop.txt',header=0,sep=\"\\t\")\n",
    "stpDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stpnew = stpDF[stpDF['samp'].isin(imputedData.index)]\n",
    "stpnew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(stpnew.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pops = np.unique(stpnew['pop']).tolist()\n",
    "pops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popDict = OrderedDict()\n",
    "for pop in pops:\n",
    "    popDict[pop] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in stpnew.index:\n",
    "    POP = stpDF.loc[row,'pop']\n",
    "    popDict[POP] += 1\n",
    "for pop in popDict.keys():\n",
    "    print pop,popDict[pop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for pop in popDict.keys():\n",
    "    sum = sum + popDict[pop]\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(stpnew.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time pd.read_csv('/home/lindb/eckertlab/wbp/bayenv2/imputedDataHEADERIDX.txt',header=0,index_col=0,sep=\"\\\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(100): #make 100 runfiles to make SNPSFILES\n",
    "    text = '''print \"starting\"\n",
    "import sys\n",
    "import os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "import random\n",
    "\n",
    "print \"importing data\"\n",
    "imputedData = pd.read_csv('/home/lindb/eckertlab/wbp/bayenv2/imputedDataHEADERIDX.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "print \"done importing\"\n",
    "\n",
    "stpDF = pd.read_csv('/home/lindb/eckertlab/wbp/hierfstat/sampsTOpop.txt',header=0,sep=\"\\\\t\")\n",
    "\n",
    "stpnew = stpDF[stpDF['samp'].isin(imputedData.index)]\n",
    "\n",
    "pops = np.unique(stpnew['pop']).tolist()\n",
    "\n",
    "popDict = OrderedDict()\n",
    "for pop in pops:\n",
    "    popDict[pop] = 0\n",
    "\n",
    "for row in stpnew.index:\n",
    "    POP = stpDF.loc[row,'pop']\n",
    "    popDict[POP] += 1\n",
    "\n",
    "\n",
    "print \"creating idx\"\n",
    "idx = range(len(imputedData.index))\n",
    "sampDict = OrderedDict()\n",
    "print \"assigning random samps to pops\"\n",
    "for pop in sorted(popDict.keys()): #randomly assign samples to pops\n",
    "    which = random.sample(idx,popDict[pop])\n",
    "    samps = itemgetter(*which)(imputedData.index.tolist())\n",
    "    sampDict[pop] = imputedData[imputedData.index.isin(samps)]\n",
    "    idx = [x for x in idx if x not in which]\n",
    "\n",
    "locCount = 0\n",
    "nullDict = OrderedDict()\n",
    "print \"iterating loci\"\n",
    "for locus in imputedData.columns:\n",
    "    nullDict[locus] = OrderedDict()\n",
    "\n",
    "    for pop in sorted(popDict.keys()):\n",
    "        data = sampDict[pop]\n",
    "        zero = str(data[locus]).count('0') #count the first homozygotes\n",
    "        one = str(data[locus]).count('1') #count the heterozygotes\n",
    "        two = str(data[locus]).count('2') #count the second homozygotes  \n",
    "        A1 = 2*zero + one\n",
    "        A2 = 2*two + one\n",
    "        if len(nullDict[locus].keys()) == 0:\n",
    "            nullDict[locus]['A1'] = OrderedDict()\n",
    "            nullDict[locus]['A2'] = OrderedDict()\n",
    "        nullDict[locus]['A1'][pop] = A1\n",
    "        nullDict[locus]['A2'][pop] = A2    \n",
    "    locCount += 1\n",
    "    if locCount %% 1000 == 0:\n",
    "        print locCount\n",
    "print \"writing file, iteration %d\"\n",
    "filE = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/UnbinnedImputedSNPSFILE%d_HEADERROW.txt' \n",
    "with open(filE, 'w') as o:\n",
    "    line = '\\\\t'.join([str(pop) for pop in sorted(popDict.keys())]) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locus in nullDict.keys():\n",
    "        for allele in nullDict[locus].keys():\n",
    "            line = str(locus) + '\\\\t' + '\\\\t'.join([str(x) for x in nullDict[locus][allele].values()]) + str('\\\\n')\n",
    "            #print locus,allele,line\n",
    "            o.write(\"%%s\" %% line)\n",
    "o.close()\n",
    "''' % (i,i)\n",
    "    FILE = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/imp_run%d.py' % i\n",
    "    with open(FILE,'w') as o:\n",
    "        o.write(text)\n",
    "    o.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for root,dirs,files in os.walk('/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/'):\n",
    "    for f in files:\n",
    "        runname = os.path.join(root,f).split(\".\")[0].split(\"/\")[-1]\n",
    "        text = '''#!/bin/bash\n",
    "#$ -N %s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "python %s\n",
    "''' % (runname,os.path.join(root,f))\n",
    "        filE = os.path.join(root,runname + '.sh')\n",
    "        print filE\n",
    "        with open(filE,'w') as o:\n",
    "            o.write('%s' % text)\n",
    "        o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1) Get list of SNPSFILEs and create a directory using that name\n",
    "#2) get SNPSFILE with header/idx, save as SNPSFILE without headerIDX in new directory\n",
    "#3) copy ./bayenv2, ENVIRONFILE to each new folder\n",
    "#4) create qsub files to create MATRIXFILE\n",
    "#5) split the SNPSFILE\n",
    "#6) run bayenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_dir = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(file_dir)\n",
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for f in files:\n",
    "        if f.endswith('txt'):\n",
    "            print f[23:].split(\"_\")[0]\n",
    "            DIR = f[23:].split(\"_\")[0]\n",
    "            if not os.path.exists(os.path.join(root,DIR)):\n",
    "                os.makedirs(os.path.join(root,'dir'+DIR))\n",
    "                shutil.copy(os.path.join(root,f),os.path.join(root,'dir'+DIR))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for d in dirs:\n",
    "        for ROOT,DIRS,FILES in os.walk(os.path.join(root,d)):\n",
    "            count = 0\n",
    "            for F in FILES:\n",
    "                filE = os.path.join(ROOT,F)\n",
    "                print filE\n",
    "                DF = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "                split = F.split(\"_\")[0]+'_noHEADERIDX.txt'\n",
    "                filE2 = os.path.join(ROOT,split)\n",
    "                print filE2\n",
    "                DF.to_csv(filE2,header=False,index=False,sep=\"\\t\")\n",
    "            break #keep this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    DIR = 'home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/dir%s/' % str(i)\n",
    "    if os.path.exists('/' + DIR):\n",
    "        print DIR\n",
    "        upper = '/'+\"/\".join(DIR.split(\"/\")[:7])\n",
    "        print upper\n",
    "        src = os.path.join(upper,'UnbinnedImputedSNPSFILE%s_noHEADERIDX.txt' % str(i))\n",
    "        print src\n",
    "        dst = '/' + DIR\n",
    "        shutil.copy(src,dst)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EF = '/home/lindb/eckertlab/wbp/bayenv2/ENVIRONFILE.txt'\n",
    "bayenv = '/home/lindb/eckertlab/wbp/bayenv2/bayenv2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for d in sorted(dirs):\n",
    "        print os.path.join(root,d)\n",
    "        shutil.copy(bayenv,os.path.join(root,d))\n",
    "        shutil.copy(EF,os.path.join(root,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del_dir = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for d in sorted(dirs):\n",
    "        if 'SNP' not in d:\n",
    "            src = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/delete2/%s/matrix_%s.out' % (d,d)\n",
    "            dst = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/%s/matrix_%s.out' % (d,d)\n",
    "            shutil.copy(src,dst)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for d in sorted(dirs):\n",
    "        print d\n",
    "        text = '''\n",
    "#!/bin/bash\n",
    "#$ -N %s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cd %s\n",
    "./bayenv2 -i UnbinnedImputedSNPSFILE%s_noHEADERIDX.txt -p 8 -k 100000 -r %s > matrix_%s.out\n",
    "''' % ('imp'+str(d)[3:],\n",
    "       os.path.join(root,d)+'/',\n",
    "       str(d)[3:],\n",
    "       str(random.sample(range(100000),1)[0]),\n",
    "       str(d))\n",
    "        filE = os.path.join(root,str(d)+'/'+'matrix_'+str(d)+'.sh')\n",
    "        print filE\n",
    "        #os.remove(filE)\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(text)\n",
    "        o.close\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#run bayenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_dir = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make the /SNPS folder & copy all files into it\n",
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for d in sorted(dirs):\n",
    "        if 'SNPs' not in d:\n",
    "            print d\n",
    "            new = os.path.join(root,d)+'/SNPs2'\n",
    "            Files = os.listdir(os.path.join(root,d))\n",
    "            if not os.path.exists(new):\n",
    "                os.makedirs(new)\n",
    "            for f in Files:\n",
    "                if f != 'SNPs':\n",
    "                    src =  os.path.join(root,d+'/%s' %f)\n",
    "                    dst = new+'/%s' % f\n",
    "                    #print src,'\\n',dst\n",
    "                    shutil.copy(src,dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for d in sorted(dirs):\n",
    "        if 'SNPs' not in d:\n",
    "            num = d[3:]\n",
    "            text = '''\n",
    "#!/bin/bash\n",
    "#$ -N %s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cd %s\n",
    "split -d -a 10 -l 2 UnbinnedImputedSNPSFILE%s_noHEADERIDX.txt ./SNPs/snp_batch\n",
    "''' % ('imp'+str(num),\n",
    "       os.path.join(root,d),\n",
    "       str(d)[3:])\n",
    "            filE = os.path.join(root,d) + '/copySNPs%s.sh' % str(num)\n",
    "            with open(filE,'w') as o:\n",
    "                o.write(text)\n",
    "            o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make SNPFILEs out of the SNPSFILE\n",
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for d in sorted(dirs):\n",
    "        if 'SNPs' not in d:\n",
    "            print os.path.join(root,d)\n",
    "            num = d[3:]\n",
    "            cmd = 'split -d -a 10 -l 2 UnbinnedmissingSNPSFILE%s_noHEADERIDX.txt ./SNPs/snp_batch' % str(num)\n",
    "            os.chdir(os.path.join(root,d))\n",
    "            !$cmd\n",
    "        #break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make usable matrices\n",
    "for i in range(100):\n",
    "    DIR = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/dir%s/SNPs2' % str(i)\n",
    "    if os.path.exists(DIR):\n",
    "        files = os.listdir(DIR)\n",
    "        for f in files:\n",
    "            if (f.startswith('matrix')) and (f.endswith('out')):\n",
    "                #print os.path.join(DIR,f)\n",
    "                F = open(os.path.join(DIR,f),'r')\n",
    "                g = F.readlines()\n",
    "                new = f[:-4] + '.txt'\n",
    "                filE = DIR + '/usable_%s' % new\n",
    "                print filE\n",
    "                with open(filE,'w') as o:\n",
    "                    for line in g[2005:][:-1]:\n",
    "                        o.write(line)\n",
    "                o.close()\n",
    "                break\n",
    "        #break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make sure each SNPs folder has 159803 snpfiles\n",
    "for i in range(100):\n",
    "    DIR = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/dir%s/SNPs' % str(i)\n",
    "    if os.path.exists(DIR):\n",
    "        files = os.listdir(DIR)\n",
    "        count = len([f for f in files if f.startswith('snp')])\n",
    "        if count != 159803:\n",
    "            print DIR, count\n",
    "        else:\n",
    "            print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len([5,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/dir77/SNPs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#don't use this\n",
    "#make bayenv2 qsub files\n",
    "for i in range(100):\n",
    "    DIR = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/dir%s/SNPs2' % str(i)\n",
    "    if os.path.exists(DIR):\n",
    "        print \"DIR =\",DIR\n",
    "        text = '''\n",
    "    #!/bin/bash\n",
    "    #$ -N %s \n",
    "    #$ -V\n",
    "    #$ -j y\n",
    "    #$ -cwd\n",
    "\n",
    "    cd %s\n",
    "    find -type f -name 'snp_batch*' | time parallel -j +0 --progress './bayenv2 -i {} \\\n",
    "    -e ENVIRONFILE.txt -m usable_matrix_dir%s.txt -k 100000 -r %s -p 8 -n 16 -t -f -c -X'\n",
    "    ''' % ('imp'+str(i), #-N %s\n",
    "           DIR,          #cd %s\n",
    "           str(i),       #...dir%s\n",
    "           str(random.sample(range(100000),1)[0])) #-r %s\n",
    "        filE = DIR + '/RUNFILEnull_%s.sh' % str(i) \n",
    "        print filE\n",
    "        #with open(filE,'w') as o:\n",
    "            #o.write(text)\n",
    "        #o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#spreading out the jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpus = defaultdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cpus = {2:24,3:24,4:24,5:24,6:56,7:24,8:24,9:24,10:24,11:56,\n",
    "        12:24,13:24,14:24,16:24,17:24,18:24,19:32,20:32,\n",
    "        22:32,23:32,24:56,25:32,26:32,27:32,28:32,29:32,\n",
    "       30:32,31:32,32:32,34:24,35:24,36:24,96:48}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cpus.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(cpus.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x for x in cpus.values() if x > 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speed = defaultdict()\n",
    "for cpu in cpus.keys():\n",
    "    speed[cpu] = cpus[cpu]/1008\n",
    "len(speed.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(speed.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speed[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "speed[24]*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jobs = defaultdict()\n",
    "for cpu in speed.keys():\n",
    "    JOBS = speed[cpu]*200\n",
    "    if JOBS >10:\n",
    "        jobs[cpu] = math.ceil(speed[cpu]*200) + 4\n",
    "    else:\n",
    "        jobs[cpu] = math.floor(speed[cpu]*200)\n",
    "    print cpu,jobs[cpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(jobs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(jobs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sums = 0\n",
    "missing = False\n",
    "for cpu in jobs.keys():                       #for each node\n",
    "    old = '''#!/bin/bash \n",
    "#$ -N %s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "''' % ('godel'+str(cpu))\n",
    "    for i in range(int(jobs[cpu])):             #add x jobs to the node\n",
    "        if (sums < 100) and (missing == False): #imputed jobs\n",
    "            print 'imputed',sums\n",
    "            DIR = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/missing/dir%s/SNPs' % str(sums)\n",
    "            text = '''\n",
    "cd %s\n",
    "find -type f -name 'snp_batch*' | find -type f -not -name '*freq*' | time parallel -j +0 --progress './bayenv2 -i {} \\\n",
    "-e ENVIRONFILE.txt -m usable_matrix_dir%s.txt -k 100000 -r %s -p 8 -n 16 -t -f -c -X'\n",
    "\n",
    "''' % (DIR,\n",
    "      str(sums),\n",
    "      str(random.sample(range(100000),1)[0]))\n",
    "            old = old + text\n",
    "            \n",
    "        else:                                   #missing jobs\n",
    "            if (sums == 100) and (missing == False): #reset for the dir%s % sums\n",
    "                sums = 0\n",
    "                missing = True\n",
    "            if (sums == 100) and (missing == True):\n",
    "                break\n",
    "            print 'imputed',sums\n",
    "            DIR = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/dir%s/SNPs' % str(sums)\n",
    "            text = '''\n",
    "cd %s\n",
    "find -type f -name 'snp_batch*' | find -type f -not -name '*freq*' | time parallel -j +0 --progress './bayenv2 -i {} \\\n",
    "-e ENVIRONFILE.txt -m usable_matrix_dir%s.txt -k 100000 -r %s -p 8 -n 16 -t -f -c -X'\n",
    "\n",
    "''' % (DIR,\n",
    "      str(sums),\n",
    "      str(random.sample(range(100000),1)[0]))\n",
    "            old = old + text\n",
    "            \n",
    "        sums += 1 \n",
    "    filE = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/runfiles/godel%s.sh' % cpu\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(old)\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove the nasty files\n",
    "for i in range(100):\n",
    "    DIR = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/dir%s/SNPs2/' % i\n",
    "    if os.path.exists(DIR):\n",
    "        files = os.listdir(DIR)\n",
    "        for f in files:\n",
    "            if f.startswith('stand'):\n",
    "                os.remove(str(DIR + f))\n",
    "            if f.startswith('Xt'):\n",
    "                os.remove(str(DIR + f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in range(100):\n",
    "    DIR = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/dir%s/SNPs2' % i \n",
    "    if os.path.exists(DIR):\n",
    "        files = os.listdir(DIR)\n",
    "        if len([x for x in files if x.startswith('snp')]) != 159803:\n",
    "            print DIR, len([x for x in files if x.startswith('snp')])\n",
    "            lst.append(DIR)\n",
    "        else:\n",
    "            print len([x for x in files if x.startswith('snp')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for DIR in lst:\n",
    "    upper = DIR[:-5]\n",
    "    splits = DIR.split(\"/\")\n",
    "    num = splits[-2][3:]\n",
    "    cpfile = 'copySNPs%s.sh' % str(num)\n",
    "    full = str(DIR[:-5] + cpfile)\n",
    "    cmd = 'qsub %s' % (full)\n",
    "    !$cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for DIR in lst:\n",
    "    upper = DIR[:-5]\n",
    "    splits = DIR.split(\"/\")\n",
    "    num = splits[-2][3:]\n",
    "    if os.path.exists(DIR):\n",
    "        print 'moving DIR'\n",
    "        src = DIR\n",
    "        dst = DIR[:-5] + 'delete'\n",
    "        shutil.move(src,dst)\n",
    "    files = os.listdir(upper)\n",
    "    if not os.path.exists(DIR):\n",
    "        print 'making DIR'\n",
    "        os.makedirs(DIR)\n",
    "    for f in files:\n",
    "        if ('delete' not in f) and ('SNPs2' not in f):\n",
    "            src = str(upper + f)\n",
    "            dst = str(DIR + '/' + f)\n",
    "            'copying files'\n",
    "            shutil.copy(src,dst)\n",
    "    cpfile = 'copySNPs%s.sh' % str(num)\n",
    "    full = str(DIR[:-5] + cpfile)\n",
    "    cmd = 'qsub %s' % (full)\n",
    "    !$cmd\n",
    "    print cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for DIR in lst:\n",
    "    if os.path.exists(DIR):\n",
    "        files = os.listdir(DIR)\n",
    "        for f in files:\n",
    "            if (f.startswith('matrix')) and (f.endswith('out')):\n",
    "                #print os.path.join(DIR,f)\n",
    "                F = open(os.path.join(DIR,f),'r')\n",
    "                g = F.readlines()\n",
    "                new = f[:-4] + '.txt'\n",
    "                filE = DIR + '/usable_%s' % new\n",
    "                print filE\n",
    "                with open(filE,'w') as o:\n",
    "                    for line in g[2005:][:-1]:\n",
    "                        o.write(line)\n",
    "                o.close()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make usable matrices\n",
    "for i in range(100):\n",
    "    DIR = '/home/lindb/eckertlab/wbp/bayenv2/nullruns/imputed/dir%s/SNPs2' % str(i)\n",
    "    if os.path.exists(DIR):\n",
    "        files = os.listdir(DIR)\n",
    "        for f in files:\n",
    "            if (f.startswith('matrix')) and (f.endswith('out')):\n",
    "                #print os.path.join(DIR,f)\n",
    "                F = open(os.path.join(DIR,f),'r')\n",
    "                g = F.readlines()\n",
    "                new = f[:-4] + '.txt'\n",
    "                filE = DIR + '/usable_%s' % new\n",
    "                print filE\n",
    "                with open(filE,'w') as o:\n",
    "                    for line in g[2005:][:-1]:\n",
    "                        o.write(line)\n",
    "                o.close()\n",
    "                break\n",
    "        #break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#reinventing the wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/bayenv2/Final/imputed/'\n",
    "files = os.listdir(DIR)\n",
    "files = [f for f in files if not f.startswith('imputed')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get list of snps to do null runs\n",
    "snpLST = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(os.path.join(DIR,f),header=0,sep=\"\\t\")\n",
    "    env = f.split(\"_\")[0]\n",
    "    loci = df[env].tolist()\n",
    "    for locus in loci:\n",
    "        if locus not in snpLST:\n",
    "            snpLST.append(locus)\n",
    "len(snpLST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/bayenv2/nullruns/imputed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    DIR = '/home/lindb/wbp/bayenv2/nullruns/imputed/dir%s' % str(i)\n",
    "    if os.path.exists(DIR):\n",
    "        print DIR\n",
    "        f = os.path.join(DIR,'UnbinnedImputedSNPSFILE%s_HEADERROW.txt' % str(i))\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        snpDIR = os.path.join(DIR,'SNPdir')\n",
    "        if not os.path.exists(snpDIR):\n",
    "            os.makedirs(snpDIR)\n",
    "        for j in range(0,len(df.index),2):\n",
    "            if df.index[j] in snpLST:\n",
    "                DF = df.loc[df.index[j],:]\n",
    "                dfile = os.path.join(snpDIR,df.index[j])\n",
    "                DF.to_csv(dfile,header=None,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#because the above was taking too long\n",
    "for i in range(100):\n",
    "    DIR = '/home/lindb/wbp/bayenv2/nullruns/imputed/dir%s' % str(i)\n",
    "    if os.path.exists(DIR):\n",
    "        text ='''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "DIR = '/home/lindb/wbp/bayenv2/Final/imputed/'\n",
    "files = os.listdir(DIR)\n",
    "files = [f for f in files if not f.startswith('imputed')]\n",
    "\n",
    "snpLST = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(os.path.join(DIR,f),header=0,sep=\"\\\\t\")\n",
    "    env = f.split(\"_\")[0]\n",
    "    loci = df[env].tolist()\n",
    "    for locus in loci:\n",
    "        if locus not in snpLST:\n",
    "            snpLST.append(locus)\n",
    "\n",
    "f = os.path.join('%s','UnbinnedImputedSNPSFILE%s_HEADERROW.txt')\n",
    "df = pd.read_csv(f,header=0,index_col=0,sep=\"\\\\t\")\n",
    "snpDIR = os.path.join('%s','SNPdir')\n",
    "if not os.path.exists(snpDIR):\n",
    "    os.makedirs(snpDIR)\n",
    "for j in range(0,len(df.index),2):\n",
    "    if df.index[j] in snpLST:\n",
    "        DF = df.loc[df.index[j],:]\n",
    "        dfile = os.path.join(snpDIR,df.index[j])\n",
    "        DF.to_csv(dfile,header=None,index=False,sep=\"\\\\t\")\n",
    "''' % (DIR,str(i),DIR)\n",
    "        filE = '/home/lindb/wbp/bayenv2/nullruns/imputed/%s_run.py' % str(i).zfill(3)\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)\n",
    "        o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('/home/lindb/wbp/bayenv2/nullruns/imputed/')\n",
    "files = [os.path.join('/home/lindb/wbp/bayenv2/nullruns/imputed/',f) for f in files if f.endswith('py')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    fname = os.path.basename(f).split(\".\")[0]\n",
    "    print fname\n",
    "\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N imp%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cd /home/lindb/wbp/bayenv2/nullruns/imputed/\n",
    "python %s\n",
    "''' % (fname.split(\"_\")[0],\n",
    "      os.path.basename(f))\n",
    "    filE = os.path.join(os.path.dirname(f),fname+'.sh')\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copy all of the needed files\n",
    "for i in range(100):\n",
    "    DIR = '/home/lindb/wbp/bayenv2/nullruns/imputed/dir%s' % str(i)\n",
    "    if os.path.exists(DIR):\n",
    "        print DIR\n",
    "        files = os.listdir(DIR)\n",
    "        files = [os.path.join(DIR,f) for f in files if not f.startswith('SNP')]\n",
    "        for f in files:\n",
    "            fn = os.path.basename(f)\n",
    "            if fn.startswith('bayenv') or fn.startswith('ENVIRON') or fn.startswith('usable') or fn.endswith('noHEADERIDX.txt'):\n",
    "                shutil.copy(f,os.path.join(DIR,'SNPdir/%s' % os.path.basename(f)))\n",
    "                #print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#look at the BFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of null run dirs\n",
    "DIR = '/home/lindb/wbp/bayenv2/nullruns/imputed/'\n",
    "dirs = os.listdir(DIR)\n",
    "dirs = [os.path.join(DIR,d) for d in dirs if d.startswith('dir')]\n",
    "len(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the rest of the null runs\n",
    "edir = '/home/lindb/wbp/bayenv2/nullruns/imputed/extra/'\n",
    "DIRS = os.listdir(edir)\n",
    "DIRS = [os.path.join(edir,d) for d in DIRS if d.startswith('dir')]\n",
    "len(DIRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add them together\n",
    "dirs = dirs + DIRS\n",
    "len(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the ENVIRONFILE\n",
    "file_dir = '/home/lindb/eckertlab/wbp/bayenv2/'\n",
    "envdf = pd.read_csv(os.path.join(file_dir,'ENVIRONFILE.txt'),sep=\"\\t\",header=None)\n",
    "envdf = envdf.loc[:,0:7]\n",
    "envcol = pd.read_csv(os.path.join(file_dir,'ENVIRONFILEcolumns.txt'),sep=\"\\t\",header=None)\n",
    "envrow = pd.read_csv(os.path.join(file_dir,'ENVIRONFILErows.txt'),sep=\"\\t\",header=None)\n",
    "envdf.index = envrow[0].tolist()\n",
    "envdf.columns = envcol[0].values.tolist()\n",
    "envdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#assign environmental variables to a list\n",
    "envs = envdf.index\n",
    "envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#assign column value for each environmental variable, to be used to name columns in bf_ENVIRONFILE.txt\n",
    "envDict = OrderedDict()\n",
    "for i, env in enumerate(envdf.index):\n",
    "    print i,env\n",
    "    envDict[env] = [(i*3),(i*3)+1,(i*3)+2]\n",
    "envDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colDict = {}\n",
    "colDict[0] = 'BF'\n",
    "colDict[1] = 'rho'\n",
    "colDict[2] = 'pearson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rowDict = OrderedDict()\n",
    "for env,rows in envDict.items():\n",
    "    print env,rows\n",
    "    rowCount = 0\n",
    "    for row in rows:\n",
    "        sub = '_'.join([str(env), colDict[rowCount]]) \n",
    "        rowDict[row] = sub\n",
    "        rowCount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get all of the bf_ENVIRONFILEs as a dataframe within a dictionary\n",
    "bfDict = OrderedDict()\n",
    "for d in dirs:\n",
    "    num = d.split(\"/\")[-1][3:]\n",
    "    snpdir = os.path.join(d,'SNPdir')\n",
    "    bffile = os.path.join(snpdir,'bf_environ.ENVIRONFILE.txt')\n",
    "    bfdf = pd.read_csv(bffile,header=None,index_col=0,sep=\"\\t\")\n",
    "    bfdf.columns = [str(x) for x in range(len(bfdf.columns))] #rename columns to align with rowDict\n",
    "    bfdf = bfdf[bfdf.columns[0:48]] #get rid of extra columns\n",
    "    bfdf.columns = [rowDict[int(col)]+'_%s'%num for col in bfdf.columns]\n",
    "    bfDict[int(num)] = bfdf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(bfDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make dataframes for each env that contain all BFs across runs\n",
    "envbfDict = OrderedDict()\n",
    "for env in envs: # for each environmental variable\n",
    "    dfcount = 0\n",
    "    for run in bfDict.keys(): #for each bf_ENVIRONFILE\n",
    "        col = '%s_BF_%s' % (env,run)\n",
    "        df = pd.DataFrame(bfDict[run][col])\n",
    "        if dfcount == 0:\n",
    "            newdf = df\n",
    "        else:\n",
    "            newdf = pd.merge(newdf,df,left_index=True,right_index=True)\n",
    "        dfcount += 1\n",
    "        #break\n",
    "    filE = '/home/lindb/wbp/bayenv2/nullruns/imputed/bf_files/%s_all_BFs.txt' % env\n",
    "    newdf.to_csv(filE,header=True,index=True,sep=\"\\t\")\n",
    "    envbfDict[env] = newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(bfDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for when I have to restart the ipython notebook:\n",
    "envbfDict = OrderedDict()\n",
    "for env in envs:\n",
    "    filE = '/home/lindb/wbp/bayenv2/nullruns/imputed/bf_files/%s_all_BFs.txt' % env\n",
    "    df = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "    envbfDict[env] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0*np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), stats.sem(a)\n",
    "    h = se * stats.t.ppf((1+confidence)/2., n-1)\n",
    "    return m, m-h, m+h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get BFs from the empirical (true data) run of bayenv2\n",
    "empBFs = pd.read_csv('/home/lindb/wbp/bayenv2/Final/imputed/imputedBFs_by_SNP_IDXHEADER.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "empBFs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(envbfDict['elev'].columns) #100 null runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the empirically IDed snps from bayenv2 (BF >= 5)\n",
    "snpdir = '/home/lindb/wbp/bayenv2/Final/imputed/'\n",
    "snpfiles = os.listdir(snpdir)\n",
    "snpfiles = [os.path.join(snpdir,f) for f in snpfiles if not f.startswith('imp')]\n",
    "snpDict = OrderedDict()\n",
    "for f in snpfiles:\n",
    "    env = os.path.basename(f).split(\"_\")[0]\n",
    "    df = pd.read_csv(f,header=0,sep=\"\\t\")\n",
    "    snpDict[env] = df[env].tolist()\n",
    "    print env,len(snpDict[env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get dataframes with just the snps IDed from empirical bayenv2 runs\n",
    "newDict = OrderedDict()\n",
    "for env in envbfDict.keys():\n",
    "    df = envbfDict[env]\n",
    "    df.index = [str(x[2:]) for x in envbfDict[env].index]\n",
    "    newDict[env] = df[df.index.isin(snpDict[env])]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make sure they match up\n",
    "for env in snpDict.keys():\n",
    "    print env,len(snpDict[env]),len(newDict[env].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get median BF across all snps across all runs, as well as percentiles\n",
    "medbfDict = OrderedDict()\n",
    "for env in newDict.keys():\n",
    "    print env\n",
    "    vals = []\n",
    "    for row in newDict[env].index:\n",
    "        v = [x for x in newDict[env].loc[row,:]]\n",
    "        vals = vals + v\n",
    "    vals = sorted(vals)\n",
    "    medbfDict[env] = OrderedDict()\n",
    "    medbfDict[env]['median'] = np.median(vals)\n",
    "    p25 = len(vals)*.25\n",
    "    medbfDict[env]['25th'] = vals[int(p25)]\n",
    "    p50 = len(vals)*.5\n",
    "    medbfDict[env]['50th'] = vals[int(p50)]\n",
    "    p75 = len(vals)*.75\n",
    "    medbfDict[env]['75th'] = vals[int(p75)]\n",
    "    p90 = len(vals)*.9\n",
    "    medbfDict[env]['90th'] = vals[int(p90)]\n",
    "    p95 = len(vals)*.95\n",
    "    medbfDict[env]['95th'] = vals[int(p95)]\n",
    "    p99 = len(vals)*.99\n",
    "    medbfDict[env]['99th'] = vals[int(p99)]\n",
    "    g5 = len([x for x in vals if x <= 5])\n",
    "    p5 = g5/len(vals)\n",
    "    medbfDict[env]['perc BF >= 5'] = p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in envs:\n",
    "    filE = '/home/lindb/wbp/bayenv2/nullruns/imputed/bf_files/%s_median_BFs.txt' % env\n",
    "    with open(filE,'w') as o:\n",
    "        line = '\\t'.join(medbfDict['elev'].keys()) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "        for ENV in medbfDict.keys():\n",
    "            line = str(ENV)+'\\t'+'\\t'.join([str(x) for x in medbfDict[ENV].values()]) + str('\\n')\n",
    "            o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "medbfDict[env]['95th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(envbfDict['elev'].loc['./NODE_2148296_length_96_cov_1.145833_78',:])[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get data specific to each locus across null runs\n",
    "avgbfDict = OrderedDict()\n",
    "for env in envbfDict.keys():\n",
    "    print env\n",
    "    avgbfDict[env] = OrderedDict()\n",
    "    for locus in envbfDict[env].index:\n",
    "        avgbfDict[env][locus] = OrderedDict()\n",
    "        avgbfDict[env][locus]['mean'] = np.mean(envbfDict[env].loc[locus,:])\n",
    "        avgbfDict[env][locus]['median'] = np.median(envbfDict[env].loc[locus,:])\n",
    "        avgbfDict[env][locus]['low CI'] = mean_confidence_interval(envbfDict[env].loc[locus,:],0.95)[1]\n",
    "        avgbfDict[env][locus]['high CI'] = mean_confidence_interval(envbfDict[env].loc[locus,:],0.95)[2]\n",
    "        avgbfDict[env][locus]['variance'] = np.var(envbfDict[env].loc[locus,:])\n",
    "        avgbfDict[env][locus]['std dev'] = np.std(envbfDict[env].loc[locus,:])\n",
    "        empBF = empBFs.loc[locus[2:],env]\n",
    "        \n",
    "        perc = len(envbfDict[env].loc[locus,:][envbfDict[env].loc[locus,:] <= empBF])/len(envbfDict[env].loc[locus,:])\n",
    "        \n",
    "        avgbfDict[env][locus]['emp BF'] = empBF\n",
    "        avgbfDict[env][locus]['percentile of emp BF'] = perc*100\n",
    "        \n",
    "        sorts = sorted(envbfDict[env].loc[locus,:])\n",
    "        avgbfDict[env][locus]['99 perc'] = sorts[98] #99th percentil is the 98th index: index starts at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for env in avgbfDict.keys():\n",
    "    filE = '/home/lindb/wbp/bayenv2/nullruns/imputed/bf_files/%s_mean_BFs.txt' % env\n",
    "    with open(filE,'w') as o:\n",
    "        line = '\\t'.join(avgbfDict['elev'][avgbfDict['elev'].keys()[0]].keys()) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "        for locus in avgbfDict[env].keys():\n",
    "            line = str(locus)+'\\t'+'\\t'.join([str(x) for x in avgbfDict[env][locus].values()]) + str('\\n')\n",
    "            o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the 'percentile of emp BF' is correct, but the dataframe has not been limited to just those empirical BFs >= 5 (see below)\n",
    "df = pd.read_csv(filE,header=0,sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get all of the files output from avgbfDict()\n",
    "files = os.listdir('/home/lindb/wbp/bayenv2/nullruns/imputed/bf_files')\n",
    "files = [os.path.join('/home/lindb/wbp/bayenv2/nullruns/imputed/bf_files',f) for f in files if 'mean' in f]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put the files into dataframes\n",
    "dfDict = OrderedDict()\n",
    "for f in files:\n",
    "    env = os.path.basename(f).split(\"_\")[0]\n",
    "    dfDict[env] = pd.read_csv(f,header=0,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#determine if any have BFs >= 5\n",
    "naughtyenvs = []\n",
    "for env in dfDict.keys():\n",
    "    print env,max(dfDict[env]['mean'])\n",
    "    if max(dfDict[env]['mean']) >=5.0:\n",
    "        naughtyenvs.append(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "naughtyenvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in naughtyenvs:\n",
    "    print env\n",
    "    if env.startswith('G'):\n",
    "        envt = 'GDD-Aug'\n",
    "    if env == 'WC3rdbar':\n",
    "        envt = env\n",
    "    naughtyloci = []\n",
    "    for locus in avgbfDict[env].keys():\n",
    "        if avgbfDict[env][locus]['mean'] >= 5.0:\n",
    "            naughtyloci.append(locus[2:])\n",
    "            print env, locus, avgbfDict[env][locus]['mean']\n",
    "    \n",
    "    f = pd.read_csv('/home/lindb/wbp/bayenv2/Final/imputed/%s_BF_imputed_sigSNPs.txt' % envt,header=0,sep=\"\\t\")\n",
    "    for locus in naughtyloci:\n",
    "        if locus in f[envt].tolist():\n",
    "            print \"fuck\",locus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "empBFs = pd.read_csv('/home/lindb/wbp/bayenv2/Final/imputed/imputedBFs_by_SNP_IDXHEADER.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "empBFs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in sorted(empBFs.columns):\n",
    "    mean = np.mean(empBFs[env])\n",
    "    med = np.median(empBFs[env])\n",
    "    print env, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "empBFs.columns = [u'elev', u'%max_rad_input', u'prock_cov', u'Tmax July', u'Tmin (Jan)',\n",
    "       u'Ann ppt', u'GDD May', u'GDD Aug', u'WC3rdbar', u'WC15Bar', u'Silt',\n",
    "       u'Sand', u'Clay', u'AWS0-50', u'AWS0-25', u'CEC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in avgbfDict.keys():\n",
    "    print env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#looking at percentile of empircal BF within null runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('/home/lindb/wbp/bayenv2/nullruns/imputed/bf_files/')\n",
    "files = [os.path.join('/home/lindb/wbp/bayenv2/nullruns/imputed/bf_files/',f) for f in files if 'mean' in f]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in sorted(snpDict.keys()):\n",
    "    print env,len(snpDict[env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare minimum percentile for sig SNPs - only a problem if there's a low percentile\n",
    "for f in sorted(files):\n",
    "    env = os.path.basename(f).split(\"_\")[0]\n",
    "    df = pd.read_csv(f,header=0,sep=\"\\t\")\n",
    "    df = df[df['emp BF'] >= 5]\n",
    "    meanmed = np.mean(df['median'])\n",
    "    medmean = np.median(df['mean'])\n",
    "    medmed = np.median(df['median'])\n",
    "    meanmean = np.mean(df['mean'])\n",
    "    minemp = min(df['percentile of emp BF'])\n",
    "    meanemp = np.mean(df['percentile of emp BF'])\n",
    "    medemp = np.median(df['percentile of emp BF'])\n",
    "    medperc = np.median(df['99 perc'])\n",
    "    meanperc = np.mean(df['99 perc'])\n",
    "    print env, medemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['percentile of emp BF'].values.tolist().count(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check to make sure sig SNP numbers match up\n",
    "files = os.listdir('/home/lindb/wbp/bayenv2/Final/imputed')\n",
    "files = [os.path.join('/home/lindb/wbp/bayenv2/Final/imputed',f) for f in files if 'sigSNPs' in f]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    env = os.path.basename(f).split(\"_\")[0]\n",
    "    df = pd.read_csv(f,header=0,sep=\"\\t\")\n",
    "    print env, len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "envs = ['GDD-Aug',\n",
    "'AWS0-25',\n",
    "'AWS0-50',\n",
    "'CEC',\n",
    "'Clay',\n",
    "'elev',\n",
    "'GDD-May',\n",
    "'prock-cov',\n",
    "'Tmax-July',\n",
    "'Sand',\n",
    "'Silt',\n",
    "'Tmin-Jan',\n",
    "'%max-rad-input',\n",
    "'WC15Bar',\n",
    "'WC3rdbar',\n",
    "'Ann-ppt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

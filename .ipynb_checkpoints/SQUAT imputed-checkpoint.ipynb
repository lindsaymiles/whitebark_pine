{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# first make global gwas.data.file without EFF\n",
    "# not efficient: make global gwas.data.files for each phenotype and input the EFF from the new_imp/mis_alphas.txt\n",
    "    # not efficient because we don't need every locus\n",
    "# go in to global gwas.data.file  \n",
    "    # for either stringent or 0.01:\n",
    "    # for each phenotype, pull loci IDed by piMASS\n",
    "    # for these loci, pull A1, A2, and FRQ from global gwas.data.file\n",
    "    # for these loci, pull EFF from the alphas\n",
    "    # write each phenotype file for each of stringent or 0.01\n",
    "# make full.dataset.file from scratch\n",
    "# freq.file will need to be made separately by indexing SNPs IDed by piMASS for both stringent and 0.01\n",
    "    #can pull from full.dataset.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to remake gwas.data.files so that the global allele freq is only from samples within the 6 pops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict, Counter\n",
    "import vcf\n",
    "import os\n",
    "from __future__ import division\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#assigning samples to pops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first make dict with k=UCD_ID and v = Population_ID, then match with sample_ID from the other file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stp = pd.read_csv('/home/lindb/eckertlab/wbp/hierfstat/sampsTOpop.txt',header=0,sep=\"\\t\")\n",
    "stp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(stp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(np.unique(stp['pop']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(stp['pop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pops = ['Dicks_Pass','Freel_Peak','Little_Round_Top','Mt_Rose_Ophir','Rifle_Peak','Snow_Valley_Peak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stpDict = OrderedDict()\n",
    "for row in stp.index:\n",
    "    if stp.loc[row,'pop'] in pops:\n",
    "        if stp.loc[row,'samp'] not in stpDict.keys():\n",
    "            stpDict[stp.loc[row,'samp']] = stp.loc[row,'pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(stpDict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#grab alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp_alphas = pd.read_csv('/home/lindb/eckertlab/wbp/piMASS/analyses2/6alphas/new_imputed_alphasHEADROW.txt',sep=\"\\t\",header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#impcols = pd.read_csv('/home/lindb/eckertlab/wbp/piMASS/analyses2/6alphas/new_imputed_columns.txt',sep=\"\\t\",header=None)\n",
    "#improws = pd.read_csv('/home/lindb/eckertlab/wbp/piMASS/analyses2/6alphas/new_imputed_rows.txt',sep=\"\\t\",header=None)\n",
    "#imp_alphas.columns = [x for x in impcols[0]]\n",
    "#imp_alphas.index = [x for x in improws[0]]\n",
    "imp_alphas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(imp_alphas.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#make gwas.data.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_dir = '/home/lindb/eckertlab/wbp/bowtie2.1/bowtierun/dup_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#VCFmissing = os.path.join(file_dir,'good_snps_good_samples_missing.vcf')\n",
    "VCFimputed = os.path.join(file_dir,'good_snps_good_samples.vcf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputed_reader = vcf.Reader(open(VCFimputed),'r')\n",
    "#miss_reader = vcf.Reader(open(VCFmissing),'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intersection = pd.read_csv('/home/lindb/eckertlab/wbp/piMASS/IntersectionRowNames.txt',sep=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intset = set(intersection[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(intset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#example of what happens\n",
    "count = 0\n",
    "recCount = 0\n",
    "for rec in imputed_reader:\n",
    "    print rec\n",
    "    for sample in rec:\n",
    "        print sample\n",
    "        count +=1\n",
    "        if count % 10 ==0:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(stpDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a key so we can sample each rec in order of sampleID\n",
    "smpLST = [rec.samples[i].sample for i,x in enumerate(rec.samples)]\n",
    "sortSMPLST = sorted(smpLST)\n",
    "sampKey = []\n",
    "for i,smp in enumerate(sortSMPLST):\n",
    "    sampKey.append(smpLST.index(sortSMPLST[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(sampKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp_alphas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grab global allele frequencies among the 6 populations\n",
    "imputed_reader = vcf.Reader(open(VCFimputed),'r')\n",
    "recCount = 0\n",
    "locDict = OrderedDict()      #key = locus, value = lineDict\n",
    "for rec in imputed_reader:   #for each locus\n",
    "    lineDict = OrderedDict()      #key = gwas.data.file column name (eg 'SNP','A1','A2','EFF','FRQ')\n",
    "                                  #value = either locus, mjr_all, mnr_all, alpha, A1_frq\n",
    "    #ref = rec.REF\n",
    "    #alt = rec.ALT[0]\n",
    "    #c = Counter()\n",
    "    \n",
    "    locus = \"_\".join([str(rec.CHROM),str(rec.POS)])\n",
    "    if locus in intset:      #if it's a locus we're interested in\n",
    "        ref = rec.REF\n",
    "        alt = rec.ALT[0]\n",
    "        c = Counter()\n",
    "        lineDict['SNP'] = locus\n",
    "        samps = 0\n",
    "        for i in sampKey:\n",
    "            sample = rec.samples[i]\n",
    "            if sample.sample in stpDict.keys(): #if the sample is in one of the 6 populations\n",
    "                gt = sample['GT'].split('|')\n",
    "                if '.' not in gt: #if '.' isn't in the sample genotype\n",
    "                    c[gt[0]] += 1 #count the first allele using the allele as a key\n",
    "                    c[gt[1]] += 1\n",
    "                #if samps % 20 == 0:\n",
    "                    #break\n",
    "        if c['0'] > c['1']: #if the ref allele is the major allele\n",
    "            lineDict['A1'] = ref\n",
    "            lineDict['A2'] = alt\n",
    "            lineDict['EFF'] = np.nan #leave this blank, then from the outfile, create files with alpha ~ phenotype\n",
    "            lineDict['FRQ'] = c['0'] / (c['0'] + c['1'])\n",
    "        else:               #if the ref allele is the minor allele\n",
    "            lineDict['A1'] = alt\n",
    "            lineDict['A2'] = ref\n",
    "            lineDict['EFF'] = np.nan #leave this blank, then from the outfile, create files with alpha ~ phenotype\n",
    "            lineDict['FRQ'] = c['1'] / (c['0'] + c['1'])\n",
    "\n",
    "        locDict[locus] = lineDict\n",
    "    recCount += 1\n",
    "    if recCount % 1000 == 0:\n",
    "        print recCount\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(locDict.keys()) #should be 159803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/eckertlab/wbp/SQUAT2/all_imp_gwas_data_file.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(filE,'w') as o:\n",
    "    rowCount = 0\n",
    "    line = '\\t'.join(['SNP','A1','A2','EFF','FRQ']) + str('\\n')\n",
    "    o.write(\"%s\" % line)\n",
    "    for locus in locDict.keys():\n",
    "        line = '\\t'.join(str(x) for x in locDict[locus].values()) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locDict[locus].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# i did the same thing for missing data in another iPython window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impgwas = pd.read_csv('/home/lindb/eckertlab/wbp/SQUAT2/all_imp_gwas_data_file.txt',sep=\"\\t\",header=0)\n",
    "impgwas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min(impgwas['FRQ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impgwas['FRQ'].tolist().count(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misgwas = pd.read_csv('/home/lindb/eckertlab/wbp/SQUAT2/all_mis_gwas_data_file.txt',sep=\"\\t\",header=0)\n",
    "misgwas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print min(misgwas['FRQ']),max(misgwas['FRQ']),np.mean(misgwas['FRQ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misgwas['FRQ'].tolist().count(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "misgwas.index = [str(x) for x in misgwas['SNP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misgwas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impgwas.index = [str(x) for x in impgwas['SNP']]\n",
    "misgwas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impgwas.to_csv('/home/lindb/eckertlab/wbp/SQUAT2/all_imp_gwas_data_fileIDXHEADER.txt',sep=\"\\t\",header=True,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "misgwas.to_csv('/home/lindb/eckertlab/wbp/SQUAT2/all_mis_gwas_data_fileIDXHEADER.txt',sep=\"\\t\",header=True,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impgwas = pd.read_csv('/home/lindb/eckertlab/wbp/SQUAT2/all_imp_gwas_data_fileIDXHEADER.txt',sep=\"\\t\",header=0,index_col=0)\n",
    "impgwas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misgwas = pd.read_csv('/home/lindb/eckertlab/wbp/SQUAT2/all_mis_gwas_data_fileIDXHEADER.txt',sep=\"\\t\",header=0,index_col=0)\n",
    "misgwas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#make pheno gwas.data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gwas = OrderedDict()\n",
    "gwas['imp'] = impgwas\n",
    "gwas['mis'] = misgwas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to take these gwas.data.files and use only loci output from piMASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mis_alphas = pd.read_csv('/home/lindb/eckertlab/wbp/piMASS/analyses2/6alphas/new_missing_alphasHEADROW.txt',sep=\"\\t\",header=0,index_col=0)\n",
    "mis_alphas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Alphas = OrderedDict()\n",
    "Alphas['imp'] = imp_alphas\n",
    "Alphas['mis'] = mis_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first make global gwas.data.file without EFF\n",
    "# not efficient: make global gwas.data.files for each phenotype and input the EFF from the new_imp/mis_alphas.txt\n",
    "    # not efficient because we don't need every locus\n",
    "# go in to global gwas.data.file  \n",
    "    # for either stringent or 0.01:\n",
    "    # for each phenotype, pull loci IDed by piMASS\n",
    "    # for these loci, pull A1, A2, and FRQ from global gwas.data.file\n",
    "    # for these loci, pull EFF from the alphas\n",
    "    # write each phenotype file for each of stringent or 0.01\n",
    "# make full.dataset.file from scratch\n",
    "# freq.file will need to be made separately by indexing SNPs IDed by piMASS for both stringent and 0.01\n",
    "    #can pull from full.dataset.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_dir = '/home/lindb/eckertlab/wbp/piMASS/analyses2/5stringent'\n",
    "put_dir = '/home/lindb/eckertlab/wbp/SQUAT2/0gwas.data.files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataframes that give strinj snps\n",
    "for root,dirs,files in os.walk(loc_dir):\n",
    "    for f in files:\n",
    "        print f\n",
    "        if f.endswith('.txt'):\n",
    "            splits = f.split(\"_\")\n",
    "            pheno = '_'.join([splits[1][:-4],splits[1][-4:-1]])\n",
    "            fz = os.path.join(root,f)\n",
    "            df = pd.read_csv(fz,sep=\"\\t\",header=0,index_col=0)\n",
    "            if len(df.index) > 0: #if piMASS identified at least one locus\n",
    "                print \"greater\"\n",
    "                filE = os.path.join(put_dir,\"gwas.data.file_%s.%s_stringent.txt\" % (splits[0],pheno))\n",
    "                with open(filE,'w') as o:\n",
    "                    line = '\\t'.join(['SNP','A1', 'A2','EFF','FRQ']) + str('\\n')\n",
    "                    o.write(\"%s\" % line)\n",
    "                    for i,idx in enumerate(df.index): #idx is not in order, df.index[0] !=0, df.index[0]=28 or another number\n",
    "                        locus = df.loc[idx,'rs']\n",
    "                        locdata = gwas[splits[0]].loc[locus,:].values.tolist()\n",
    "                        locdata[-2] = Alphas[splits[0]].loc[locus,pheno] #fill in alpha value\n",
    "                        line = '\\t'.join([str(x) for x in locdata]) + str('\\n')\n",
    "                        o.write(\"%s\" % line)\n",
    "                    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_dir = '/home/lindb/eckertlab/wbp/piMASS/analyses2/3sig_snps/'\n",
    "put_dir = '/home/lindb/eckertlab/wbp/SQUAT2/0gwas.data.files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataframes that give 0.01 snps\n",
    "for root,dirs,files in os.walk(loc_dir):\n",
    "    for f in files:\n",
    "        print f\n",
    "        if f.endswith('.txt'):\n",
    "            splits = f.split(\"_\")\n",
    "            pheno = '_'.join([splits[1][:-4],splits[1][-4:-1]])\n",
    "            fz = os.path.join(root,f)\n",
    "            df = pd.read_csv(fz,sep=\"\\t\",header=0,index_col=0)\n",
    "            if len(df.index) > 0: #if piMASS identified at least one locus\n",
    "                print \"greater\"\n",
    "                filE = os.path.join(put_dir,\"gwas.data.file_%s.%s_pointOhOne.txt\" % (splits[0],pheno))\n",
    "                with open(filE,'w') as o:\n",
    "                    line = '\\t'.join(['SNP','A1', 'A2','EFF','FRQ']) + str('\\n')\n",
    "                    o.write(\"%s\" % line)\n",
    "                    for i,idx in enumerate(df.index): #idx is not in order, df.index[0] !=0, df.index[0]=28 or another number\n",
    "                        locus = df.loc[idx,'rs']\n",
    "                        locdata = gwas[splits[0]].loc[locus,:].values.tolist()\n",
    "                        locdata[-2] = Alphas[splits[0]].loc[locus,pheno] #fill in alpha value\n",
    "                        line = '\\t'.join([str(x) for x in locdata]) + str('\\n')\n",
    "                        o.write(\"%s\" % line)\n",
    "                    o.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#make full.data.set.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make full.data.set.file before making freq.files\n",
    "#need one for imputed, one for missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stpDict['009compiled_sorted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampDict = OrderedDict()\n",
    "for pop in pops:\n",
    "    sampDict[pop] = []\n",
    "    print pop,\"=\",sampDict[pop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(smpLST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(stpDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smp in stpDict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampDict = OrderedDict()\n",
    "for pop in pops:\n",
    "    print pop\n",
    "    sampDict[pop] = []\n",
    "for i,smp in enumerate(smpLST):\n",
    "    if smp in stpDict.keys():\n",
    "        sampDict[stpDict[smp]].append(i)\n",
    "    else:\n",
    "        print smp\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(sampDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#it is 175 because not all samples made the cut to 'good samples'\n",
    "sums = 0\n",
    "for pop in sampDict.keys():\n",
    "    print pop,len(sampDict[pop])\n",
    "    sums = sums + len(sampDict[pop])\n",
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hump = '/home/lindb/eckertlab/wbp/hierfstat/sampsTOpop.txt'\n",
    "hump = pd.read_csv(hump,sep=\"\\t\",header=0)\n",
    "hump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(hump.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "POPS = np.unique(hump['pop'].values.tolist()).tolist()\n",
    "POPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "humpDict = OrderedDict()\n",
    "for row in hump.index:\n",
    "    pop = hump.loc[row,'pop']\n",
    "    samp = hump.loc[row,'samp']\n",
    "    if pop not in humpDict.keys():\n",
    "        humpDict[pop] = []\n",
    "    humpDict[pop].append(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sums = 0\n",
    "for pop in sorted(humpDict.keys()):\n",
    "    print pop,len(humpDict[pop])\n",
    "    sums = sums + len(humpDict[pop])\n",
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sums = 0\n",
    "for pop in sorted(pops):\n",
    "    print pop, len(sampDict[pop])\n",
    "    sums = sums + len(sampDict[pop])\n",
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(smpLST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "itemgetter(*sampDict['Mt_Rose_Ophir'])(rec.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_dir = '/home/lindb/eckertlab/wbp/bowtie2.1/bowtierun/dup_dir/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampDict['Dicks_Pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(itemgetter(*sampDict['Dicks_Pass'])(rec.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#grab global allele frequencies among the 6 populations\n",
    "#freqDict[locus][pop] = \n",
    "recCount = 0\n",
    "freqDict = OrderedDict()      #key = locus, value = lineDict\n",
    "\n",
    "imputed_reader = vcf.Reader(open(VCFimputed),'r')\n",
    "for rec in imputed_reader:   #for each locus\n",
    "    locus = \"_\".join([str(rec.CHROM),str(rec.POS)])\n",
    "    #print locus\n",
    "    \n",
    "    if locus in intset: #if it's a locus we're interested in\n",
    "        ref = rec.REF\n",
    "        alt = rec.ALT[0]\n",
    "        \n",
    "        freqDict[locus] = OrderedDict()\n",
    "        for pop in pops:    \n",
    "            samples = itemgetter(*sampDict[pop])(rec.samples)\n",
    "            c = Counter()\n",
    "            lineDict = OrderedDict()\n",
    "            for sampl in samples:\n",
    "                gt = sampl['GT'].split('|')\n",
    "                if '.' not in gt:\n",
    "                    c[gt[0]] += 1\n",
    "                    c[gt[1]] += 1\n",
    "            if c['0'] > c['1']: #if the ref allele is the major allele\n",
    "                lineDict['SNP'] = locus\n",
    "                lineDict['CLST'] = pop\n",
    "                lineDict['A1'] = ref\n",
    "                lineDict['A2'] = alt\n",
    "                lineDict['FRQ'] = c['0'] / (c['0']+c['1'])\n",
    "            else:               #if the alt allele is the major allele\n",
    "                lineDict['SNP'] = locus\n",
    "                lineDict['CLST'] = pop\n",
    "                lineDict['A1'] = alt\n",
    "                lineDict['A2'] = ref\n",
    "                lineDict['FRQ'] = c['1'] / (c['0']+c['1'])\n",
    "            \n",
    "            lineDict['IMP']  = 0\n",
    "            lineDict['POS']  = 0\n",
    "            lineDict['CHR']  = 0\n",
    "            lineDict['BVAL'] = 0\n",
    "            \n",
    "            freqDict[locus][pop] = lineDict\n",
    "\n",
    "    recCount += 1\n",
    "    if recCount % 1000 == 0:\n",
    "        print recCount\n",
    "        #break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(freqDict.keys()) #should be 159803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/eckertlab/wbp/SQUAT2/2full.dataset.files/full.dataset.imputed.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(filE,'w') as o:\n",
    "    line = '\\t'.join(['SNP','CLST','A1','A2','FRQ','IMP','POS','CHR','BVAL']) + str('\\n')\n",
    "    o.write(\"%s\" % line)\n",
    "    for locus in freqDict.keys():\n",
    "        for pop in freqDict[locus].keys():\n",
    "            line = '\\t'.join([str(x) for x in freqDict[locus][pop].values()]) + str('\\n')\n",
    "            o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impfull = pd.read_csv(filE,header=0,sep=\"\\t\")\n",
    "impfull.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = ['strinj','0.1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#make freq.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "look_dir = '/home/lindb/eckertlab/wbp/SQUAT2/0gwas.data.files/'\n",
    "put_dir = '/home/lindb/eckertlab/wbp/SQUAT2/2freq.files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for root,dirs,files in os.walk(look_dir):\n",
    "    for f in files:\n",
    "        print f\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for root,dirs,files in os.walk(look_dir):\n",
    "    for f in files:\n",
    "        if ('imp' in f) and ('stringent' in f):\n",
    "            print f\n",
    "            df = pd.read_csv(os.path.join(root,f),sep=\"\\t\",header=0)\n",
    "            splits = f.split(\"_\")\n",
    "            pheno = \"\".join([splits[1],splits[2]])\n",
    "            filE = os.path.join(put_dir,'freq.file_%s_stringent.txt' % pheno)\n",
    "            print \"pheno =\",pheno\n",
    "            with open(filE,'w') as o:    \n",
    "                line = '\\t'.join(['SNP','CLST','A1','A2','FRQ','IMP','POS','CHR','BVAL']) + str('\\n')\n",
    "                o.write(\"%s\" % line)\n",
    "                for locus in df['SNP'].values: #for each locus\n",
    "                    for pop in freqDict[locus].keys():\n",
    "                        line = '\\t'.join([str(x) for x in freqDict[locus][pop].values()]) + str('\\n')\n",
    "                        o.write(\"%s\" % line)\n",
    "            o.close()\n",
    "            #print \"done with\",f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "look_dir = '/home/lindb/eckertlab/wbp/SQUAT2/0gwas.data.files/'\n",
    "put_dir = '/home/lindb/eckertlab/wbp/SQUAT2/2freq.files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#can't loop in the missing data because I need freqDict\n",
    "for root,dirs,files in os.walk(look_dir):\n",
    "    for f in files:\n",
    "        df = pd.read_csv(os.path.join(root,f),sep=\"\\t\",header=0)\n",
    "        if ('imp' in f) and ('point' in f):\n",
    "            splits = f.split(\"_\")\n",
    "            pheno = \"\".join([splits[1],splits[2]])\n",
    "            filE = os.path.join(put_dir,'freq.file_%s_pointOhOne.txt' % pheno)\n",
    "            with open(filE,'w') as o:  \n",
    "                line = '\\t'.join(['SNP','CLST','A1','A2','FRQ','IMP','POS','CHR','BVAL']) + str('\\n')\n",
    "                o.write(\"%s\" % line)\n",
    "                for locus in df['SNP'].values: #for each locus\n",
    "                    for pop,vals in freqDict[locus].items():\n",
    "                        line = '\\t'.join([str(x) for x in freqDict[locus][pop].values()]) + str('\\n')\n",
    "                        o.write(\"%s\" % line)\n",
    "            o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sums = 0\n",
    "for k,v in sampDict.items():\n",
    "    print  k, len(sampDict[k])\n",
    "    sums = sums + len(sampDict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#get env.var.data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "envdf = pd.read_csv('/home/lindb/eckertlab/wbp/bowtie2.1/bowtierun/dup_dir/WBP_IDS_MATCHED_POP_FINAL_05182015_canonical.csv',header=0,sep=\",\")\n",
    "envdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colz = [str(x) for x in envdf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, col in enumerate(colz[15:20]):\n",
    "    print i+15,col\n",
    "    colz[i+15] = \"_\".join(col.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envdf.columns = colz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envdf.columns[15:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envs = pd.read_csv('/home/lindb/eckertlab/wbp/BayEnv/ENVIRONFILErows.txt',header=None,sep=\"\\t\")\n",
    "envs = [str(x) for x in envs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,env in enumerate(envs[3:8]):\n",
    "    print i+3\n",
    "    envs[i+3] = \"_\".join(env.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envs[3:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(envdf['Population_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = OrderedDict()\n",
    "for env in envs:\n",
    "    data[env] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popDict = OrderedDict()\n",
    "for pop in pops:\n",
    "    popDict[pop] = []\n",
    "for row in envdf.index:\n",
    "    pop = envdf.loc[row,'Population_ID']\n",
    "    if pop in pops:\n",
    "        popDict[pop].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vals = pd.DataFrame(index=envs,columns = pops)\n",
    "for pop in pops:\n",
    "    samps = envdf.loc[popDict[pop],:]\n",
    "    #envDict = OrderedDict()\n",
    "    for env in envs:\n",
    "        #envDict[env] = np.mean(samps[env].valuse.tolist())\n",
    "        vals.loc[env,pop] = \"%.8f\" % np.mean(samps[env].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals.to_csv('/home/lindb/eckertlab/wbp/SQUAT/4env.var.data.files/all_env_valuesHEADERIDX.txt',header=True,index=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_dir = '/home/lindb/eckertlab/wbp/SQUAT/4env.var.data.files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for env in vals.index:\n",
    "    filE = os.path.join(file_dir,\"env.var_%s.txt\" % env)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"CLST\\tENV\\tREG\\n\")\n",
    "        for i,pop in enumerate(vals.columns):\n",
    "            o.write(\"%s\\t%s\\t%d\\n\" % (pop, vals.loc[env,pop], i+1))\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_dir = '/home/lindb/eckertlab/wbp/SQUAT/2full.dataset.files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for f in files:\n",
    "        print f\n",
    "        df = pd.read_csv(os.path.join(root,f),sep=\"\\t\",header=None)\n",
    "        df.columns = ['SNP','CLST','A1','A2','FRQ','IMP','POS','CHR','BVAL']\n",
    "        df.to_csv(os.path.join(root,f),sep=\"\\t\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#make run files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_dir = '/home/lindb/eckertlab/wbp/SQUAT2/2freq.files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LST = []\n",
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for f in files:\n",
    "        splits = f.split(\"_\")[1:]\n",
    "        sec = splits[1].split(\".\")\n",
    "        LST.append(\"_\".join([splits[0] , sec[0]]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(LST),LST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phenos = []\n",
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for f in files:\n",
    "        pheno =  f.split(\"_\")[1]\n",
    "        if pheno not in phenos:\n",
    "            phenos.append(f.split(\"_\")[1])\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(phenos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sets = ['imp','mis']\n",
    "levels = ['pointOhOne','stringent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "/home/lindb/eckertlab/wbp/SQUAT/4runfiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for pheno in phenos:\n",
    "    print pheno.split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "look_dir = '/home/lindb/eckertlab/wbp/SQUAT2/0gwas.data.files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for root,dirs,files in os.walk(look_dir):\n",
    "    for f in files:\n",
    "        print f\n",
    "        splits = f.split(\"_\")\n",
    "        pheno = \"\".join([splits[1],splits[2]])\n",
    "        fz = \"_\".join([splits[0],pheno,splits[3]])\n",
    "        print splits\n",
    "        print fz\n",
    "        df  = pd.read_csv(os.path.join(root,f),header=0,sep=\"\\t\")\n",
    "        df.to_csv(os.path.join('/home/lindb/eckertlab/wbp/SQUAT2/moved',fz),header=True,index=False,sep=\"\\t\")\n",
    "        #break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for level in levels:\n",
    "    for pheno in phenos:\n",
    "        print pheno,level\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for level in levels:\n",
    "    for pheno in phenos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for level in levels:\n",
    "    for pheno in phenos:\n",
    "        joined = \"_\".join([pheno,level])\n",
    "        if joined in LST:\n",
    "            filE = '/home/lindb/eckertlab/wbp/SQUAT2/4runfiles/%s_%s_runfile.R' % (pheno,level)\n",
    "            text = '''\n",
    "setwd(\"/home/lindb/eckertlab/wbp/SQUAT2/\")\n",
    "source (\"Scripts/functions.R\")\n",
    "PolygenicAdaptationFunction (\n",
    "\tgwas.data.file = \"0gwas.data.files/gwas.data.file_%s_%s.txt\", # (pheno,level)\n",
    "\tfreqs.file = \"2freq.files/freq.file_%s_%s.txt\", # (pheno,level)\n",
    "\tenv.var.data.files = list(\t\n",
    "\t\t\"3env.var.data.files/env.var_elev.txt\",\n",
    "        \"3env.var.data.files/env.var_Ann_ppt.txt\",\n",
    "\t\t\"3env.var.data.files/env.var_AWS0-25.txt\",\n",
    "\t\t\"3env.var.data.files/env.var_AWS0-50.txt\",\n",
    "\t\t\"3env.var.data.files/env.var_CEC.txt\", \n",
    "\t\t\"3env.var.data.files/env.var_Clay.txt\",\n",
    "\t\t\"3env.var.data.files/env.var_GDD_Aug.txt\",\n",
    "\t\t\"3env.var.data.files/env.var_GDD_May.txt\",\n",
    "\t\t\"3env.var.data.files/env.var_prock_cov.txt\",\n",
    "\t\t\"3env.var.data.files/env.var_Sand.txt\",\n",
    "\t\t\"3env.var.data.files/env.var_Silt.txt\" ,\n",
    "\t\t\"3env.var.data.files/env.var_Tmax_(July).txt\",\n",
    "\t\t\"3env.var.data.files/env.var_Tmin_(Jan).txt\",\n",
    "\t\t\"3env.var.data.files/env.var_WC3rdbar.txt\",\n",
    "\t\t\"3env.var.data.files/env.var_WC15Bar.txt\",\n",
    "\t\t\"3env.var.data.files/env.var_%%max_rad_input.txt\"\n",
    "\t),\n",
    "\tmatch.pop.file = \"5match.pop.files/match.pop.file_%s.txt\" , #  imp or mis\n",
    "\tfull.dataset.file = \"1full.dataset.files/full.dataset.%s.txt\", #  imp or mis\n",
    "\tpath = \"6out.folders/%s/%s_%s\", #  (imp/mis,pheno,level)\n",
    "\tmatch.categories = c ( \"MAF\" ),\n",
    "\tmatch.bins = list ( seq ( 0 , 0.5 , 0.02 ), c ( 2 ) , seq ( 0,1000,100 ) ), \n",
    "\tcov.SNPs.per.cycle = 5000, \n",
    "\tcov.cycles = 1, \n",
    "\tnull.phenos.per.cycle = 1000, \n",
    "\tnull.cycles = 1,\n",
    "\tload.cov.mat = F,\n",
    "\tsim.null = T,\n",
    "\tcheck.allele.orientation = T\n",
    "\t)\n",
    "''' % (pheno,\n",
    "       level,\n",
    "       pheno,\n",
    "       level,\n",
    "       pheno.split(\".\")[0],\n",
    "       pheno.split(\".\")[0],\n",
    "       pheno.split(\".\")[0],\n",
    "       pheno,\n",
    "       level)\n",
    "            with open(filE,'w') as o:\n",
    "                o.write(text)\n",
    "            o.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#make match.pop.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#god damn it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# freqDict[locus][pop] = lineDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(freqDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqDict['NODE_658_length_101_cov_1.207921_8']['Dicks_Pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_dir = '/home/lindb/eckertlab/wbp/SQUAT2/5match.pop.files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pop in pops:\n",
    "    filE = os.path.join(file_dir,'match.pop.file_imp_%s.txt' % pop)\n",
    "    with open(filE,'w') as o:\n",
    "        line = '\\t'.join(['SNP','CLST','A1','A2','FRQ','IMP','POS','CHR','BVAL']) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "        for locus in freqDict.keys(): #for each locus\n",
    "            line = '\\t'.join([str(x) for x in freqDict[locus][pop].values()]) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataDict = OrderedDict()\n",
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for f in files:\n",
    "        if 'imp_' in f:\n",
    "            splits = f.split(\"_\")\n",
    "            #sec = splits[4].split(\".\")\n",
    "            p = \"_\".join(splits[2:])\n",
    "            dataDict[p] = pd.read_csv(os.path.join(root,f),sep=\"\\t\",header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataDict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pop in dataDict.keys():\n",
    "    print pop,'\\t',max(dataDict[pop]['FRQ']),'\\t', dataDict[pop]['FRQ'].tolist().count(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataDict[pop]['FRQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/eckertlab/wbp/SQUAT/5match.pop.files/match.pop.file_imp.txt'\n",
    "oldfreq = pd.read_csv(filE,header=0,sep=\"\\t\")\n",
    "oldfreq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print max(oldfreq['FRQ']), oldfreq['FRQ'].tolist().count(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##adjust files to only include those loci which are not fixed in Freel Peak (match.pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for root,dirs,files in os.walk('/home/lindb/eckertlab/wbp/SQUAT2/0gwas.data.files'):\n",
    "    for f in files:\n",
    "        print f.split(\"_\")\n",
    "        pheno = \"\".join(f.split(\"_\")[1:3])\n",
    "        print pheno\n",
    "        newfile = \"_\".join([f.split(\"_\")[0],pheno,f.split(\"_\")[3]])\n",
    "        newfile = os.path.join(root,newfile)\n",
    "        print newfile\n",
    "        f = os.path.join(root,f)\n",
    "        !mv $f $newfile\n",
    "        #break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/eckertlab/wbp/SQUAT2/4runfiles/imp.bffam_stringent_runfile.R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.path.dirname(filE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impmatch = pd.read_csv('/home/lindb/eckertlab/wbp/SQUAT2/5match.pop.files/match.pop.file_imp.txt',sep=\"\\t\",header=0)\n",
    "mismatch = pd.read_csv('/home/lindb/eckertlab/wbp/SQUAT2/5match.pop.files/match.pop.file_mis.txt',sep=\"\\t\",header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mismatch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goodsnpsimp = impmatch['SNP'][impmatch['FRQ'] < 1].values\n",
    "goodsnpsmis = mismatch['SNP'][mismatch['FRQ'] < 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(goodsnpsimp),len(goodsnpsmis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randdf = impmatch[impmatch['SNP'].isin(goodsnpsimp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(randdf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newimpmatch = impmatch[impmatch['SNP'].isin(goodsnpsimp)]\n",
    "newmismatch = mismatch[mismatch['SNP'].isin(goodsnpsmis)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(newimpmatch),len(newmismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newmismatch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newmismatch.to_csv('/home/lindb/eckertlab/wbp/SQUAT2/goodfiles/5match.pop.files/match.pop.file_mis.txt',sep=\"\\t\",header=True,index=False)\n",
    "newimpmatch.to_csv('/home/lindb/eckertlab/wbp/SQUAT2/goodfiles/5match.pop.files/match.pop.file_imp.txt',sep=\"\\t\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open(filE,'r') as o:\n",
    "    for line in o:\n",
    "        print count,line.strip()\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "workdir = '/home/lindb/eckertlab/wbp/SQUAT2/'\n",
    "savedir = '/home/lindb/eckertlab/wbp/SQUAT2/goodfiles/'\n",
    "run_dir = '/home/lindb/eckertlab/wbp/SQUAT2/4runfiles/'\n",
    "\n",
    "for root,dirs,files in os.walk(run_dir):\n",
    "    for f in files:\n",
    "        filE = os.path.join(root,f)\n",
    "        with open(filE,'r') as o:\n",
    "            #print o.readlines()\n",
    "\n",
    "            hey = [str(x) for x in o.readlines()]\n",
    "\n",
    "            gwasfile  = hey[4].strip().split(\" \")[2][1:-2]\n",
    "            GWASDF = pd.read_csv(str(workdir+gwasfile),sep=\"\\t\",header=0)\n",
    "            \n",
    "            freqfile  = hey[5].strip().split(\" \")[2][1:-2]\n",
    "            FREQDF = pd.read_csv(str(workdir+freqfile),sep=\"\\t\",header=0)\n",
    "            \n",
    "            matchfile = hey[24].strip().split(\" \")[2][1:-1]\n",
    "            MATCHDF = pd.read_csv(str(workdir+matchfile),sep=\"\\t\",header=0)\n",
    "            \n",
    "            fullfile  = hey[25].strip().split(\" \")[2][1:-2]\n",
    "            FULLDF = pd.read_csv(str(workdir+fullfile),sep=\"\\t\",header=0)\n",
    "\n",
    "            if 'imp' in matchfile:\n",
    "                newgwas = GWASDF[GWASDF['SNP'].isin(goodsnpsimp)]\n",
    "                newgwas.to_csv(str(savedir+gwasfile),header=True,sep=\"\\t\",index=False)\n",
    "                newfreq = FREQDF[FREQDF['SNP'].isin(goodsnpsimp)]\n",
    "                newfreq.to_csv(str(savedir+freqfile),header=True,sep=\"\\t\",index=False)\n",
    "                newfull = FULLDF[FULLDF['SNP'].isin(goodsnpsimp)]\n",
    "                newfull.to_csv(str(savedir+fullfile),header=True,sep=\"\\t\",index=False)\n",
    "                newmatch = MATCHDF[MATCHDF['SNP'].isin(goodsnpsimp)]\n",
    "                newmatch.to_csv(str(savedir+matchfile),header=True,sep=\"\\t\",index=False)\n",
    "\n",
    "            if 'mis' in matchfile:\n",
    "                newgwas = GWASDF[GWASDF['SNP'].isin(goodsnpsmis)]\n",
    "                newgwas.to_csv(str(savedir+gwasfile),header=True,sep=\"\\t\",index=False)\n",
    "                newfreq = FREQDF[FREQDF['SNP'].isin(goodsnpsmis)]\n",
    "                newfreq.to_csv(str(savedir+freqfile),header=True,sep=\"\\t\",index=False)\n",
    "                newfull = FULLDF[FULLDF['SNP'].isin(goodsnpsmis)]\n",
    "                newfull.to_csv(str(savedir+fullfile),header=True,sep=\"\\t\",index=False)\n",
    "                newmatch = MATCHDF[MATCHDF['SNP'].isin(goodsnpsmis)]\n",
    "                newmatch.to_csv(str(savedir+matchfile),header=True,sep=\"\\t\",index=False)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FREQDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for root,dirs,files in os.walk('/home/lindb/eckertlab/wbp/SQUAT2/goodfiles/0gwas.data.files/'):\n",
    "    for f in files:\n",
    "        df = pd.read_csv(os.path.join(root,f),sep=\"\\t\",header=0)\n",
    "        if not len(df.index) > 1:\n",
    "            print f \n",
    "            # I removed each of the corresponding runfiles from goodfiles/4runfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#now figure out how many snps were taken from each gwas file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gDict = OrderedDict()\n",
    "for root,dirs,files in os.walk('/home/lindb/eckertlab/wbp/SQUAT2/0gwas.data.files'):\n",
    "    for f in files:\n",
    "        df = pd.read_csv(os.path.join(root,f),header=0,sep=\"\\t\")\n",
    "        gDict[f] = OrderedDict()\n",
    "        gDict[f]['orig'] = len(df.index)\n",
    "for root,dirs,files in os.walk('/home/lindb/eckertlab/wbp/SQUAT2/goodfiles/0gwas.data.files/'):\n",
    "    for f in files:\n",
    "        df = pd.read_csv(os.path.join(root,f),header=0,sep=\"\\t\")\n",
    "        gDict[f]['new'] = len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in sorted(gDict.keys()):\n",
    "    print f,gDict[f].values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#run them R files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_dir = '/home/lindb/eckertlab/wbp/SQUAT2/goodfiles/4runfiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squat_files = []\n",
    "for root,dirs,files in os.walk(file_dir):\n",
    "    for f in files:\n",
    "        squat_files.append(os.path.join(root,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(squat_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "squat_files[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_r(): ####/gpfs_fs/home/lindb/anaconda/envs/conda/lib/R ####/home/lindb/g/R3/lib64/R/\n",
    "    os.environ['R_HOME'] = '/home/lindb/g/R3/lib64/R/' \n",
    "    os.environ['LD_LIBRARY_PATH'] = \"%s/lib:%s\" % (os.environ['R_HOME'], ####/home/lindb/g/R3/lib64/R/bin/R\n",
    "                                                   os.environ['LD_LIBRARY_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setup_r()\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri as pd2ri\n",
    "pd2ri.activate()\n",
    "r = robjects.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r('pi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for squat in squat_files:\n",
    "    print squat\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for squat in squat_files:\n",
    "    print squat\n",
    "    r(\"source('%s')\" % squat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#nothing to see here... move along..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

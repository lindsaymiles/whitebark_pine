{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "from __future__ import division\n",
    "import math\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first write script to do lower covariance matrix for a single pop\n",
    "#then write .sh files for each pop or maybe by each snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impfile = pd.read_csv('/home/lindb/eckertlab/wbp/bayenv2/UnbinnedImputedSNPSFILE.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "impfile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use all pops at first, will need all MAFs for bayenv2 covariances\n",
    "#later in script limit MAF dataframe to 6/8 pops which had seedlings in common gardens\n",
    "pops = [str(x) for x in impfile.columns]\n",
    "pops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#freqDict[pop][locus] = minor allele freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loci = np.unique(impfile.index).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pop in pops:\n",
    "    text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "F2 = '/home/lindb/wbp/covariances/pimass/imputed/MAF/uncombinedMAF/%sUpdate.txt'\n",
    "with open(F2,'w') as F:\n",
    "    F.write(\"importing file \\\\n\")\n",
    "F.close()\n",
    "\n",
    "impfile = pd.read_csv('/home/lindb/eckertlab/wbp/bayenv2/UnbinnedImputedSNPSFILE.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "loci = np.unique(impfile.index).tolist()\n",
    "\n",
    "with open(F2,'a') as F:\n",
    "    F.write(\"creating dictionary \\\\n\")\n",
    "F.close()\n",
    "\n",
    "freqDict = OrderedDict()\n",
    "loccount = 0\n",
    "for locus in sorted(loci):\n",
    "    first = impfile.loc[locus,'%s'][0]\n",
    "    second = impfile.loc[locus,'%s'][1]\n",
    "\n",
    "    minor = min(first,second)\n",
    "    freq = minor/(first+second)\n",
    "\n",
    "    freqDict[locus] = freq\n",
    "\n",
    "    loccount += 1\n",
    "    if loccount %% 1000 == 0:\n",
    "        with open(F2,'a') as F:\n",
    "            F.write(\"%%s\\\\n\" %% loccount)\n",
    "        F.close()\n",
    "\n",
    "\n",
    "with open(F2,'a') as F:\n",
    "    F.write(\"writing file \\\\n\")\n",
    "F.close()\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/pimass/imputed/MAF/uncombinedMAF/imputed_%s.txt'\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\\\t'.join(['locus','minor freq']) + str('\\\\n')\n",
    "    o.write(line)\n",
    "    for locus in freqDict.keys():\n",
    "        line = '\\\\t'.join([locus,str(freqDict[locus])]) + str('\\\\n')\n",
    "        o.write(line)\n",
    "o.close()\n",
    "\n",
    "''' % (pop,pop,pop,pop)\n",
    "    filE = '/home/lindb/wbp/covariances/pimass/imputed/MAF/uncombinedMAF/runfiles/imputed_%s.py' % pop\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(text)\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = os.listdir('/home/lindb/eckertlab/wbp/covariances/imputed')\n",
    "count = 0\n",
    "for f in files:\n",
    "    if f.startswith('imp') and f.endswith('py'):\n",
    "        count += 1\n",
    "        text = '''#!/bin/bash\n",
    "#$ -N imp%s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cd %s\n",
    "python %s\n",
    "''' % (str(count),\n",
    "      '/home/lindb/eckertlab/wbp/covariances/imputed',\n",
    "      os.path.join('/home/lindb/eckertlab/wbp/covariances/imputed',f))\n",
    "        filE = '/home/lindb/eckertlab/wbp/covariances/imputed/%s' % (f.split(\".\")[0] + '.sh')\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(text)\n",
    "        o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#figure out the SNPs with PIPs in the 99.95th percentile\n",
    "#I did all of this in the \"covariances missing.ipynb\"\n",
    "files = os.listdir('/home/lindb/eckertlab/wbp/covariances/missing')\n",
    "count = 0\n",
    "#for f in files: #commented so it won't run by accident\n",
    "    if f.startswith('mis') and f.endswith('py'):\n",
    "        count += 1\n",
    "        text = '''#!/bin/bash\n",
    "#$ -N mis%s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cd %s\n",
    "python %s\n",
    "''' % (str(count),\n",
    "      '/home/lindb/eckertlab/wbp/covariances/missing',\n",
    "      os.path.join('/home/lindb/eckertlab/wbp/covariances/missing',f))\n",
    "        filE = '/home/lindb/eckertlab/wbp/covariances/missing/%s' % (f.split(\".\")[0] + '.sh')\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(text)\n",
    "        o.close()\n",
    "hmean = OrderedDict()\n",
    "hmean['imp'] = OrderedDict()\n",
    "hmean['mis'] = OrderedDict()\n",
    "#for root,dirs,files in os.walk('/home/lindb/eckertlab/wbp/piMASS/analyses2/1hmean/'):\n",
    "    for f in files:\n",
    "        print f\n",
    "        if 'imp' in f:\n",
    "            splits = f.split(\"_\")\n",
    "            phen = splits[1]\n",
    "            hmean['imp'][phen] = pd.read_csv(os.path.join(root,f),sep=\"\\t\", header=0)\n",
    "        if 'mis' in f:\n",
    "            splits = f.split(\"_\")\n",
    "            phen = splits[1]\n",
    "            hmean['mis'][phen] = pd.read_csv(os.path.join(root,f),sep=\"\\t\", header=0)\n",
    "def percent_difference(x, y):\n",
    "    x = float(x)\n",
    "    y = float(y)\n",
    "    return (np.abs(x-y)/np.mean([x, y]))*100\n",
    "\n",
    "def get_quantile_max(name, data, q):\n",
    "    d = data.quantile(q)\n",
    "    d.index = [str(x) for x in d.index]\n",
    "    d['median_val'] = data.median()\n",
    "    d['mean_val'] = data.mean()\n",
    "    d['cutoff'] = 0.01\n",
    "    #added start\n",
    "    d['x995_cutoff'] = percent_difference(d['0.995'],d['cutoff'])\n",
    "    d['x995_median'] = percent_difference(d['0.995'],d['median_val'])\n",
    "    d['strinj_cutoff'] = d['0.995']\n",
    "\n",
    "    d['x9995_cutoff'] = percent_difference(d['0.9995'],d['cutoff'])\n",
    "    d['x9995_median'] = percent_difference(d['0.9995'],d['median_val'])\n",
    "    d['extra_strinj_cutoff'] = d['0.9995']\n",
    "    \n",
    "    #added end\n",
    "    d[\"x99_cutoff\"] = percent_difference(d['0.99'], d['cutoff'])\n",
    "    d[\"x99_median\"] =  percent_difference(d['0.99'], d['median_val'])\n",
    "    d[\"x95_cutoff\"] = percent_difference(d['0.95'], d['cutoff'])\n",
    "    d[\"x95_median\"] =  percent_difference(d['0.95'], d['median_val'])\n",
    "    d['relaxed_cutoff'] = d['0.99']\n",
    "    d['min'] = data.min()\n",
    "    d['max'] = data.max()\n",
    "    d.name = name\n",
    "    return d\n",
    "an_dir = '/home/lindb/eckertlab/wbp/piMASS/analyses2'\n",
    "quant = OrderedDict()\n",
    "#for dset in hmean.keys():\n",
    "    quant[dset] = OrderedDict()\n",
    "    for pheno in phenos:\n",
    "        quant[dset][pheno] = get_quantile_max(str(pheno),hmean[dset][pheno].postrb_hmean,[0.95,0.99,0.995,0.9995])\n",
    "        quant[dset][pheno].to_csv(os.path.join(an_dir + str('/2quant'),\"%s_%s_quantFINAL.txt\" % (x,pheno)),\n",
    "                                  header=True,index=False,sep=\"\\t\")\n",
    "quantDF2 = pd.DataFrame()\n",
    "#for dset in quant.keys():\n",
    "    for pheno in phenos:\n",
    "        test = pd.DataFrame(quant[dset][pheno])\n",
    "        test.columns = [\"%s_%s\" % (dset,str(x)) for x in test.columns]\n",
    "        quantDF2 = pd.concat([quantDF2,test],axis =1)\n",
    "quantDF2.to_csv(os.path.join(an_dir + str('/2quant'),'allquantFINAL.txt'),header=True,index=True,sep=\"\\t\")\n",
    "quantDF2.head()\n",
    "#print sig snps out to a file to access later\n",
    "strnj_snps = OrderedDict()\n",
    "#for dset in quant.keys():\n",
    "    strnj_snps[dset] = OrderedDict()\n",
    "    for pheno in phenos:\n",
    "        strnj_snps[dset][pheno] = hmean[dset][pheno][hmean[dset][pheno].postrb_hmean > \n",
    "                                                     quant[dset][pheno].extra_strinj_cutoff]\n",
    "        strnj_snps[dset][pheno].to_csv(os.path.join(an_dir + str('/7xstringent'),\"%s_%s_EXTRAstrinjSNPS.txt\" % \n",
    "                                                    (dset,pheno)),header=True,Index=True,sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of files\n",
    "DIR = '/home/lindb/wbp/covariances/pimass/imputed/MAF/uncombinedMAF/'\n",
    "files = os.listdir(DIR)\n",
    "files = [f for f in files if f.endswith('txt')]\n",
    "sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reformat files\n",
    "locDIR = '/home/lindb/wbp/covariances/pimass/imputed/MAF/uncombinedMAF/'\n",
    "mvDIR = '/home/lindb/wbp/covariances/pimass/imputed/MAF/'\n",
    "for f in files:\n",
    "    pop = f[8:-4]\n",
    "    f = os.path.join(locDIR,f)\n",
    "    df = pd.read_csv(f,header=0,sep=\"\\t\")\n",
    "    df.index = df['locus'].values.tolist()\n",
    "    df = pd.DataFrame(df['minor freq'])\n",
    "    df.to_csv(os.path.join(mvDIR,pop+'_MAF.txt'),header=False,index=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine files\n",
    "files = os.listdir(mvDIR)\n",
    "files = [f for f in files if 'MAF.' in f if 'combined' not in f]\n",
    "sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/covariances/pimass/imputed/MAF/'\n",
    "DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reformat and combine\n",
    "newdf = pd.DataFrame()\n",
    "count = 0\n",
    "for f in files:\n",
    "    pop = f[:-8]\n",
    "    filE = os.path.join(DIR,f)\n",
    "    df = pd.read_csv(filE,header=None,sep=\"\\t\")\n",
    "    df.index = df[0].values.tolist()\n",
    "    df = pd.DataFrame(df[1])\n",
    "    df.columns = [pop]\n",
    "    \n",
    "    if count == 0:\n",
    "        newdf = df\n",
    "    else:\n",
    "        newdf = pd.merge(newdf,df,left_index=True,right_index=True)\n",
    "    count += 1\n",
    "filE = os.path.join(DIR,'combined_imputed_MAF.txt')\n",
    "newdf.to_csv(filE,header=True,index=True,sep=\"\\t\")\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#only 6/8 populations had seedlings in common gardens, which measured phenotype\n",
    "pops = ['Little_Round_Top','Mt_Rose_Ophir','Snow_Valley_Peak','Rifle_Peak','Freel_Peak','Dicks_Pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newdf = pd.read_csv('/home/lindb/wbp/covariances/pimass/imputed/MAF/combined_imputed_MAF.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "newdf = newdf[pops]\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in newdf.columns:\n",
    "    print col,min(newdf[col].tolist()),max(newdf[col].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(newdf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of snps for each phenotype\n",
    "pDIR = '/home/lindb/wbp/piMASS/analyses2/7xstringent/'\n",
    "files = os.listdir(pDIR)\n",
    "files = sorted([f for f in files if f.startswith('imp')])\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snpDict = OrderedDict()\n",
    "for f in files:\n",
    "    pheno = f.split(\"_\")[1]\n",
    "    print pheno\n",
    "    df = pd.read_csv(os.path.join(pDIR,f),header=0,sep=\"\\t\")\n",
    "    snpDict[pheno] = df['rs'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#template for each phenotype\n",
    "d = OrderedDict()\n",
    "count = 0\n",
    "for i in range(10):\n",
    "    d[i] = OrderedDict()\n",
    "    for j in range(10):\n",
    "        if i > j: #i is row, j is col\n",
    "            d[i][j] = count # = value calculated\n",
    "            count += 1\n",
    "        else:\n",
    "            d[i][j] = np.nan\n",
    "rowcount = 0\n",
    "for row in d.keys():\n",
    "    if rowcount ==0:\n",
    "        print '\\t'+'\\t'.join([str(x) for x in range(len(d[row].values()))])\n",
    "    print str(rowcount)+'\\t'+'\\t'.join([str(x) for x in d[row].values()])\n",
    "    rowcount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phenos = ['bffamx',\n",
    " 'rspopx',\n",
    " 'c13famx',\n",
    " 'htpopx',\n",
    " 'c13popx',\n",
    " 'rsfamx',\n",
    " 'htfamx',\n",
    " 'bfpopx',\n",
    " 'n15popx',\n",
    " 'n15famx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do pairwise to get D\n",
    "dDict = OrderedDict()\n",
    "for pheno in phenos:\n",
    "    icount = 0\n",
    "    dDict[pheno] = OrderedDict()\n",
    "    for i,locusi in enumerate(snpDict[pheno]): # change pheno to %s\n",
    "        dDict[pheno][locusi] = OrderedDict()\n",
    "        for j,locusj in enumerate(snpDict[pheno]): # change pheno to %s\n",
    "            if i > j: # i=row,j=col : fill in lower triange of matrix\n",
    "                asums = 0\n",
    "                bsums = 0\n",
    "                csums = 0\n",
    "                for pop in newdf.columns:\n",
    "                    p_i = newdf.loc[locusi,pop]\n",
    "                    p_j = newdf.loc[locusj,pop]\n",
    "\n",
    "                    #calculate \"a\"\n",
    "                    product = p_i*p_j\n",
    "                    asums = asums + product\n",
    "\n",
    "                    #calculate \"b\"\n",
    "                    bsums = bsums + p_i\n",
    "\n",
    "                    #calculate \"c\"\n",
    "                    csums = csums + p_j    \n",
    "\n",
    "                a = asums/len(newdf.columns) #a = average product of f(locus_i) and f(locus_j) across pops\n",
    "                b = bsums/len(newdf.columns) #b = average allele freq of locus_i across populations\n",
    "                c = csums/len(newdf.columns) #c = average allele freq of locus_j across populations\n",
    "\n",
    "                d = a - (b*c) #covariance\n",
    "                dDict[pheno][locusi][locusj] = d\n",
    "            else: #fill in upper triangle of matrix\n",
    "                dDict[pheno][locusi][locusj] = np.nan \n",
    "        icount += 1\n",
    "        if icount % 10 == 0:\n",
    "            print pheno,icount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pheno in dDict.keys():\n",
    "    rowcount = 0\n",
    "    filE = '/home/lindb/wbp/covariances/imputed/%s_imputed.txt' % pheno\n",
    "    with open(filE,'w') as o:\n",
    "        key0 = dDict[pheno].keys()[0]\n",
    "        line = '\\t'.join(dDict[pheno][key0].keys()) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "        for locusi in dDict[pheno].keys():\n",
    "            line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dDict[pheno][locusi].values()]) + str('\\n')\n",
    "            o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setup_r(): ####/gpfs_fs/home/lindb/anaconda/envs/conda/lib/R ####/home/lindb/g/R3/lib64/R/\n",
    "    os.environ['R_HOME'] = '/home/lindb/g/R3/lib64/R/' \n",
    "    os.environ['LD_LIBRARY_PATH'] = \"%s/lib:%s\" % (os.environ['R_HOME'], ####/home/lindb/g/R3/lib64/R/bin/R\n",
    "                                                   os.environ['LD_LIBRARY_PATH'])\n",
    "setup_r()\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri as pd2ri\n",
    "pd2ri.activate()\n",
    "r = robjects.r\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext rpy2.ipython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "n15famx = read.csv('/home/lindb/wbp/covariances/imputed/n15famx_imputed.txt',header=TRUE,sep=\"\\t\")\n",
    "nums = n15famx[lower.tri(n15famx)]\n",
    "hist(nums,breaks=80,main=\"imputed n15famx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos = ['bffamx',\n",
    " 'rspopx',\n",
    " 'c13famx',\n",
    " 'htpopx',\n",
    " 'c13popx',\n",
    " 'rsfamx',\n",
    " 'htfamx',\n",
    " 'bfpopx',\n",
    " 'n15popx',\n",
    " 'n15famx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(phenos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "rspopx = read.csv('/home/lindb/wbp/covariances/imputed/rspopx_imputed.txt',header=TRUE,sep=\"\\t\")\n",
    "nums = rspopx[lower.tri(rspopx)]\n",
    "hist(nums,breaks=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n15famx = pd.read_csv('/home/lindb/wbp/covariances/imputed/n15famx_imputed.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "n15famx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [x for x in n15famx.unstack().tolist() if math.isnan(x) == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#random set of SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/piMASS/IntersectionRowNames.txt'\n",
    "intersection = pd.read_csv(filE,header=None,sep=\"\\t\")\n",
    "intersection = [x for x in intersection[1]]\n",
    "intersection[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bDict = OrderedDict()\n",
    "for pheno in phenos:\n",
    "    bDict[pheno] = list(set(intersection) - set(snpDict[pheno]))\n",
    "    df = pd.DataFrame(bDict[pheno])\n",
    "    filE = '/home/lindb/wbp/covariances/pimass/imputed/drawbuckets/%s_bucket.txt' % pheno\n",
    "    df.to_csv(filE,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bDict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make 1000 snp sets of d-values for random SNPs to compare with sig set\n",
    "for pheno in phenos:                                     #each phenotype gets 20 .py files (200 .sh files total)\n",
    "    snpBucket = bDict[pheno]                             #the snps that can be drawn\n",
    "    for i in range(20):                                  #make 20 .py files\n",
    "        for j in range(50):                              #each .py file makes 50 matrices\n",
    "            snps = random.sample(snpBucket,80)           #select random snps\n",
    "\n",
    "            DIR = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/%s' % pheno\n",
    "            if not os.path.exists(DIR):\n",
    "                os.makedirs(DIR)\n",
    "            filE = os.path.join(DIR,\"%s_%s_%s_imputed.txt\" % (pheno,str(i).zfill(2),str(j).zfill(2)))\n",
    "            df = pd.DataFrame(snps)\n",
    "            df.to_csv(filE,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'/home/lindb/wbp/covariances/pimass/imputed/randmatrices/htfamx/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make .py files\n",
    "for pheno in phenos:\n",
    "    for i in range(20):\n",
    "        for j in range(50):\n",
    "            text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "filE= '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/%s/%s_%s_%s_imputed.txt'\n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "snps = df[1].tolist()\n",
    "\n",
    "newdf = pd.read_csv('/home/lindb/wbp/covariances/pimass/imputed/MAF/combined_imputed_MAF.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "icount = 0\n",
    "rDict = OrderedDict()\n",
    "for i,locusi in enumerate(snps):\n",
    "    rDict[locusi] = OrderedDict()\n",
    "    for j,locusj in enumerate(snps):\n",
    "        if i > j: #i=row, j=col : lower tri\n",
    "            asums = 0\n",
    "            bsums = 0\n",
    "            csums = 0\n",
    "            for pop in newdf.columns:\n",
    "                p_i = newdf.loc[locusi,pop]\n",
    "                p_j = newdf.loc[locusj,pop]\n",
    "                \n",
    "                #calc \"a\"\n",
    "                product = p_i*p_j\n",
    "                asums = asums + product\n",
    "                \n",
    "                #calc \"b\"\n",
    "                bsums = bsums + p_i\n",
    "                \n",
    "                #calc \"c\"\n",
    "                csums = csums + p_j\n",
    "            \n",
    "            a = asums/len(newdf.columns)\n",
    "            b = bsums/len(newdf.columns)\n",
    "            c = csums/len(newdf.columns)\n",
    "            \n",
    "            d = a - (b*c)\n",
    "            rDict[locusi][locusj] = d\n",
    "        else:\n",
    "            rDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount %% 10 == 0:\n",
    "        print icount\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/0outfiles/%s_%s_%s_imputedDVALS.txt'\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\\\t'.join(snps) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in rDict.keys():\n",
    "        line = str(locusi) + '\\\\t' + '\\\\t'.join([str(x) for x in rDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "    \n",
    "''' % (pheno,pheno,str(i).zfill(2),str(j).zfill(2), \n",
    "      pheno,str(i).zfill(2),str(j).zfill(2))\n",
    "            filE = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/0pyfiles/%s_%s_%s_imputed.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(2),\n",
    "                                                                                                      str(j).zfill(2))\n",
    "            with open(filE,'w') as o:\n",
    "                o.write(text)\n",
    "            o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make .sh files\n",
    "for pheno in phenos:\n",
    "    for i in range(20):\n",
    "        text = '''#!/bin/bash\n",
    "#$ -N %s_%s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "''' % (pheno,i)\n",
    "        for j in range(50):\n",
    "            new = '''\n",
    "cd %s\n",
    "python %s\n",
    "''' % ('/home/lindb/wbp/covariances/imputed/randmatrices/0pyfiles/',\n",
    "      '%s_%s_%s_imputed.py' % (pheno,str(i).zfill(2),str(j).zfill(2)))\n",
    "            text = text + new\n",
    "        filE = '/home/lindb/wbp/covariances/imputed/randmatrices/0runfiles/%s_%s.sh' % (pheno,i)\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)\n",
    "        o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('/home/lindb/wbp/covariances/imputed/randmatrices/0outfiles/')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#KS tests - true SNPs vs rand SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import anderson_ksamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pheno in phenos:\n",
    "    text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import anderson_ksamp\n",
    "\n",
    "#get observed dvals\n",
    "DF = pd.read_csv('/home/lindb/wbp/covariances/pimass/imputed/Dvals/%s_imputed.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "dvals = []\n",
    "for lst in DF.values.tolist():\n",
    "    for x in lst:\n",
    "        if math.isnan(x) == False:\n",
    "            dvals.append(x)\n",
    "            \n",
    "DIR = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/0outfiles/'\n",
    "files = os.listdir(DIR)\n",
    "files = [f for f in files if f.startswith('%s')]\n",
    "\n",
    "pvals = []\n",
    "fcount = 0\n",
    "for f in files:\n",
    "    df = pd.read_csv(os.path.join(DIR,f),header=0,index_col=0,sep=\"\\\\t\")\n",
    "    rvals = []\n",
    "    for lst in df.values.tolist():\n",
    "        for x in lst:\n",
    "            if math.isnan(x) == False:\n",
    "                rvals.append(x)\n",
    "    \n",
    "    pvals.append(ks_2samp(rvals,dvals)[1])\n",
    "    fcount += 1\n",
    "    if fcount %% 10 == 0:\n",
    "        print fcount\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/0OBSpvals/%s_observedpvalues.txt'\n",
    "pvals = pd.DataFrame(pvals)\n",
    "pvals.to_csv(filE,header=None,index=False,sep=\"\\\\t\")\n",
    "\n",
    "''' % (pheno,pheno,pheno)\n",
    "    filE = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/1OBSpvalrunfiles/%s_pvalscript.py' % pheno\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('/home/lindb/wbp/covariances/pimass/imputed/randmatrices/1OBSpvalrunfiles/')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    text ='''#!/bin/bash\n",
    "#$ -N imp%s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cd /home/lindb/wbp/covariances/pimass/imputed/randmatrices/1OBSpvalrunfiles/\n",
    "python %s\n",
    "''' % (f.split(\"_\")[0],f)\n",
    "    filE = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/1OBSpvalrunfiles/%s.sh' % f.split(\".\")[0]\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(text)\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#KS tests - rand SNPs vs rand SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(bDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make 1000 snp sets of d-values for random SNPs to compare with 1000 rand below\n",
    "for pheno in phenos:                                     #each phenotype gets 20 .py files (200 .sh files total)\n",
    "    snpBucket = bDict[pheno]                             #the snps that can be drawn\n",
    "    for i in range(20):                                  #make 20 .py files\n",
    "        for j in range(50):                              #each .py file makes 50 matrices\n",
    "            snps = random.sample(snpBucket,len(snpDict[pheno]))           #select random snps\n",
    "\n",
    "            DIR = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/%s' % pheno\n",
    "            if not os.path.exists(DIR):\n",
    "                os.makedirs(DIR)\n",
    "            filE = os.path.join(DIR,\"%s_%s_%s_imputed.txt\" % (pheno,str(i).zfill(2),str(j).zfill(2)))\n",
    "            df = pd.DataFrame(snps)\n",
    "            df.to_csv(filE,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in snpDict.keys():\n",
    "    print pheno,len(snpDict[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a second set of 1000 snp sets of d-values for random SNPs to compare with first random 1000 above\n",
    "for pheno in phenos:                                     #each phenotype gets 20 .py files (200 .sh files total)\n",
    "    snpBucket = bDict[pheno]                             #the snps that can be drawn\n",
    "    for i in range(20):                                  #make 20 .py files\n",
    "        for j in range(50):                              #each .py file makes 50 matrices\n",
    "            snps = random.sample(snpBucket,len(snpDict[pheno]))           #select random snps\n",
    "\n",
    "            DIR = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/%s2' % pheno\n",
    "            if not os.path.exists(DIR):\n",
    "                os.makedirs(DIR)\n",
    "            filE = os.path.join(DIR,\"%s_%s_%s_imputed.txt\" % (pheno,str(i).zfill(2),str(j).zfill(2)))\n",
    "            df = pd.DataFrame(snps)\n",
    "            df.to_csv(filE,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make .py files to get dvals\n",
    "for pheno in phenos:\n",
    "    for i in range(20):\n",
    "        for j in range(50):\n",
    "            text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "filE= '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/%s/%s_%s_%s_imputed.txt'\n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "snps = df[1].tolist()\n",
    "\n",
    "newdf = pd.read_csv('/home/lindb/wbp/covariances/pimass/imputed/MAF/combined_imputed_MAF.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "icount = 0\n",
    "rDict = OrderedDict()\n",
    "for i,locusi in enumerate(snps):\n",
    "    rDict[locusi] = OrderedDict()\n",
    "    for j,locusj in enumerate(snps):\n",
    "        if i > j: #i=row, j=col : lower tri\n",
    "            asums = 0\n",
    "            bsums = 0\n",
    "            csums = 0\n",
    "            for pop in newdf.columns:\n",
    "                p_i = newdf.loc[locusi,pop]\n",
    "                p_j = newdf.loc[locusj,pop]\n",
    "                \n",
    "                #calc \"a\"\n",
    "                product = p_i*p_j\n",
    "                asums = asums + product\n",
    "                \n",
    "                #calc \"b\"\n",
    "                bsums = bsums + p_i\n",
    "                \n",
    "                #calc \"c\"\n",
    "                csums = csums + p_j\n",
    "            \n",
    "            a = asums/len(newdf.columns)\n",
    "            b = bsums/len(newdf.columns)\n",
    "            c = csums/len(newdf.columns)\n",
    "            \n",
    "            d = a - (b*c)\n",
    "            rDict[locusi][locusj] = d\n",
    "        else:\n",
    "            rDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount %% 10 == 0:\n",
    "        print icount\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/0outfiles/%s_%s_%s_imputedDVALS.txt'\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\\\t'.join(snps) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in rDict.keys():\n",
    "        line = str(locusi) + '\\\\t' + '\\\\t'.join([str(x) for x in rDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "\n",
    "#get dvals for second set\n",
    "filE= '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/%s2/%s_%s_%s_imputed.txt'\n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "snps = df[1].tolist()\n",
    "\n",
    "icount = 0\n",
    "rDict = OrderedDict()\n",
    "for i,locusi in enumerate(snps):\n",
    "    rDict[locusi] = OrderedDict()\n",
    "    for j,locusj in enumerate(snps):\n",
    "        if i > j: #i=row, j=col : lower tri\n",
    "            asums = 0\n",
    "            bsums = 0\n",
    "            csums = 0\n",
    "            for pop in newdf.columns:\n",
    "                p_i = newdf.loc[locusi,pop]\n",
    "                p_j = newdf.loc[locusj,pop]\n",
    "                \n",
    "                #calc \"a\"\n",
    "                product = p_i*p_j\n",
    "                asums = asums + product\n",
    "                \n",
    "                #calc \"b\"\n",
    "                bsums = bsums + p_i\n",
    "                \n",
    "                #calc \"c\"\n",
    "                csums = csums + p_j\n",
    "            \n",
    "            a = asums/len(newdf.columns)\n",
    "            b = bsums/len(newdf.columns)\n",
    "            c = csums/len(newdf.columns)\n",
    "            \n",
    "            d = a - (b*c)\n",
    "            rDict[locusi][locusj] = d\n",
    "        else:\n",
    "            rDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount %% 10 == 0:\n",
    "        print icount\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/0outfiles2/%s_%s_%s_imputedDVALS.txt'\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\\\t'.join(snps) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in rDict.keys():\n",
    "        line = str(locusi) + '\\\\t' + '\\\\t'.join([str(x) for x in rDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "    \n",
    "''' % (pheno,pheno,str(i).zfill(2),str(j).zfill(2), \n",
    "       pheno,str(i).zfill(2),str(j).zfill(2),\n",
    "       pheno,pheno,str(i).zfill(2),str(j).zfill(2),\n",
    "       pheno,str(i).zfill(2),str(j).zfill(2))\n",
    "            filE = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/0pyfiles/%s_%s_%s_imputed.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(2),\n",
    "                                                                                                      str(j).zfill(2))\n",
    "            with open(filE,'w') as o:\n",
    "                o.write(text)\n",
    "            o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "i created .sh files in the 'covariances missing.ipynb' to include imputed.py files created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get pvals\n",
    "for pheno in phenos:\n",
    "    text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import anderson_ksamp\n",
    "\n",
    "pvals = []\n",
    "for i in range(20):\n",
    "    for j in range(50):\n",
    "\n",
    "        F1 = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/0outfiles/%s_%%s_%%s_imputedDVALS.txt' %% (str(i).zfill(2),str(j).zfill(2))\n",
    "        F2 = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/0outfiles2/%s_%%s_%%s_imputedDVALS.txt' %% (str(i).zfill(2),str(j).zfill(2))\n",
    "\n",
    "        df1 = pd.read_csv(F1, header=0,index_col=0,sep=\"\\\\t\")\n",
    "        df2 = pd.read_csv(F2, header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "        dvals1 = []\n",
    "        for lst in df1.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    dvals1.append(x)\n",
    "        dvals2 = []\n",
    "        for lst in df2.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    dvals2.append(x)\n",
    "\n",
    "        pvals.append(ks_2samp(dvals1,dvals2)[1])\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/0randpvals/%s_randpvalues.txt'\n",
    "pvals = pd.DataFrame(pvals)\n",
    "pvals.to_csv(filE,header=None,index=False,sep=\"\\\\t\")\n",
    "\n",
    "''' % (pheno,\n",
    "       pheno,\n",
    "       pheno)\n",
    "    filE = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/1RANDpvalrunfiles/%s_pvalscript.py' % pheno\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/1RANDpvalrunfiles/'\n",
    "files = os.listdir(DIR)\n",
    "for f in files:\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N imp%s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cd /home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/1RANDpvalrunfiles/\n",
    "python %s\n",
    "''' % (f.split(\".\")[0],f)\n",
    "    filE = os.path.join(DIR,f.split(\".\")[0]+'.sh')\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#mann whitneytests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for pheno in phenos:\n",
    "    filE1 = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/0randpvals/%s_randpvalues.txt' % pheno\n",
    "    filE2 = '/home/lindb/wbp/covariances/pimass/imputed/randmatrices/0OBSpvals/%s_observedpvalues.txt' % pheno\n",
    "    \n",
    "    obs = pd.read_csv(filE2,header=None,sep=\"\\t\")\n",
    "    rand = pd.read_csv(filE1,header=None,sep=\"\\t\")\n",
    "    \n",
    "    pval = mannwhitneyu(obs,rand)\n",
    "    print pheno,pval\n",
    "    \n",
    "    newtext = \"%s\\t%s\\n\" % (pheno,pval)\n",
    "    text = text + newtext\n",
    "filE = '/home/lindb/wbp/covariances/pimass/results/imputed.txt'\n",
    "with open(filE,'w') as o:\n",
    "    o.write('pheno\\t(statistic,p-val)\\n')\n",
    "    o.write(text)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get minor allele frequencies for each population\n",
    "#impMAF made in 'pimass covariances imputed.ipynb'\n",
    "filE = '/home/lindb/wbp/covariances/pimass/imputed/MAF/combined_imputed_MAF.txt'\n",
    "impMAF = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "impMAF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pop in impMAF.columns:\n",
    "    print min(impMAF[pop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/bayenv2/Final/imputed/'\n",
    "files = os.listdir(DIR)\n",
    "files = [f for f in files if not f.startswith('imputed')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get sig snps (with BF >= 5)\n",
    "snpDict = OrderedDict()\n",
    "for f in files:\n",
    "    df = pd.read_csv(os.path.join(DIR,f),header=0,sep=\"\\t\")\n",
    "    env = f.split(\"_\")[0]\n",
    "    snpDict[env] = df[env].tolist()\n",
    "    print env, len(snpDict[env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise to get D\n",
    "dDict = OrderedDict()\n",
    "for env in snpDict.keys():\n",
    "    icount = 0\n",
    "    dDict[env] = OrderedDict()\n",
    "    for i,locusi in enumerate(snpDict[env]):\n",
    "        dDict[env][locusi] = OrderedDict()\n",
    "        for j,locusj in enumerate(snpDict[env]):\n",
    "            if i > j: #i=row, j=col : lower triangle\n",
    "                asums = 0\n",
    "                bsums = 0\n",
    "                csums = 0\n",
    "                for pop in impMAF.columns:\n",
    "                    p_i = impMAF.loc[locusi,pop]\n",
    "                    p_j = impMAF.loc[locusj,pop]\n",
    "                    \n",
    "                    #calc \"a\"\n",
    "                    product = p_i*p_j\n",
    "                    asums = asums + product\n",
    "                    \n",
    "                    #calc \"b\"\n",
    "                    bsums = bsums + p_i\n",
    "                    \n",
    "                    #calc \"c\" \n",
    "                    csums = csums + p_j\n",
    "                    \n",
    "                a = asums/len(impMAF.columns)\n",
    "                b = bsums/len(impMAF.columns)\n",
    "                c = csums/len(impMAF.columns)\n",
    "                \n",
    "                d = a - (b*c)\n",
    "                dDict[env][locusi][locusj] = d\n",
    "            else:\n",
    "                dDict[env][locusi][locusj] = np.nan\n",
    "        icount += 1\n",
    "        if icount % 10 == 0:\n",
    "            print env,icount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for env in dDict.keys():\n",
    "    rowcount = 0\n",
    "    filE = '/home/lindb/wbp/covariances/bayenv2/imputed/dvals/%s_imputed_dvals.txt' % env\n",
    "    with open(filE,'w') as o:\n",
    "        key0 = dDict[env].keys()[0]\n",
    "        line = '\\t'.join(dDict[env][key0].keys()) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "        for locusi in dDict[env].keys():\n",
    "            line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dDict[env][locusi].values()]) + str('\\n')\n",
    "            o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check one of the files\n",
    "df = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Random set of SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/piMASS/IntersectionRowNames.txt'\n",
    "intersection = pd.read_csv(filE,header=None,sep=\"\\t\")\n",
    "intersection = [x for x in intersection[1]]\n",
    "intersection[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of SNPs that aren't significant\n",
    "bDict = OrderedDict()\n",
    "for env in snpDict.keys():\n",
    "    bDict[env] = list(set(intersection) - set(snpDict[env]))\n",
    "    df = pd.DataFrame(bDict[env])\n",
    "    filE = '/home/lindb/wbp/covariances/bayenv2/imputed/drawbuckets/%s_bucket.txt' % env\n",
    "    df.to_csv(filE,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in case I have to restart my notebook\n",
    "bDict = OrderedDict()\n",
    "files = os.listdir('/home/lindb/wbp/covariances/bayenv2/imputed/drawbuckets/')\n",
    "for f in files:\n",
    "    env = f.split(\"_\")[0]\n",
    "    print env\n",
    "    df = pd.read_csv(os.path.join('/home/lindb/wbp/covariances/bayenv2/imputed/drawbuckets/',f),header=None,sep=\"\\t\")\n",
    "    bDict[env] = pd.DataFrame(df[1].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in bDict.keys():\n",
    "    print env,len(bDict[env]),len(snpDict[env]),len(bDict[env])+len(snpDict[env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make 1000 matrices of d-values for random SNPs to compare with sig set\n",
    "for env in snpDict.keys():                                     #each phenotype gets 20 .py files (200 .sh files total)\n",
    "    print env\n",
    "    snpBucket = bDict[env]                             #the snps that can be drawn\n",
    "    for i in range(20):                                  #make 20 .py files\n",
    "        for j in range(50):                              #each .py file makes 50 matrices\n",
    "            snps = random.sample(snpBucket,len(snpDict[env]))           #select random snps\n",
    "\n",
    "            DIR = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/%s' % env\n",
    "            if not os.path.exists(DIR):\n",
    "                os.makedirs(DIR)\n",
    "            filE = os.path.join(DIR,\"%s_%s_%s_imputed.txt\" % (env,str(i).zfill(2),str(j).zfill(2)))\n",
    "            df = pd.DataFrame(snps)\n",
    "            df.to_csv(filE,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dirs = os.listdir('/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices')\n",
    "len(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in dirs:\n",
    "    files = os.listdir(os.path.join('/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices',d))\n",
    "    print d,len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make .py files\n",
    "for env in snpDict.keys():\n",
    "    for i in range(20):\n",
    "        for j in range(50):\n",
    "            text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "filE= '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/%s/%s_%s_%s_imputed.txt'\n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "snps = df[1].tolist()\n",
    "\n",
    "newdf = pd.read_csv('/home/lindb/wbp/covariances/pimass/imputed/MAF/combined_imputed_MAF.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "icount = 0\n",
    "rDict = OrderedDict()\n",
    "for i,locusi in enumerate(snps):\n",
    "    rDict[locusi] = OrderedDict()\n",
    "    for j,locusj in enumerate(snps):\n",
    "        if i > j: #i=row, j=col : lower tri\n",
    "            asums = 0\n",
    "            bsums = 0\n",
    "            csums = 0\n",
    "            for pop in newdf.columns:\n",
    "                p_i = newdf.loc[locusi,pop]\n",
    "                p_j = newdf.loc[locusj,pop]\n",
    "                \n",
    "                #calc \"a\"\n",
    "                product = p_i*p_j\n",
    "                asums = asums + product\n",
    "                \n",
    "                #calc \"b\"\n",
    "                bsums = bsums + p_i\n",
    "                \n",
    "                #calc \"c\"\n",
    "                csums = csums + p_j\n",
    "            \n",
    "            a = asums/len(newdf.columns)\n",
    "            b = bsums/len(newdf.columns)\n",
    "            c = csums/len(newdf.columns)\n",
    "            \n",
    "            d = a - (b*c)\n",
    "            rDict[locusi][locusj] = d\n",
    "        else:\n",
    "            rDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount %% 10 == 0:\n",
    "        print icount\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/0outfiles/%s_%s_%s_imputedDVALS.txt'\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\\\t'.join(snps) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in rDict.keys():\n",
    "        line = str(locusi) + '\\\\t' + '\\\\t'.join([str(x) for x in rDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "    \n",
    "''' % (env,env,str(i).zfill(2),str(j).zfill(2), \n",
    "      env,str(i).zfill(2),str(j).zfill(2))\n",
    "            filE = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/0pyfiles/%s_%s_%s_imputed.py' % (env,\n",
    "                                                                                                      str(i).zfill(2),\n",
    "                                                                                                      str(j).zfill(2))\n",
    "            with open(filE,'w') as o:\n",
    "                o.write(text)\n",
    "            o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I made .sh files in 'bayenv covariance missing.ipynb' to get all imputed pyfiles too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#KS tests - true SNPs vs rand SNPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/0outfiles/')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envs = []\n",
    "for f in sorted(files):\n",
    "    env = f.split(\"_\")[0]\n",
    "    if env not in envs:\n",
    "        envs.append(env)\n",
    "for env in envs:\n",
    "    print env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for env in envs:\n",
    "    text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import anderson_ksamp\n",
    "\n",
    "#get observed dvals\n",
    "DF = pd.read_csv('/home/lindb/wbp/covariances/bayenv2/imputed/dvals/%s_imputed_dvals.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "dvals = []\n",
    "for lst in DF.values.tolist():\n",
    "    for x in lst:\n",
    "        if math.isnan(x) == False:\n",
    "            dvals.append(x)\n",
    "            \n",
    "DIR = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/0outfiles'\n",
    "files = os.listdir(DIR)\n",
    "files = [f for f in files if f.startswith('%s')]\n",
    "\n",
    "pvals = []\n",
    "fcount = 0\n",
    "for f in files:\n",
    "    df = pd.read_csv(os.path.join(DIR,f),header=0,index_col=0,sep=\"\\\\t\")\n",
    "    rvals = []\n",
    "    for lst in df.values.tolist():\n",
    "        for x in lst:\n",
    "            if math.isnan(x) == False:\n",
    "                rvals.append(x)\n",
    "    \n",
    "    pvals.append(ks_2samp(rvals,dvals)[1])\n",
    "    fcount += 1\n",
    "    if fcount %% 10 == 0:\n",
    "        print fcount\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/0OBSpvals/%s_imputed_observedpvalues.txt'\n",
    "pvals = pd.DataFrame(pvals)\n",
    "pvals.to_csv(filE,header=None,index=False,sep=\"\\\\t\")\n",
    "\n",
    "''' % (env,env,env)\n",
    "    filE = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/1OBSpvalrunfiles/%s_imputed_pvalscript.py' % env\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I made .sh files to run imp.py files in 'bayenv covariances missing.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#KS tests - rand SNPs vs rand SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in snpDict.keys():\n",
    "    print env,len(snpDict[env]),len(bDict[env]),len(snpDict[env])+len(bDict[env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make 1000 snp sets of random SNPs to compare with 1000 rand below\n",
    "for env in envs:                                     #each phenotype gets 20 .py files (200 .sh files total)\n",
    "    snpBucket = bDict[env]                             #the snps that can be drawn\n",
    "    for i in range(20):                                  #make 20 .py files\n",
    "        for j in range(50):                              #each .py file makes 50 matrices\n",
    "            snps = random.sample(snpBucket,len(snpDict[env]))           #select random snps\n",
    "\n",
    "            DIR = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/%s' % env\n",
    "            if not os.path.exists(DIR):\n",
    "                os.makedirs(DIR)\n",
    "            filE = os.path.join(DIR,\"%s_%s_%s_imputed.txt\" % (env,str(i).zfill(2),str(j).zfill(2)))\n",
    "            df = pd.DataFrame(snps)\n",
    "            df.to_csv(filE,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a second set of 1000 snp sets of random SNPs to compare with first random 1000 above\n",
    "for env in envs:                                     #each phenotype gets 20 .py files (200 .sh files total)\n",
    "    snpBucket = bDict[env]                             #the snps that can be drawn\n",
    "    for i in range(20):                                  #make 20 .py files\n",
    "        for j in range(50):                              #each .py file makes 50 matrices\n",
    "            snps = random.sample(snpBucket,len(snpDict[env]))           #select random snps\n",
    "\n",
    "            DIR = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/%s2' % env\n",
    "            if not os.path.exists(DIR):\n",
    "                os.makedirs(DIR)\n",
    "            filE = os.path.join(DIR,\"%s_%s_%s_imputed.txt\" % (env,str(i).zfill(2),str(j).zfill(2)))\n",
    "            df = pd.DataFrame(snps)\n",
    "            df.to_csv(filE,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make .py files to get dvals from both sets of 1000 snps created above\n",
    "for env in envs:\n",
    "    for i in range(20):\n",
    "        for j in range(50):\n",
    "            text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "filE= '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/%s/%s_%s_%s_imputed.txt'\n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "snps = df[1].tolist()\n",
    "\n",
    "newdf = pd.read_csv('/home/lindb/wbp/covariances/pimass/imputed/MAF/combined_imputed_MAF.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "icount = 0\n",
    "rDict = OrderedDict()\n",
    "for i,locusi in enumerate(snps):\n",
    "    rDict[locusi] = OrderedDict()\n",
    "    for j,locusj in enumerate(snps):\n",
    "        if i > j: #i=row, j=col : lower tri\n",
    "            asums = 0\n",
    "            bsums = 0\n",
    "            csums = 0\n",
    "            for pop in newdf.columns:\n",
    "                p_i = newdf.loc[locusi,pop]\n",
    "                p_j = newdf.loc[locusj,pop]\n",
    "                \n",
    "                #calc \"a\"\n",
    "                product = p_i*p_j\n",
    "                asums = asums + product\n",
    "                \n",
    "                #calc \"b\"\n",
    "                bsums = bsums + p_i\n",
    "                \n",
    "                #calc \"c\"\n",
    "                csums = csums + p_j\n",
    "            \n",
    "            a = asums/len(newdf.columns)\n",
    "            b = bsums/len(newdf.columns)\n",
    "            c = csums/len(newdf.columns)\n",
    "            \n",
    "            d = a - (b*c)\n",
    "            rDict[locusi][locusj] = d\n",
    "        else:\n",
    "            rDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount %% 10 == 0:\n",
    "        print icount\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/0outfiles/%s_%s_%s_imputedDVALS.txt'\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\\\t'.join(snps) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in rDict.keys():\n",
    "        line = str(locusi) + '\\\\t' + '\\\\t'.join([str(x) for x in rDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "\n",
    "#get dvals for second set\n",
    "filE= '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/%s2/%s_%s_%s_imputed.txt'\n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "snps = df[1].tolist()\n",
    "\n",
    "icount = 0\n",
    "rDict = OrderedDict()\n",
    "for i,locusi in enumerate(snps):\n",
    "    rDict[locusi] = OrderedDict()\n",
    "    for j,locusj in enumerate(snps):\n",
    "        if i > j: #i=row, j=col : lower tri\n",
    "            asums = 0\n",
    "            bsums = 0\n",
    "            csums = 0\n",
    "            for pop in newdf.columns:\n",
    "                p_i = newdf.loc[locusi,pop]\n",
    "                p_j = newdf.loc[locusj,pop]\n",
    "                \n",
    "                #calc \"a\"\n",
    "                product = p_i*p_j\n",
    "                asums = asums + product\n",
    "                \n",
    "                #calc \"b\"\n",
    "                bsums = bsums + p_i\n",
    "                \n",
    "                #calc \"c\"\n",
    "                csums = csums + p_j\n",
    "            \n",
    "            a = asums/len(newdf.columns)\n",
    "            b = bsums/len(newdf.columns)\n",
    "            c = csums/len(newdf.columns)\n",
    "            \n",
    "            d = a - (b*c)\n",
    "            rDict[locusi][locusj] = d\n",
    "        else:\n",
    "            rDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount %% 10 == 0:\n",
    "        print icount\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/0outfiles2/%s_%s_%s_imputedDVALS.txt'\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\\\t'.join(snps) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in rDict.keys():\n",
    "        line = str(locusi) + '\\\\t' + '\\\\t'.join([str(x) for x in rDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "    \n",
    "''' % (env,env,str(i).zfill(2),str(j).zfill(2), \n",
    "       env,str(i).zfill(2),str(j).zfill(2),\n",
    "       env,env,str(i).zfill(2),str(j).zfill(2),\n",
    "       env,str(i).zfill(2),str(j).zfill(2))\n",
    "            filE = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/0pyfiles/%s_%s_%s_imputed.py' % (env,\n",
    "                                                                                                      str(i).zfill(2),\n",
    "                                                                                                      str(j).zfill(2))\n",
    "            with open(filE,'w') as o:\n",
    "                o.write(text)\n",
    "            o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get pvals\n",
    "for env in envs:\n",
    "    text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import anderson_ksamp\n",
    "\n",
    "pvals = []\n",
    "for i in range(20):\n",
    "    for j in range(50):\n",
    "\n",
    "        F1 = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/0outfiles/%s_%%s_%%s_imputedDVALS.txt' %% (str(i).zfill(2),str(j).zfill(2))\n",
    "        F2 = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/0outfiles2/%s_%%s_%%s_imputedDVALS.txt' %% (str(i).zfill(2),str(j).zfill(2))\n",
    "\n",
    "        df1 = pd.read_csv(F1, header=0,index_col=0,sep=\"\\\\t\")\n",
    "        df2 = pd.read_csv(F2, header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "        dvals1 = []\n",
    "        for lst in df1.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    dvals1.append(x)\n",
    "        dvals2 = []\n",
    "        for lst in df2.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    dvals2.append(x)\n",
    "\n",
    "        pvals.append(ks_2samp(dvals1,dvals2)[1])\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/0randpvals/%s_randpvalues.txt'\n",
    "pvals = pd.DataFrame(pvals)\n",
    "pvals.to_csv(filE,header=None,index=False,sep=\"\\\\t\")\n",
    "\n",
    "''' % (env,\n",
    "       env,\n",
    "       env)\n",
    "    filE = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/1RANDpvalrunfiles/%s_pvalscript.py' % env\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILES = os.listdir('/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/1RANDpvalrunfiles/')\n",
    "FILES = [os.path.join('/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/1RANDpvalrunfiles/',f) for f in FILES]\n",
    "len(FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#originally ran this in '...missing.ipynb', then moved file with command line\n",
    "#for f in FILES:\n",
    "    dset = f.split(\"/\")[6]\n",
    "    DIR = os.path.dirname(f)\n",
    "    fname = os.path.basename(f)\n",
    "    env = fname.split(\"_\")[0]\n",
    "    \n",
    "    text = '''#!/bin/bash\n",
    "#$ -N %s%s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cd %s\n",
    "python %s\n",
    "''' % (dset,env,DIR,fname)\n",
    "    Dir = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/1RANDpvalshfiles'\n",
    "    filE = os.path.join(Dir,env+'_%s.sh' % dset)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Mann Whitney U tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for env in envs:\n",
    "    filE1 = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/rand/0randpvals/%s_randpvalues.txt' % env\n",
    "    filE2 = '/home/lindb/wbp/covariances/bayenv2/imputed/randmatrices/0OBSpvals/%s_imputed_observedpvalues.txt' % env\n",
    "    \n",
    "    obs = pd.read_csv(filE2,header=None,sep=\"\\t\")\n",
    "    rand = pd.read_csv(filE1,header=None,sep=\"\\t\")\n",
    "    \n",
    "    pval = mannwhitneyu(obs,rand)\n",
    "    print env,pval\n",
    "    \n",
    "    newtext = \"%s\\t%s\\n\" % (env,pval)\n",
    "    text = text + newtext\n",
    "filE = '/home/lindb/wbp/covariances/bayenv2/results/imputed.txt'\n",
    "with open(filE,'w') as o:\n",
    "    o.write('env\\t(statistic,p-val)\\n')\n",
    "    o.write(text)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mannwhitneyu(obs[0].tolist(),rand[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for now calculate MAF for all pops, and limit later on to 6 pops (that way bayenv2 cov can use the full dataframe)\n",
    "pops = ['Dicks_Pass',\n",
    "'Freel_Peak',\n",
    "'Heavenly',\n",
    "'Little_Round_Top',\n",
    "'Mt_Rose_Ophir',\n",
    "'Rifle_Peak',\n",
    "'Snow_Valley_Peak',\n",
    "'West_Shore_Peaks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pop in pops:\n",
    "    text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "F2 = '/home/lindb/wbp/covariances/pimass/missing/MAF/uncombinedMAF/%sUpdate.txt'\n",
    "with open(F2,'w') as F:\n",
    "    F.write(\"importing file\\\\n\")\n",
    "F.close()\n",
    "\n",
    "misfile = pd.read_csv('/home/lindb/eckertlab/wbp/bayenv2/UnbinnedMissingSNPSFILE.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "loci = np.unique(misfile.index).tolist()\n",
    "\n",
    "with open(F2,'a') as F:\n",
    "    F.write(\"creating dictionary\\\\n\")\n",
    "F.close()\n",
    "\n",
    "freqDict = OrderedDict()\n",
    "loccount = 0\n",
    "for locus in sorted(loci):\n",
    "    first = misfile.loc[locus,'%s'][0]\n",
    "    second = misfile.loc[locus,'%s'][1]\n",
    "\n",
    "    minor = min(first,second)\n",
    "    freq = minor/(first+second)\n",
    "\n",
    "    freqDict[locus] = freq\n",
    "    \n",
    "    loccount += 1\n",
    "    if loccount %% 1000 == 0:\n",
    "        with open(F2,'a') as F:\n",
    "            F.write(\"%%s\\\\n\" %% loccount)\n",
    "        F.close()\n",
    "\n",
    "\n",
    "with open(F2,'a') as F:\n",
    "    F.write(\"writing file\\\\n\")\n",
    "F.close()\n",
    "filE = '/home/lindb/eckertlab/wbp/covariances/pimass/missing/MAF/uncombinedMAF/missing_%s.txt'\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\\\t'.join(['locus','minor freq']) + str('\\\\n')\n",
    "    o.write(line)\n",
    "    for locus in freqDict.keys():\n",
    "        line = '\\\\t'.join([locus,str(freqDict[locus])]) + str('\\\\n')\n",
    "        o.write(line)        \n",
    "o.close()\n",
    "\n",
    "with open(F2,'a') as F:\n",
    "    F.write(\"done\")\n",
    "F.close()\n",
    "\n",
    "''' % (pop,pop,pop,pop)\n",
    "    filE = '/home/lindb/wbp/covariances/pimass/missing/MAF/uncombinedMAF/runfiles/missing_%s.py' % pop\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(text)\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make .sh files to qsub to run in parallel for each pop\n",
    "files = os.listdir('/home/lindb/eckertlab/wbp/covariances/pimass/missing/MAF/uncombinedMAF/runfiles/')\n",
    "count = 0\n",
    "for f in files:\n",
    "    if f.startswith('mis') and f.endswith('py'):\n",
    "        count += 1\n",
    "        text = '''#!/bin/bash\n",
    "#$ -N mis%s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cd %s\n",
    "python %s\n",
    "''' % (str(count),\n",
    "      '/home/lindb/eckertlab/wbp/covariances/pimass/missing/MAF/uncombinedMAF/runfiles/',\n",
    "      os.path.join('/home/lindb/eckertlab/wbp/covariances/pimass/missing/MAF/uncombinedMAF/runfiles/',f))\n",
    "        filE = '/home/lindb/eckertlab/wbp/covariances/pimass/missing/MAF/uncombinedMAF/runfiles/%s' % (f.split(\".\")[0] + '.sh')\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(text)\n",
    "        o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hmean = OrderedDict()\n",
    "hmean['imp'] = OrderedDict()\n",
    "hmean['mis'] = OrderedDict()\n",
    "for root,dirs,files in os.walk('/home/lindb/eckertlab/wbp/piMASS/analyses2/1hmean/'):\n",
    "    for f in files:\n",
    "        print f\n",
    "        if 'imp' in f:\n",
    "            splits = f.split(\"_\")\n",
    "            phen = splits[1]\n",
    "            hmean['imp'][phen] = pd.read_csv(os.path.join(root,f),sep=\"\\t\", header=0)\n",
    "        if 'mis' in f:\n",
    "            splits = f.split(\"_\")\n",
    "            phen = splits[1]\n",
    "            hmean['mis'][phen] = pd.read_csv(os.path.join(root,f),sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hmean['imp']['bffamx'].shape,hmean['imp']['bffamx'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos = hmean['mis'].keys()\n",
    "phenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percent_difference(x, y):\n",
    "    x = float(x)\n",
    "    y = float(y)\n",
    "    return (np.abs(x-y)/np.mean([x, y]))*100\n",
    "\n",
    "def get_quantile_max(name, data, q):\n",
    "    d = data.quantile(q)\n",
    "    d.index = [str(x) for x in d.index]\n",
    "    d['median_val'] = data.median()\n",
    "    d['mean_val'] = data.mean()\n",
    "    d['cutoff'] = 0.01\n",
    "    #added start\n",
    "    d['x995_cutoff'] = percent_difference(d['0.995'],d['cutoff'])\n",
    "    d['x995_median'] = percent_difference(d['0.995'],d['median_val'])\n",
    "    d['strinj_cutoff'] = d['0.995']\n",
    "\n",
    "    d['x9995_cutoff'] = percent_difference(d['0.9995'],d['cutoff'])\n",
    "    d['x9995_median'] = percent_difference(d['0.9995'],d['median_val'])\n",
    "    d['extra_strinj_cutoff'] = d['0.9995']\n",
    "    \n",
    "    #added end\n",
    "    d[\"x99_cutoff\"] = percent_difference(d['0.99'], d['cutoff'])\n",
    "    d[\"x99_median\"] =  percent_difference(d['0.99'], d['median_val'])\n",
    "    d[\"x95_cutoff\"] = percent_difference(d['0.95'], d['cutoff'])\n",
    "    d[\"x95_median\"] =  percent_difference(d['0.95'], d['median_val'])\n",
    "    d['relaxed_cutoff'] = d['0.99']\n",
    "    d['min'] = data.min()\n",
    "    d['max'] = data.max()\n",
    "    d.name = name\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "an_dir = '/home/lindb/eckertlab/wbp/piMASS/analyses2'\n",
    "quant = OrderedDict()\n",
    "for dset in hmean.keys():\n",
    "    quant[dset] = OrderedDict()\n",
    "    for pheno in phenos:\n",
    "        quant[dset][pheno] = get_quantile_max(str(pheno),hmean[dset][pheno].postrb_hmean,[0.95,0.99,0.995,0.9995])\n",
    "        quant[dset][pheno].to_csv(os.path.join(an_dir + str('/2quant'),\"%s_%s_quantFINAL.txt\" % (x,pheno)),header=True,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quantDF2 = pd.DataFrame()\n",
    "for dset in quant.keys():\n",
    "    for pheno in phenos:\n",
    "        test = pd.DataFrame(quant[dset][pheno])\n",
    "        test.columns = [\"%s_%s\" % (dset,str(x)) for x in test.columns]\n",
    "        quantDF2 = pd.concat([quantDF2,test],axis =1)\n",
    "quantDF2.to_csv(os.path.join(an_dir + str('/2quant'),'allquantFINAL.txt'),header=True,index=True,sep=\"\\t\")\n",
    "quantDF2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print sig snps out to a file to access later\n",
    "strnj_snps = OrderedDict()\n",
    "for dset in quant.keys():\n",
    "    strnj_snps[dset] = OrderedDict()\n",
    "    for pheno in phenos:\n",
    "        strnj_snps[dset][pheno] = hmean[dset][pheno][hmean[dset][pheno].postrb_hmean > quant[dset][pheno].extra_strinj_cutoff]\n",
    "        strnj_snps[dset][pheno].to_csv(os.path.join(an_dir + str('/7xstringent'),\"%s_%s_EXTRAstrinjSNPS.txt\" % (dset,pheno)),header=True,Index=True,sep=\"\\t\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(strnj_snps[dset][pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(np.unique(strnj_snps[dset][pheno]['chr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I have minor allele freqs for each locus in each pop, for each data set (wbp/covariances/missing/)\n",
    "#I have a list of significant SNPs identified by piMASS (wbp/pimass/analyses2/7xstringent)\n",
    "\n",
    "#use the set of sig snps to calc D using equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of files\n",
    "DIR = '/home/lindb/wbp/covariances/pimass/missing/MAF/uncombinedMAF/'\n",
    "files = os.listdir(DIR)\n",
    "files = [f for f in files if f.endswith('txt')]\n",
    "sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reformat files\n",
    "mvDIR = '/home/lindb/wbp/covariances/pimass/missing/MAF'\n",
    "for f in files:\n",
    "    pop = f[8:-4]\n",
    "    f = os.path.join(DIR,f)\n",
    "    df = pd.read_csv(f,header=0,sep=\"\\t\")\n",
    "    df.index = df['locus'].values.tolist()\n",
    "    df = pd.DataFrame(df['minor freq'])\n",
    "    df.to_csv(os.path.join(mvDIR,pop+'_MAF.txt'),header=False,index=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine files\n",
    "DIR ='/home/lindb/wbp/covariances/pimass/missing/MAF/'\n",
    "files = os.listdir(DIR)\n",
    "files = [f for f in files if 'MAF.' in f if 'combined' not in f]\n",
    "sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reformat again and combine\n",
    "newdf = pd.DataFrame()\n",
    "count = 0\n",
    "for f in files:\n",
    "    pop = f[:-8]\n",
    "    filE = os.path.join(DIR,f)\n",
    "    df = pd.read_csv(filE,header=None,sep=\"\\t\")\n",
    "    df.index = df[0].values.tolist()\n",
    "    df = pd.DataFrame(df[1])\n",
    "    df.columns = [pop]\n",
    "    \n",
    "    if count == 0:\n",
    "        newdf = df\n",
    "    else:\n",
    "        newdf = pd.merge(newdf,df,left_index=True,right_index=True)\n",
    "    count += 1\n",
    "filE = os.path.join(DIR,'combined_missing_MAF.txt')\n",
    "newdf.to_csv(filE,header=True,index=True,sep=\"\\t\")\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#only use 6/8 pops\n",
    "pops = ['Dicks_Pass',\n",
    "'Freel_Peak',\n",
    "'Little_Round_Top',\n",
    "'Mt_Rose_Ophir',\n",
    "'Rifle_Peak',\n",
    "'Snow_Valley_Peak']\n",
    "newdf = newdf[pops]\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(newdf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of files containing the sig SNPs for each phenotype\n",
    "pDIR = '/home/lindb/wbp/piMASS/analyses2/7xstringent/'\n",
    "files = os.listdir(pDIR)\n",
    "files = sorted([f for f in files if f.startswith('mis')])\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of snps for each phenotype\n",
    "snpDict = OrderedDict()\n",
    "for f in files:\n",
    "    pheno = f.split(\"_\")[1]\n",
    "    print pheno\n",
    "    df = pd.read_csv(os.path.join(pDIR,f),header=0,sep=\"\\t\")\n",
    "    snpDict[pheno] = df['rs'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#template for each phenotype\n",
    "d = OrderedDict()\n",
    "count = 0\n",
    "for i in range(10):\n",
    "    d[i] = OrderedDict()\n",
    "    for j in range(10):\n",
    "        if i > j: #i is row, j is col\n",
    "            d[i][j] = count # = value calculated\n",
    "            count += 1\n",
    "        else:\n",
    "            d[i][j] = np.nan\n",
    "rowcount = 0\n",
    "for row in d.keys():\n",
    "    if rowcount ==0:\n",
    "        print '\\t'+'\\t'.join([str(x) for x in range(len(d[row].values()))])\n",
    "    print str(rowcount)+'\\t'+'\\t'.join([str(x) for x in d[row].values()])\n",
    "    rowcount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do pairwise to get D\n",
    "dDict = OrderedDict()\n",
    "for pheno in phenos:\n",
    "    icount = 0\n",
    "    dDict[pheno] = OrderedDict()\n",
    "    for i,locusi in enumerate(snpDict[pheno]): # change pheno to %s\n",
    "        dDict[pheno][locusi] = OrderedDict()\n",
    "        for j,locusj in enumerate(snpDict[pheno]): # change pheno to %s\n",
    "            if i > j: # i=row,j=col : fill in lower triange of matrix\n",
    "                asums = 0\n",
    "                bsums = 0\n",
    "                csums = 0\n",
    "                for pop in newdf.columns:\n",
    "                    p_i = newdf.loc[locusi,pop]\n",
    "                    p_j = newdf.loc[locusj,pop]\n",
    "\n",
    "                    #calculate \"a\"\n",
    "                    product = p_i*p_j\n",
    "                    asums = asums + product\n",
    "\n",
    "                    #calculate \"b\"\n",
    "                    bsums = bsums + p_i\n",
    "\n",
    "                    #calculate \"c\"\n",
    "                    csums = csums + p_j    \n",
    "\n",
    "                a = asums/len(newdf.columns) #a = average product of f(locus_i) and f(locus_j) across pops\n",
    "                b = bsums/len(newdf.columns) #b = average allele freq of locus_i across populations\n",
    "                c = csums/len(newdf.columns) #c = average allele freq of locus_j across populations\n",
    "\n",
    "                d = a - (b*c) #covariance\n",
    "                dDict[pheno][locusi][locusj] = d\n",
    "            else: #fill in upper triangle of matrix\n",
    "                dDict[pheno][locusi][locusj] = np.nan \n",
    "        icount += 1\n",
    "        if icount % 10 == 0:\n",
    "            print pheno,icount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in dDict.keys():\n",
    "    rowcount = 0\n",
    "    filE = '/home/lindb/wbp/covariances/missing/%s_missing.txt' % pheno\n",
    "    with open(filE,'w') as o:\n",
    "        key0 = dDict[pheno].keys()[0]\n",
    "        line = '\\t'.join(dDict[pheno][key0].keys()) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "        for locusi in dDict[pheno].keys():\n",
    "            line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dDict[pheno][locusi].values()]) + str('\\n')\n",
    "            o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#each df should have a different set of SNPs, mostly anyways\n",
    "dataDict = OrderedDict()\n",
    "for pheno in phenos:\n",
    "    filE = '/home/lindb/wbp/covariances/missing/%s_missing.txt' % pheno\n",
    "    dataDict[pheno] = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataDict[phenos[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataDict[phenos[1]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataDict[phenos[2]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setup_r(): ####/gpfs_fs/home/lindb/anaconda/envs/conda/lib/R ####/home/lindb/g/R3/lib64/R/\n",
    "    os.environ['R_HOME'] = '/home/lindb/g/R3/lib64/R/' \n",
    "    os.environ['LD_LIBRARY_PATH'] = \"%s/lib:%s\" % (os.environ['R_HOME'], ####/home/lindb/g/R3/lib64/R/bin/R\n",
    "                                                   os.environ['LD_LIBRARY_PATH'])\n",
    "setup_r()\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri as pd2ri\n",
    "pd2ri.activate()\n",
    "r = robjects.r\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext rpy2.ipython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "bffamx = read.csv('/home/lindb/wbp/covariances/missing/bffamx_missing.txt',header=TRUE,sep=\"\\t\")\n",
    "nums = bffamx[lower.tri(bffamx)]\n",
    "hist(nums,breaks=80,main = \"bffam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "c13popx = read.csv('/home/lindb/wbp/covariances/imputed/Dvals/c13popx_imputed.txt',header=TRUE,sep=\"\\t\")\n",
    "nums = c13popx[lower.tri(c13popx)]\n",
    "hist(abs(nums),breaks=80,main = \"c13popx\")\n",
    "mean(abs(nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "n15famx = read.csv('/home/lindb/wbp/covariances/imputed/Dvals/n15famx_imputed.txt',header=TRUE,sep=\"\\t\")\n",
    "nums = n15famx[lower.tri(n15famx)]\n",
    "hist(nums,breaks=80,main = \"n15famx\")\n",
    "mean(abs(nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#randdata was made below, I came back up for comparison\n",
    "%%R\n",
    "randdata = read.csv('/home/lindb/wbp/covariances/missing/randmatrices/0outfiles/rspopx_19_49_missingDVALS.txt',header=TRUE,sep=\"\\t\")\n",
    "nums = randdata[lower.tri(n15famx)]\n",
    "hist(nums,breaks=80,main = \"randdata\")\n",
    "mean(abs(nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "randdata = read.csv('/home/lindb/wbp/covariances/missing/randmatrices/0outfiles/rspopx_19_48_missingDVALS.txt',header=TRUE,sep=\"\\t\")\n",
    "nums = randdata[lower.tri(n15famx)]\n",
    "hist(nums,breaks=80,main = \"randdata\")\n",
    "mean(abs(nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos = ['bffamx',\n",
    " 'rspopx',\n",
    " 'c13famx',\n",
    " 'htpopx',\n",
    " 'c13popx',\n",
    " 'rsfamx',\n",
    " 'htfamx',\n",
    " 'bfpopx',\n",
    " 'n15popx',\n",
    " 'n15famx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "?hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#random set of SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/piMASS/IntersectionRowNames.txt'\n",
    "intersection = pd.read_csv(filE,header=None,sep=\"\\t\")\n",
    "intersection = [x for x in intersection[1]]\n",
    "intersection[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intersection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of SNPs that aren't significant\n",
    "bDict = OrderedDict()\n",
    "for pheno in phenos:\n",
    "    bDict[pheno] = list(set(intersection) - set(snpDict[pheno]))\n",
    "    df = pd.DataFrame(bDict[pheno])\n",
    "    filE = '/home/lindb/wbp/covariances/pimass/missing/drawbuckets/%s_bucket.txt' % pheno\n",
    "    df.to_csv(filE,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make 1000 matrices of d-values for random SNPs to compare with sig set\n",
    "for pheno in phenos:                                     #each phenotype gets 20 .py files (200 .sh files total)\n",
    "    snpBucket = bDict[pheno]                             #the snps that can be drawn\n",
    "    for i in range(20):                                  #make 20 .py files\n",
    "        for j in range(50):                              #each .py file makes 50 matrices\n",
    "            snps = random.sample(snpBucket,80)           #select random snps\n",
    "\n",
    "            DIR = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/%s' % pheno\n",
    "            if not os.path.exists(DIR):\n",
    "                os.makedirs(DIR)\n",
    "            filE = os.path.join(DIR,\"%s_%s_%s_missing.txt\" % (pheno,str(i).zfill(2),str(j).zfill(2)))\n",
    "            df = pd.DataFrame(snps)\n",
    "            df.to_csv(filE,header=None,sep=\"\\t\")\n",
    "        \n",
    "        #write .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phenos = ['bffamx',\n",
    " 'rspopx',\n",
    " 'c13famx',\n",
    " 'htpopx',\n",
    " 'c13popx',\n",
    " 'rsfamx',\n",
    " 'htfamx',\n",
    " 'bfpopx',\n",
    " 'n15popx',\n",
    " 'n15famx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make .py files\n",
    "for pheno in phenos:\n",
    "    for i in range(20):\n",
    "        for j in range(50):\n",
    "            text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "filE= '/home/lindb/wbp/covariances/pimass/missing/randmatrices/%s/%s_%s_%s_missing.txt'\n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "snps = df[1].tolist()\n",
    "\n",
    "newdf = pd.read_csv('/home/lindb/wbp/covariances/pimass/missing/MAF/combined_missing_MAF.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "icount = 0\n",
    "rDict = OrderedDict()\n",
    "for i,locusi in enumerate(snps):\n",
    "    rDict[locusi] = OrderedDict()\n",
    "    for j,locusj in enumerate(snps):\n",
    "        if i > j: #i=row, j=col : lower tri\n",
    "            asums = 0\n",
    "            bsums = 0\n",
    "            csums = 0\n",
    "            for pop in newdf.columns:\n",
    "                p_i = newdf.loc[locusi,pop]\n",
    "                p_j = newdf.loc[locusj,pop]\n",
    "                \n",
    "                #calc \"a\"\n",
    "                product = p_i*p_j\n",
    "                asums = asums + product\n",
    "                \n",
    "                #calc \"b\"\n",
    "                bsums = bsums + p_i\n",
    "                \n",
    "                #calc \"c\"\n",
    "                csums = csums + p_j\n",
    "            \n",
    "            a = asums/len(newdf.columns)\n",
    "            b = bsums/len(newdf.columns)\n",
    "            c = csums/len(newdf.columns)\n",
    "            \n",
    "            d = a - (b*c)\n",
    "            rDict[locusi][locusj] = d\n",
    "        else:\n",
    "            rDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount %% 10 == 0:\n",
    "        print icount\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/0outfiles/%s_%s_%s_missingDVALS.txt'\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\\\t'.join(snps) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in rDict.keys():\n",
    "        line = str(locusi) + '\\\\t' + '\\\\t'.join([str(x) for x in rDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "    \n",
    "''' % (pheno,pheno,str(i).zfill(2),str(j).zfill(2), \n",
    "      pheno,str(i).zfill(2),str(j).zfill(2))\n",
    "            filE = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/0pyfiles/%s_%s_%s_missing.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(2),\n",
    "                                                                                                      str(j).zfill(2))\n",
    "            with open(filE,'w') as o:\n",
    "                o.write(text)\n",
    "            o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get all sh files including imputed\n",
    "mfiles = os.listdir('/home/lindb/wbp/covariances/pimass/missing/randmatrices/0pyfiles/')\n",
    "mfiles = [os.path.join('/home/lindb/wbp/covariances/pimass/missing/randmatrices/0pyfiles/',f) for f in mfiles]\n",
    "ifiles = os.listdir('/home/lindb/wbp/covariances/pimass/imputed/randmatrices/0pyfiles/')\n",
    "ifiles = [os.path.join('/home/lindb/wbp/covariances/pimass/imputed/randmatrices/0pyfiles/',f) for f in ifiles]\n",
    "files = ifiles + mfiles\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('/home/lindb/wbp/covariances/pimass/missing/randmatrices/0pyfiles')\n",
    "files = [os.path.join('/home/lindb/wbp/covariances/pimass/missing/randmatrices/0pyfiles',f) for f in files]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make .sh files\n",
    "fcount =0\n",
    "shcount =0\n",
    "tcount =0\n",
    "newsh = True\n",
    "for f in sorted(files):\n",
    "    if newsh == True:\n",
    "        text = '''#!/bin/bash\n",
    "#$ -N run%s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "''' % str(shcount).zfill(3)\n",
    "    newtext = '''\n",
    "cd %s\n",
    "python %s\n",
    "''' % (os.path.dirname(f),os.path.basename(f))\n",
    "    text = text + newtext\n",
    "    \n",
    "    fcount += 1\n",
    "    tcount += 1\n",
    "    newsh = False\n",
    "    if (fcount == 52) or (tcount == 10000):\n",
    "        newsh = True\n",
    "        fcount = 0\n",
    "        filE = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/0runfiles/mis_%s_run.sh' % str(shcount).zfill(3)\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)\n",
    "        o.close()\n",
    "        \n",
    "        shcount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/lindb/wbp/covariances/missing/randmatrices/0outfiles/bffamx_00_00_missingDVALS.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('/home/lindb/wbp/covariances/missing/randmatrices/0outfiles')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pDict = OrderedDict()\n",
    "for pheno in phenos:\n",
    "    pDict[pheno] = 0\n",
    "    for f in files:\n",
    "        if f.startswith('%s' % pheno):\n",
    "            pDict[pheno] += 1\n",
    "for pheno in pDict.keys():\n",
    "    print pheno,pDict[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here is where I left off 01/13/2016 @ 01:27am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#KS Tests - true SNPs vs. rand SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import anderson_ksamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in phenos:\n",
    "    text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import anderson_ksamp\n",
    "\n",
    "#get observed dvals\n",
    "DF = pd.read_csv('/home/lindb/wbp/covariances/pimass/missing/Dvals/%s_missing.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "dvals = []\n",
    "for lst in DF.values.tolist():\n",
    "    for x in lst:\n",
    "        if math.isnan(x) == False:\n",
    "            dvals.append(x)\n",
    "            \n",
    "DIR = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/0outfiles/'\n",
    "files = os.listdir(DIR)\n",
    "files = [f for f in files if f.startswith('%s')]\n",
    "\n",
    "pvals = []\n",
    "fcount = 0\n",
    "for f in files:\n",
    "    df = pd.read_csv(os.path.join(DIR,f),header=0,index_col=0,sep=\"\\\\t\")\n",
    "    rvals = []\n",
    "    for lst in df.values.tolist():\n",
    "        for x in lst:\n",
    "            if math.isnan(x) == False:\n",
    "                rvals.append(x)\n",
    "    \n",
    "    pvals.append(ks_2samp(rvals,dvals)[1])\n",
    "    fcount += 1\n",
    "    if fcount %% 10 == 0:\n",
    "        print fcount\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/0OBSpvals/%s_observedpvalues.txt'\n",
    "pvals = pd.DataFrame(pvals)\n",
    "pvals.to_csv(filE,header=None,index=False,sep=\"\\\\t\")\n",
    "\n",
    "''' % (pheno,pheno,pheno)\n",
    "    filE = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/1OBSpvalrunfiles/%s_pvalscript.py' % pheno\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('/home/lindb/wbp/covariances/pimass/missing/randmatrices/1OBSpvalrunfiles/')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    text ='''#!/bin/bash\n",
    "#$ -N mis%s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cd /home/lindb/wbp/covariances/pimass/missing/randmatrices/1OBSpvalrunfiles/\n",
    "python %s\n",
    "''' % (f.split(\"_\")[0],f)\n",
    "    filE = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/1OBSpvalrunfiles/%s.sh' % f.split(\".\")[0]\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(text)\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#KS tests - rand SNPs vs rand SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select 2000 sets of 80 SNPs for each phenotype\n",
    "#for each set, determine d-values\n",
    "#run KS test for each of first 1000 d-values with one from second 1000 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(bDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make 1000 snp sets of random SNPs to compare with 1000 rand below\n",
    "for pheno in phenos:                                     #each phenotype gets 20 .py files (200 .sh files total)\n",
    "    snpBucket = bDict[pheno]                             #the snps that can be drawn\n",
    "    for i in range(20):                                  #make 20 .py files\n",
    "        for j in range(50):                              #each .py file makes 50 matrices\n",
    "            snps = random.sample(snpBucket,len(snpDict[pheno]))           #select random snps\n",
    "\n",
    "            DIR = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/%s' % pheno\n",
    "            if not os.path.exists(DIR):\n",
    "                os.makedirs(DIR)\n",
    "            filE = os.path.join(DIR,\"%s_%s_%s_missing.txt\" % (pheno,str(i).zfill(2),str(j).zfill(2)))\n",
    "            df = pd.DataFrame(snps)\n",
    "            df.to_csv(filE,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a second set of 1000 snp sets of random SNPs to compare with first random 1000 above\n",
    "for pheno in phenos:                                     #each phenotype gets 20 .py files (200 .sh files total)\n",
    "    snpBucket = bDict[pheno]                             #the snps that can be drawn\n",
    "    for i in range(20):                                  #make 20 .py files\n",
    "        for j in range(50):                              #each .py file makes 50 matrices\n",
    "            snps = random.sample(snpBucket,len(snpDict[pheno]))           #select random snps\n",
    "\n",
    "            DIR = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/%s2' % pheno\n",
    "            if not os.path.exists(DIR):\n",
    "                os.makedirs(DIR)\n",
    "            filE = os.path.join(DIR,\"%s_%s_%s_missing.txt\" % (pheno,str(i).zfill(2),str(j).zfill(2)))\n",
    "            df = pd.DataFrame(snps)\n",
    "            df.to_csv(filE,header=None,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make .py files to get dvals from both sets of 1000 snps created above\n",
    "for pheno in phenos:\n",
    "    for i in range(20):\n",
    "        for j in range(50):\n",
    "            text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "filE= '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/%s/%s_%s_%s_missing.txt'\n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "snps = df[1].tolist()\n",
    "\n",
    "newdf = pd.read_csv('/home/lindb/wbp/covariances/pimass/missing/MAF/combined_missing_MAF.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "icount = 0\n",
    "rDict = OrderedDict()\n",
    "for i,locusi in enumerate(snps):\n",
    "    rDict[locusi] = OrderedDict()\n",
    "    for j,locusj in enumerate(snps):\n",
    "        if i > j: #i=row, j=col : lower tri\n",
    "            asums = 0\n",
    "            bsums = 0\n",
    "            csums = 0\n",
    "            for pop in newdf.columns:\n",
    "                p_i = newdf.loc[locusi,pop]\n",
    "                p_j = newdf.loc[locusj,pop]\n",
    "                \n",
    "                #calc \"a\"\n",
    "                product = p_i*p_j\n",
    "                asums = asums + product\n",
    "                \n",
    "                #calc \"b\"\n",
    "                bsums = bsums + p_i\n",
    "                \n",
    "                #calc \"c\"\n",
    "                csums = csums + p_j\n",
    "            \n",
    "            a = asums/len(newdf.columns)\n",
    "            b = bsums/len(newdf.columns)\n",
    "            c = csums/len(newdf.columns)\n",
    "            \n",
    "            d = a - (b*c)\n",
    "            rDict[locusi][locusj] = d\n",
    "        else:\n",
    "            rDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount %% 10 == 0:\n",
    "        print icount\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/0outfiles/%s_%s_%s_missingDVALS.txt'\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\\\t'.join(snps) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in rDict.keys():\n",
    "        line = str(locusi) + '\\\\t' + '\\\\t'.join([str(x) for x in rDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "\n",
    "#get dvals for second set\n",
    "filE= '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/%s2/%s_%s_%s_missing.txt'\n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "snps = df[1].tolist()\n",
    "\n",
    "icount = 0\n",
    "rDict = OrderedDict()\n",
    "for i,locusi in enumerate(snps):\n",
    "    rDict[locusi] = OrderedDict()\n",
    "    for j,locusj in enumerate(snps):\n",
    "        if i > j: #i=row, j=col : lower tri\n",
    "            asums = 0\n",
    "            bsums = 0\n",
    "            csums = 0\n",
    "            for pop in newdf.columns:\n",
    "                p_i = newdf.loc[locusi,pop]\n",
    "                p_j = newdf.loc[locusj,pop]\n",
    "                \n",
    "                #calc \"a\"\n",
    "                product = p_i*p_j\n",
    "                asums = asums + product\n",
    "                \n",
    "                #calc \"b\"\n",
    "                bsums = bsums + p_i\n",
    "                \n",
    "                #calc \"c\"\n",
    "                csums = csums + p_j\n",
    "            \n",
    "            a = asums/len(newdf.columns)\n",
    "            b = bsums/len(newdf.columns)\n",
    "            c = csums/len(newdf.columns)\n",
    "            \n",
    "            d = a - (b*c)\n",
    "            rDict[locusi][locusj] = d\n",
    "        else:\n",
    "            rDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount %% 10 == 0:\n",
    "        print icount\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/0outfiles2/%s_%s_%s_missingDVALS.txt'\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\\\t'.join(snps) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in rDict.keys():\n",
    "        line = str(locusi) + '\\\\t' + '\\\\t'.join([str(x) for x in rDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "    \n",
    "''' % (pheno,pheno,str(i).zfill(2),str(j).zfill(2), \n",
    "       pheno,str(i).zfill(2),str(j).zfill(2),\n",
    "       pheno,pheno,str(i).zfill(2),str(j).zfill(2),\n",
    "       pheno,str(i).zfill(2),str(j).zfill(2))\n",
    "            filE = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/0pyfiles/%s_%s_%s_missing.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(2),\n",
    "                                                                                                      str(j).zfill(2))\n",
    "            with open(filE,'w') as o:\n",
    "                o.write(text)\n",
    "            o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of all 20000 py files combined between missing and imputed\n",
    "files = os.listdir('/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/0pyfiles/')\n",
    "files = [os.path.join('/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/0pyfiles/',f) for f in files]\n",
    "FILES = os.listdir('/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/0pyfiles/')\n",
    "FILES = [os.path.join('/home/lindb/wbp/covariances/pimass/imputed/randmatrices/rand/0pyfiles/',f) for f in FILES]\n",
    "Files = files + FILES\n",
    "len(Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#195 sh files with ~103 py files within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#when to stop in the next frame\n",
    "tcount = 0\n",
    "for f in Files:\n",
    "    tcount += 1\n",
    "tcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make 195 sh files to include imputed py files too\n",
    "fcount  = 0\n",
    "shcount = 0\n",
    "tcount  = 0\n",
    "newsh   = True\n",
    "for f in sorted(Files):\n",
    "    if newsh == True:\n",
    "        text = '''#!/bin/bash\n",
    "#$ -N run%s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "''' % str(shcount).zfill(3)\n",
    "    newtext = '''\n",
    "cd %s\n",
    "python %s\n",
    "''' % (os.path.dirname(f),f.split(\"/\")[-1])\n",
    "    text = text + newtext\n",
    "    \n",
    "    fcount += 1\n",
    "    tcount += 1\n",
    "    newsh = False\n",
    "    if (fcount == 103) or (tcount == 20000):\n",
    "        newsh = True\n",
    "        fcount = 0\n",
    "        filE = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/0runfiles/%s_run.sh' % str(shcount).zfill(3)\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(text)\n",
    "        o.close()\n",
    "        \n",
    "        shcount += 1\n",
    "    \n",
    "    if 'bffamx_00_00_missing.py' in f: #to see (vim the file) where it transitions from imputed to missing\n",
    "        print \"shcount\",shcount\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get pvals\n",
    "for pheno in phenos:\n",
    "    text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import anderson_ksamp\n",
    "\n",
    "pvals = []\n",
    "for i in range(20):\n",
    "    for j in range(50):\n",
    "\n",
    "        F1 = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/0outfiles/%s_%%s_%%s_missingDVALS.txt' %% (str(i).zfill(2),str(j).zfill(2))\n",
    "        F2 = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/0outfiles2/%s_%%s_%%s_missingDVALS.txt' %% (str(i).zfill(2),str(j).zfill(2))\n",
    "\n",
    "        df1 = pd.read_csv(F1, header=0,index_col=0,sep=\"\\\\t\")\n",
    "        df2 = pd.read_csv(F2, header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "        dvals1 = []\n",
    "        for lst in df1.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    dvals1.append(x)\n",
    "        dvals2 = []\n",
    "        for lst in df2.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    dvals2.append(x)\n",
    "\n",
    "        pvals.append(ks_2samp(dvals1,dvals2)[1])\n",
    "\n",
    "filE = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/0randpvals/%s_randpvalues.txt'\n",
    "pvals = pd.DataFrame(pvals)\n",
    "pvals.to_csv(filE,header=None,index=False,sep=\"\\\\t\")\n",
    "\n",
    "''' % (pheno,\n",
    "       pheno,\n",
    "       pheno)\n",
    "    filE = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/1RANDpvalrunfiles/%s_pvalscript.py' % pheno\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)\n",
    "    o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/1RANDpvalrunfiles/'\n",
    "files = os.listdir(DIR)\n",
    "print \"len(files) =\",len(files)\n",
    "for f in files:\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N mis%s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cd /home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/1RANDpvalrunfiles/\n",
    "python %s\n",
    "''' % (f.split(\".\")[0],f)\n",
    "    filE = os.path.join(DIR,f.split(\".\")[0]+'.sh')\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#mann whitney tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for pheno in phenos:\n",
    "    filE1 = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/rand/0randpvals/%s_randpvalues.txt' % pheno\n",
    "    filE2 = '/home/lindb/wbp/covariances/pimass/missing/randmatrices/0OBSpvals/%s_observedpvalues.txt' % pheno\n",
    "    \n",
    "    obs = pd.read_csv(filE2,header=None,sep=\"\\t\")\n",
    "    rand = pd.read_csv(filE1,header=None,sep=\"\\t\")\n",
    "    \n",
    "    pval = mannwhitneyu(obs,rand)\n",
    "    print pheno,pval\n",
    "    \n",
    "    newtext = \"%s\\t%s\\n\" % (pheno,pval)\n",
    "    text = text + newtext\n",
    "filE = '/home/lindb/wbp/covariances/pimass/results/missing.txt'\n",
    "with open(filE,'w') as o:\n",
    "    o.write('pheno\\t(statistic,p-val)\\n')\n",
    "    o.write(text)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "phenos = c('bffamx','rspopx','c13famx', 'htpopx', 'c13popx', 'rsfamx', 'htfamx', 'bfpopx', 'n15popx', 'n15famx')\n",
    "filE1 = '/home/lindb/wbp/covariances/missing/randmatrices/rand/0randpvals/bffamx_randpvalues.txt'\n",
    "rand = read.csv(filE1,header=F,sep=\"\\t\")\n",
    "\n",
    "filE2 = '/home/lindb/wbp/covariances/imputed/randmatrices/0OBSpvals/bffamx_observedpvalues.txt'\n",
    "obs = read.csv(filE2,header=F,sep=\"\\t\")\n",
    "\n",
    "OBS = c()\n",
    "for(val in obs[\"V1\"]){OBS <- val}\n",
    "RAND = c()\n",
    "for(val in rand[\"V1\"]){RAND <- val}\n",
    "hist(log(OBS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "hist(log(RAND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do imputed vs missing pvals to see if they aren't different\n",
    "\n",
    "#actually look at distributions of p-values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

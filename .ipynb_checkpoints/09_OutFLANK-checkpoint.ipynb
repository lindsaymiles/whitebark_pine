{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "import math\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import anderson_ksamp\n",
    "import os.path as op\n",
    "import os.listdir as ls\n",
    "from scipy.stats import spearmanr\n",
    "import skbio\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make SNPmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the hierftrans for all SNPS, created above as infile for multilocus FST\n",
    "#z12 file created in 06_pca.ipyn\n",
    "filE = '/home/lindb/wbp/OutFLANK/imputed_z12_maf_swp_trans_z12.txt'\n",
    "imp012 = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "imp012.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get pop assignment for each samp\n",
    "filE = '/home/lindb/wbp/sampsTOpop.txt'\n",
    "stp = pd.read_csv(filE,header=0,index_col='sampID',sep=\"\\t\")\n",
    "stp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = pd.merge(imp012,stp,left_index=True,right_index=True)\n",
    "cols = ['pop'] + [col for col in merged.columns if 'NODE' in col]\n",
    "merged = merged[cols]\n",
    "merged.sort_index(inplace=True)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make sure there aren't any weird data in the dataframe - like 'NAs' or 'np.nan'\n",
    "for col in merged.columns:\n",
    "    uni = np.unique(merged[col].tolist()).tolist()\n",
    "    if (uni == [0,1]) or (uni == [0,1,2]) or (uni == [1,2]) or (uni == [0,2]):\n",
    "        1+1\n",
    "    else:\n",
    "        print col, np.unique(merged[col].tolist()).tolist() #should only print the pop column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = [col for col in merged.columns if 'NODE' in col]\n",
    "snpmat = merged[cols]\n",
    "snpmat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pops = pd.DataFrame(merged['pop'].tolist())\n",
    "pops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/SNPmat_HEADERIDX.txt'\n",
    "filE2 = '/home/lindb/wbp/OutFLANK/SNPmat_noHEADERIDX.txt'\n",
    "print 'making 1st snpmat' #so I don't have to watch ls -lt\n",
    "snpmat.to_csv(filE,header=True,index=True,sep=\"\\t\")\n",
    "print 'making 2nd snpmat'\n",
    "snpmat.to_csv(filE2,header=None,index=False,sep=\"\\t\")\n",
    "\n",
    "popfile = '/home/lindb/wbp/OutFLANK/SNPmat_popNames.txt'\n",
    "print 'making popfile'\n",
    "pops.to_csv(popfile,header=False,index=False,sep=\"\\t\")\n",
    "\n",
    "print 'making locfile'\n",
    "locfile = '/home/lindb/wbp/OutFLANK/SNPmat_locusNames.txt'\n",
    "cols = pd.DataFrame(snpmat.columns)\n",
    "cols.to_csv(locfile,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put into R\n",
    "\n",
    "```R\n",
    "\n",
    "library(OutFLANK)\n",
    "library(data.table)\n",
    "\n",
    "SNPmat = data.frame(fread('/home/lindb/wbp/OutFLANK/SNPmat_noHEADERIDX.txt',header=F,sep=\"\\t\"))\n",
    "\n",
    "locusNames = read.csv('/home/lindb/wbp/OutFLANK/SNPmat_locusNames.txt',header=F,sep=\"\\t\")\n",
    "\n",
    "popNames = read.csv('/home/lindb/wbp/OutFLANK/SNPmat_popNames.txt',header=F,sep=\"\\t\")\n",
    "\n",
    "FstDataFrame = MakeDiploidFSTMat(SNPmat,locusNames,popNames)\n",
    "\n",
    "out = OutFLANK(FstDataFrame = FstDataFrame,NumberOfSamples = 8)\n",
    "\n",
    "df = out$results\n",
    "\n",
    "outliers = df[which(df$OutlierFlag == 'TRUE'),]\n",
    "\n",
    "loci = outliers$LocusName\n",
    "\n",
    "write.table(df,'/home/lindb/wbp/OutFLANK/OutFLANK_results.txt',row.names=F,col.names=T,sep='\\t')\n",
    "\n",
    "write.table(loci,'/home/lindb/wbp/OutFLANK/OutFLANK_snps.txt',row.names=F,sep='\\t')\n",
    "\n",
    "print(\"DONE!\")\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snps = pd.read_csv('/home/lindb/wbp/OutFLANK/OutFLANK_snps.txt',header=0,sep=\"\\t\")\n",
    "snps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = pd.read_csv('/home/lindb/wbp/OutFLANK/OutFLANK_results.txt',header=0,sep='\\t')\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fstnocorr = res['FSTNoCorr'].tolist()\n",
    "min(fstnocorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(snps.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# are outliers caused by effects from missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I want 'missing' because the 'imputed' will all have perc data (%missing data at a snp) == 1.0\n",
    "#this file was made in 07_hierfstat_missing.ipynb\n",
    "percdata = pd.read_csv('/home/lindb/wbp/hierfstat/missing/missing_hierarchical_Fstats.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "percdata = pd.DataFrame(percdata['perc data'])\n",
    "percdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use as index to merge\n",
    "snps.index = [x for x in snps['x']]\n",
    "snps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = pd.merge(snps,percdata,left_index=True,right_index=True)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(merged.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(merged['perc data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(percdata['perc data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv('/home/lindb/wbp/OutFLANK/OutFLANK_results.txt',header=0,index_col='LocusName',sep=\"\\t\")\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(results['OutlierFlag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trues = results[results[u'OutlierFlag'] == True]\n",
    "len(trues.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged2 = pd.merge(merged,pd.DataFrame(trues[['FST','FSTNoCorr']]),left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(merged2['perc data'],merged2['FSTNoCorr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(merged2['perc data'],merged2['FST'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged2['FSTNoCorr'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(merged2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get multilocus f stats for the imputed outlier snps\n",
    "#this file was made in 07_hierfstat_imputed.ipynb\n",
    "impfstats = pd.read_csv('/home/lindb/wbp/hierfstat/imputed/imputed_hierarchical_Fstats.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "impfstats = impfstats.loc[:,[col for col in impfstats.columns if not 'perc' in col]]\n",
    "impfstats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the important columns\n",
    "allstats = pd.merge(impfstats,merged2.loc[:,[col for col in merged2.columns if not 'x' in col]],\n",
    "                    left_index=True,right_index=True)\n",
    "allstats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rename so I don't have to search my script to find what they mean or where they come from\n",
    "cols = ['pop_total_hierfstat', 'plot_total_hierfstat', 'plot_pop_hierfstat', 'perc missing genotypes', 'FST_outflank', \n",
    "        'FSTNoCorr_outflank']\n",
    "allstats.columns = [col for col in cols]\n",
    "allstats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write to file\n",
    "filE = '/home/lindb/wbp/OutFLANK/hierarchical_Fstats_outflankoutliers.txt'\n",
    "allstats.to_csv(filE,header=True,index=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allstats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariances using H_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#imputed_z12_maf_swp_trans_z12.txt was made in 06_pca.ipynb and is symlinked in /OutFLANK\n",
    "filE = '/home/lindb/wbp/OutFLANK/imputed_z12_maf_swp_trans_z12.txt'\n",
    "imp012 = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "imp012.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp012.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get expected heterozygosity\n",
    "Hexp = OrderedDict()\n",
    "count = 0\n",
    "for snp in imp012.columns:\n",
    "    zero = imp012[snp].tolist().count(0)\n",
    "    one  = imp012[snp].tolist().count(1)\n",
    "    two  = imp012[snp].tolist().count(2)\n",
    "    \n",
    "    p = ((2*zero)+one)/(2*(zero+one+two))\n",
    "    q = ((2*two)+one)/(2*(zero+one+two))\n",
    "    \n",
    "    Hexp[snp] = 2*p*q\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print count\n",
    "len(Hexp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/Hexp_by_snp.txt'\n",
    "with open(filE,'w') as o:\n",
    "    text = 'locus\\tH_exp\\n'\n",
    "    o.write(\"%s\" % text)\n",
    "    for snp in Hexp.keys():\n",
    "        text = '\\t'.join([snp,str(Hexp[snp])])+'\\n'\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/Hexp_by_snp.txt'\n",
    "H = pd.read_csv(filE,header=0,sep='\\t')\n",
    "H.index = [snp for snp in H['locus'].tolist()]\n",
    "H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(H['h_exp'],bins = [x for x in np.arange(0,.51,0.01)])[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(np.arange(0,0.51,0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#see what will happen\n",
    "#1st bin is the j=0th bin, 50th bin is the j=49th bin\n",
    "for Bin,j in enumerate(np.arange(0,.51,.01)):\n",
    "    #print Bin,j\n",
    "    if 0.50>j: #0.50 don't need their own bin\n",
    "        print \"heyo\",Bin,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.round(H.loc[1043,'h_exp'],decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assign bins to samps\n",
    "#1st bin is the j=0th bin, 50th bin is the j=49th bin\n",
    "count = 0\n",
    "binDict = OrderedDict()\n",
    "for row in H.index:\n",
    "    h = np.round(H.loc[row,'h_exp'],decimals=3)\n",
    "    binDict[row] = 0 #because 2pq will never be equal to 0 for a SNP, but may be less than 0.01\n",
    "    for Bin,j in enumerate(np.arange(0,0.51,0.01)): #1st bin is the j=0th bin, 50th bin is the j=49th bin\n",
    "        if h>j: #binDict[row] will constantly replace the value, which is good. don't want 2pq=0.5 having its own group\n",
    "            binDict[row] = Bin\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(np.unique(binDict.values()).tolist()) # how many bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binDict.values()[:10],binDict.values()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H['bin'] = binDict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(H['bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write the file\n",
    "filE = '/home/lindb/wbp/OutFLANK/Hexp_by_snp_withbins.txt'\n",
    "H.to_csv(filE,header=True,index=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read back the file\n",
    "filE = '/home/lindb/wbp/OutFLANK/Hexp_by_snp_withbins.txt'\n",
    "H = pd.read_csv(filE, header=0,index_col=0,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/OutFLANK/figures/expected_heterozygosit_all_SNPs.pdf') as pdf:\n",
    "    fig = plt.figure()\n",
    "    plt.hist(H['h_exp'],bins = [x for x in np.arange(0,0.51,0.01)])[2]\n",
    "    plt.xlabel('Expected Heterozygosity')\n",
    "    plt.ylabel('Count')\n",
    "    #set_size_inches(5,5)\n",
    "    pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/OutFLANK/figures/expected_heterozygosit_all_SNPs.pdf') as pdf:\n",
    "    fig = plt.figure()\n",
    "    plt.hist(H['bin'],bins = [x for x in range(51)])[2]\n",
    "    plt.xlabel('Expected Heterozygosity')\n",
    "    plt.ylabel('Count')\n",
    "    #set_size_inches(5,5)\n",
    "    pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(H.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make minor allele freq dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get allele counts by pop - first locus = counts of 0 allele, second = counts of 2 allele\n",
    "    #012 counts global minor allele\n",
    "counts = pd.read_csv('/home/lindb/wbp/bayenv2/UnbinnedImputedSNPSFILE.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first row + second row of DataFrame(counts)\n",
    "43+91+43+42+87+42+41+40+7+5+7+8+11+6+7+8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(counts.loc['NODE_1000013_length_91_cov_1.802198_37',:])\n",
    "df.index = ['major','minor']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(df['Dicks_Pass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc['minor','Dicks_Pass']/sum(df['Dicks_Pass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(my notebook is on a slow node)\n",
    "#make minor allele freq dataframe\n",
    "text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "f2 = '/home/lindb/wbp/OutFLANK/update_maf.txt'\n",
    "with open(f2,'w') as o:\n",
    "    text = 'starting\\\\n'\n",
    "    o.write(\"%s\" % text)\n",
    "\n",
    "counts = pd.read_csv('/home/lindb/wbp/bayenv2/UnbinnedImputedSNPSFILE.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "loci = np.unique(counts.index).tolist()\n",
    "\n",
    "loccount = 0\n",
    "mafDict = OrderedDict()\n",
    "for locus in loci:\n",
    "    mafDict[locus] = OrderedDict()\n",
    "    data = pd.DataFrame(counts.loc[locus,:])\n",
    "    data.index = ['major','minor']\n",
    "    for pop in data.columns:\n",
    "        MAF = data.loc['minor',pop]/sum(data[pop]) #get allele freq corresponding to global minor allele\n",
    "        mafDict[locus][pop] = MAF\n",
    "    loccount += 1\n",
    "    if loccount % 1000 == 0:\n",
    "        with open(f2,'a') as o:\n",
    "            text = \"%s\" % str(loccount)\n",
    "            o.write(\"%s\\\\n\" % text)\n",
    "        print loccount\n",
    "\n",
    "with open(f2,'a') as o:\n",
    "    text = 'writing file\\\\n'\n",
    "    o.write(\"%s\" % text)\n",
    "\n",
    "filE = '/home/lindb/wbp/OutFLANK/imputed_MAF.txt'\n",
    "with open(filE,'w') as o:\n",
    "    text = '\\\\t'.join([x for x in mafDict[mafDict.keys()[0]].keys()]) + '\\\\n'\n",
    "    o.write(\"%s\" % text)\n",
    "    print text\n",
    "    count = 0\n",
    "    for locus in mafDict.keys():\n",
    "        text = locus + '\\\\t' + '\\\\t'.join([str(x) for x in mafDict[locus].values()]) + '\\\\n'\n",
    "        o.write(\"%s\" % text)\n",
    "        count += 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/maf.py'\n",
    "with open(filE,'w') as o:\n",
    "    o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shtext = '''#!/bin/bash\n",
    "#$ -N maf\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cd /home/lindb/wbp/OutFLANK/\n",
    "python maf.py\n",
    "\n",
    "'''\n",
    "filE = '/home/lindb/wbp/OutFLANK/get_maf.sh'\n",
    "with open(filE,'w') as o:\n",
    "    o.write(\"%s\" % shtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!qsub /home/lindb/wbp/OutFLANK/get_maf.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outliersnps = snps['x'].tolist()\n",
    "len(outliersnps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outliersnps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impMAF = pd.read_csv('/home/lindb/wbp/OutFLANK/imputed_MAF.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "impMAF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE= '/home/lindb/wbp/OutFLANK/imputed_MAF.txt'\n",
    "impMAF.to_csv(filE,header=True,index=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impMAF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impMAF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pop in impMAF.columns:\n",
    "    print pop,min(impMAF[pop]),max(impMAF[pop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get global maf allele freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glob = OrderedDict() \n",
    "count = 0\n",
    "for snp in imp012.columns:\n",
    "    zero = imp012[snp].tolist().count(0)\n",
    "    one  = imp012[snp].tolist().count(1)\n",
    "    two  = imp012[snp].tolist().count(2)\n",
    "    \n",
    "    a1 = ((2*zero)+one)/(2*(zero+one+two))\n",
    "    a2 = ((2*two)+one)/(2*(zero+one+two))\n",
    "    \n",
    "    q = min(a1,a2)\n",
    "    \n",
    "    glob[snp] = q\n",
    "    \n",
    "    count +=1 \n",
    "    if count % 10000 == 0:\n",
    "        print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(glob.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/global_mafs.txt'\n",
    "with open(filE,'w') as o:\n",
    "    text = '\\t'.join(['locus','maf'])+'\\n'\n",
    "    o.write(\"%s\" % text)\n",
    "    for snp in glob.keys():\n",
    "        text = '\\t'.join([snp,str(glob[snp])])+'\\n'\n",
    "        o.write(\"%s\" % text)\n",
    "globmafs = pd.read_csv(filE,header=0,sep='\\t')\n",
    "globmafs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make sure global allele freq and H_exp make sense\n",
    "H.loc['NODE_1000031_length_98_cov_2.000000_30','h_exp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "2*0.151639*(1-0.151639)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(globmafs.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get pop sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/sampsTOpop.txt'\n",
    "stp = pd.read_csv(filE,header=0,sep=\"\\t\")\n",
    "stp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pops matched to samps\n",
    "ptsDict = OrderedDict() #pop to samp dictionary\n",
    "for row in stp.index:\n",
    "    pop = stp.loc[row,'pop']\n",
    "    if not pop in ptsDict.keys():\n",
    "        ptsDict[pop] = []\n",
    "    ptsDict[pop].append(stp.loc[row,'sampID'])\n",
    "for pop in ptsDict.keys():\n",
    "    print pop,len(ptsDict[pop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dict with num indiv in pop\n",
    "popDict = OrderedDict()\n",
    "total = 0\n",
    "for pop in ptsDict.keys():\n",
    "    popDict[pop] = len(ptsDict[pop])\n",
    "    print pop,popDict[pop]\n",
    "    total += popDict[pop]\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariances using H_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snps = pd.read_csv('/home/lindb/wbp/OutFLANK/OutFLANK_snps.txt',header=0,sep='\\t')\n",
    "outliersnps = snps['x'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/OutFLANK_snps.txt'\n",
    "snps.to_csv(filE,header=True,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(outliersnps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    D_ij = sum { (nk/n)*((qik*qjk)-(qi*qj)) } for 1:k pops\n",
    "    qik = snp i maf for pop k\n",
    "    qik = snp j maf for pop k\n",
    "    \n",
    "    qi = global maf\n",
    "    qj = global maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise to get D\n",
    "dijDict = OrderedDict() \n",
    "icount = 0\n",
    "for i,locusi in enumerate(outliersnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "    \n",
    "    for j,locusj in enumerate(outliersnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "            \n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "                \n",
    "                sums += (nk/244)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount % 10 == 0:\n",
    "        print icount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write out the file\n",
    "rowcount = 0\n",
    "filE = '/home/lindb/wbp/OutFLANK/covariances/dvals/imputed_dvals.txt'\n",
    "if not op.exists(op.dirname(filE)):\n",
    "    os.makedirs(op.dirname(filE))\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\t'.join(dijDict[key0].keys()) + str('\\n')\n",
    "    o.write(\"%s\" % line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\n')\n",
    "        o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvals = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "dvals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataframe with the outlier loci and their bins\n",
    "outlierdata = pd.DataFrame(H[H['locus'].isin(outliersnps)])\n",
    "outlierdata.index = [snp for snp in outlierdata['locus'].tolist()]\n",
    "outlierdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/OutFLANK/figures/expected_heterozygosit_outlier_SNPs.pdf') as pdf:\n",
    "    fig = plt.figure()\n",
    "    plt.hist(outlierdata['h_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    plt.xlabel('Expected Heterozygosity')\n",
    "    plt.ylabel('Count')\n",
    "    #set_size_inches(5,5)\n",
    "    pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(outlierdata['h_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonsigs = set(H.index.tolist()) - set(outlierdata.index.tolist())\n",
    "nonsigs = [x for x in nonsigs]\n",
    "len(nonsigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonsigdata = pd.DataFrame(H[H['locus'].isin(nonsigs)])\n",
    "nonsigdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many random snps from each bin?\n",
    "binCounter = Counter()\n",
    "for row in outlierdata.index:\n",
    "    binCounter[outlierdata.loc[row,'bin']] += 1\n",
    "for b in binCounter.keys():\n",
    "    print b,binCounter[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make 1000 dataframes with a set of snps == 110 = len(outliersnps)\n",
    "for i in range(20):                                  #make 20 .py files\n",
    "    for j in range(50):                              #each .py file makes 50 matrices\n",
    "        snps = []        \n",
    "        for binn in binCounter.keys():\n",
    "            data = nonsigdata[nonsigdata['bin'] == binn]\n",
    "            \n",
    "            [snps.append(snp) for snp in random.sample(data.index,binCounter[binn])]\n",
    "        \n",
    "        print len(snps)\n",
    "        DIR = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/randsnps'\n",
    "        if not op.exists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = op.join(DIR,\"outflank_%s_%s_randsnps.txt\" % (str(i).zfill(2),str(j).zfill(2)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I'm using all of my engines for bayenv2 at the moment, ran these with GNU parallel\n",
    "#get dvals for 1000 sets of random snps of len=110\n",
    "for k in range(20):\n",
    "    for l in range(50):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import math\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import anderson_ksamp\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "#get pop assignment for each samp\n",
    "filE = '/home/lindb/wbp/sampsTOpop.txt'\n",
    "stp = pd.read_csv(filE,header=0,sep=\"\\t\")\n",
    "\n",
    "\n",
    "#pops matched to samps\n",
    "ptsDict = OrderedDict() #pop to samp dictionary\n",
    "for row in stp.index:\n",
    "    pop = stp.loc[row,'pop']\n",
    "    if not pop in ptsDict.keys():\n",
    "        ptsDict[pop] = []\n",
    "    ptsDict[pop].append(stp.loc[row,'sampID'])\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "popDict = OrderedDict()\n",
    "total = 0\n",
    "for pop in ptsDict.keys():\n",
    "    popDict[pop] = len(ptsDict[pop])\n",
    "    print pop,popDict[pop]\n",
    "    total += popDict[pop]\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/OutFLANK/global_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/OutFLANK/imputed_MAF.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/OutFLANK/covariances/randmatrices/randsnps/outflank_%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/sum(popDict.values()))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/0outfiles/outflank_%s_%s_imputedDVALS.txt'\n",
    "DIR = opdirname(filE)\n",
    "if not opexists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "\n",
    "\n",
    "''' % (str(k).zfill(2),str(l).zfill(2),\n",
    "       str(k).zfill(2),str(l).zfill(2))\n",
    "        filE = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/0pyfiles/get_rand_dvals_%s_%s.py' % (str(k).zfill(2),\n",
    "                                                                                                       str(l).zfill(2)\n",
    "                                                                                                      )\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make qsub files\n",
    "count = 0\n",
    "shcount = 0\n",
    "for i in range(20):\n",
    "    for j in range(50):\n",
    "        filE = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/0pyfiles/get_rand_dvals_%s_%s.py' % (str(i).zfill(2),\n",
    "                                                                                                       str(j).zfill(2)\n",
    "                                                                                                      )\n",
    "        if count == 0:\n",
    "            text = '''#!/bin/bash\n",
    "#$ -N snpsfile\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "python %s\n",
    "''' % filE\n",
    "        else:\n",
    "            newtext = '''\n",
    "python %s\n",
    "''' % filE\n",
    "            text = text + newtext\n",
    "        count += 1\n",
    "        if count == 40:\n",
    "            count = 0\n",
    "            filE = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/0runfiles/%s_run.sh' % str(shcount).zfill(2)\n",
    "            shcount += 1\n",
    "            if not op.exists(op.dirname(filE)):\n",
    "                os.makedirs(op.dirname(filE))\n",
    "            with open(filE,'w') as o:\n",
    "                o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### place median observed dij in distribution of median dij for random snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "DF = pd.read_csv('/home/lindb/wbp/OutFLANK/covariances/dvals/imputed_dvals.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "dvals = []\n",
    "for i,row in enumerate(DF.index):\n",
    "    for j,col in enumerate(DF.columns):\n",
    "        if i > j:\n",
    "            dvals.append(abs(DF.loc[row,col]))\n",
    "            \n",
    "DIR = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/0outfiles'\n",
    "files = ls(DIR)\n",
    "files = [f for f in files if f.startswith('outflank')]\n",
    "\n",
    "fcount = 0\n",
    "medvals = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(opjoin(DIR,f),header=0,index_col=0,sep=\"\\t\")\n",
    "    rvals = [] #random dij values\n",
    "    for i,row in enumerate(df.index):\n",
    "        for j,col in enumerate(df.columns):\n",
    "            if i>j:\n",
    "                rvals.append(df.loc[row,col])\n",
    "\n",
    "    \n",
    "    medvals.append(np.median([abs(x) for x in rvals]))\n",
    "\n",
    "    fcount += 1\n",
    "    if fcount % 100 == 0:\n",
    "        print fcount\n",
    "\n",
    "filE = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/0randmedvals/outflank_imputed_randmedvalues.txt'\n",
    "if not op.exists(op.dirname(filE)):\n",
    "    os.makedirs(op.dirname(filE))\n",
    "medvals = pd.DataFrame(medvals)\n",
    "medvals.to_csv(filE,header=None,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "medDF = pd.read_csv(filE,header=None,sep='\\t')\n",
    "medDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorts = sorted(medvals[0].tolist())\n",
    "n5th = sorts[949] #95th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this is the distribution of median values of random SNPs\n",
    "#red line is the 95th percentile (95th = 0.0006213)\n",
    "plt.hist(sorts)[2] \n",
    "plt.axvline(x=n5th,c=\"red\",linewidth=2,zorder=0) #should be zorder=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this is the distribution of observed Dvals\n",
    "#red line is the median value (median = 0.0043892)\n",
    "med = np.median(dvals)\n",
    "fig = plt.hist(dvals)[2]\n",
    "plt.axvline(x=med,c=\"red\",linewidth=5,zorder=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#how much bigger is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "np.median(dvals)/max(sorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.median(dvals)/n5th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#below what percentile of the observed dvals less than the maximum randdvals\n",
    "for i,medi in enumerate(sorted(dvals)):\n",
    "    if not medi < max(sorts):\n",
    "        print i,i/len(dvals)\n",
    "        break      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for i,D in enumerate(sorted(dvals)):\n",
    "    if not D < sorts[950]: #if the observed D-value isn't less than the 95th percentile of the random distribution of D\n",
    "        print i,i/len(dvals)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(sorts),med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n5th,med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# allele frequency shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using median abs Dij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dataframe to get population MAF across 8 pops using all n=244 samples (can't use this for GEMMA since pop sizes r diff\n",
    "impMAF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#population sizes\n",
    "popDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(outliersnps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise pops for empirical SNPs IDed by OutFLANK and calculate median Dij for each pop pair\n",
    "shiftDict = OrderedDict() \n",
    "kcount = 0\n",
    "for m,popm in enumerate(impMAF.columns):\n",
    "    print popm\n",
    "    shiftDict[popm] = OrderedDict()\n",
    "    for l,popl in enumerate(impMAF.columns):\n",
    "        if m>l: #only need to do the lower triangle\n",
    "            dijlist = []\n",
    "            for i,locusi in enumerate(outliersnps):\n",
    "                for j,locusj in enumerate(outliersnps):\n",
    "                    if i > j: #i=row, j=col : lower triangle \n",
    "                        sums =0\n",
    "                        kcount += 1\n",
    "                        for popk in [popm,popl]:\n",
    "                            qik = impMAF.loc[locusi,popk]        #get locusi maf for pop k\n",
    "                            qjk = impMAF.loc[locusj,popk]        #get locusj maf for pop k\n",
    "                            nk = popDict[popk]                   #N  individuals  in pop k\n",
    "                            \n",
    "                            globN = 2*(popDict[popm]+popDict[popl]) # number of alleles across 2 pops\n",
    "                            \n",
    "                            #get global mafs\n",
    "                            fqim = impMAF.loc[locusi,popm]        #minor allele locusi freq  in popm\n",
    "                            nqim = round(fqim*2*popDict[popm])    #minor allele locusi count in popm\n",
    "                            fqil = impMAF.loc[locusi,popl]        #minor allele locusi freq  in popl\n",
    "                            nqil = round(fqil*2*popDict[popl])    #minor allele locusi count in popl\n",
    "                            \n",
    "                            fqjm = impMAF.loc[locusj,popm]        #minor allele locusj freq  in popm\n",
    "                            nqjm = round(fqjm*2*popDict[popm])    #minor allele locusj count in popm\n",
    "                            fqjl = impMAF.loc[locusj,popl]        #minor allele locusj freq  in popl\n",
    "                            nqjl = round(fqjl*2*popDict[popl])    #minor allele locusj count in popl\n",
    "                            \n",
    "                            qi = (nqim+nqil)/((2*popDict[popm])+(2*popDict[popl])) #global maf locusi\n",
    "                            qj = (nqjm+nqjl)/((2*popDict[popm])+(2*popDict[popl])) #global maf locusj\n",
    "\n",
    "                            sums += (nk/(popDict[popm]+popDict[popl]))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                        dijlist.append(sums) #each pairwise pop comparison has a matrix of Dij\n",
    "                    else:\n",
    "                        pass #no redundancies, no diagonal. \n",
    "            shiftDict[popm][popl] = np.median([abs(d) for d in dijlist])\n",
    "        else:\n",
    "            shiftDict[popm][popl] = np.nan #no redundancies,no diagonal. will be faster to reflect across diag later\n",
    "#    if kcount > 1:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/freqshifts/pop_pairwise_dij.text'\n",
    "with open(filE,'w') as o:\n",
    "    key0 = shiftDict.keys()[0]\n",
    "    line = '\\t'.join(shiftDict[key0].keys()) + str('\\n')\n",
    "    o.write(\"%s\" % line)\n",
    "    for popk in shiftDict.keys():\n",
    "        text = str(popk)+'\\t'+'\\t'.join([str(d) for d in shiftDict[popk].values()])+'\\n'\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/freqshifts/pop_pairwise_dij.text'\n",
    "shiftDF = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "shiftDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shifts = []\n",
    "for i,row in enumerate(shiftDF.index):\n",
    "    for j,col in enumerate(shiftDF.columns):\n",
    "        if i > j:\n",
    "            shifts.append(shiftDF.loc[row,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,popi in enumerate(shiftDF.index):\n",
    "    for j,popj in enumerate(shiftDF.columns):\n",
    "        if i == j:\n",
    "            shiftDF.loc[popi,popj] =0\n",
    "        elif math.isnan(shiftDF.loc[popi,popj]) == True:\n",
    "            shiftDF.loc[popi,popj] = shiftDF.loc[popj,popi]\n",
    "shiftDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/bayenv2/distance_matrices/geographic_distances.txt'\n",
    "geodist = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "geodist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geodist.loc['Freel_Peak','Dicks_Pass'][:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geos = []\n",
    "for i,row in enumerate(geodist.index):\n",
    "    for j,col in enumerate(geodist.columns):\n",
    "        if i > j:\n",
    "            geos.append(float(geodist.loc[row,col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get rid of the km\n",
    "for i,popi in enumerate(geodist.index):\n",
    "    for j,popj in enumerate(geodist.columns):\n",
    "        if i>j:\n",
    "            geodist.loc[popi,popj] = float(geodist.loc[popi,popj][:-3])\n",
    "geodist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,popi in enumerate(geodist.index):\n",
    "    for j,popj in enumerate(geodist.columns):\n",
    "        if i == j:\n",
    "            geodist.loc[popi,popj] = 0\n",
    "        elif math.isnan(geodist.loc[popi,popj]) == True:\n",
    "            geodist.loc[popi,popj] = geodist.loc[popj,popi]\n",
    "geodist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/bayenv2/distance_matrices/geographic_distances.txt'\n",
    "geodist.to_csv(filE,header=True,index=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(shifts,geos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mantel shift vs geodist\n",
    "skbio.stats.distance.mantel(shiftDF,geodist,permutations=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#file made in 08_bayen2\n",
    "envdist = pd.read_csv('/home/lindb/wbp/bayenv2/matrices/environmental_distances.txt',header=0,index_col=0,sep='\\t')\n",
    "envdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "envs = []\n",
    "for i,row in enumerate(envdist.index):\n",
    "    for j,col in enumerate(envdist.columns):\n",
    "        if i > j:\n",
    "            envs.append(float(envdist.loc[row,col]))\n",
    "envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(envs,shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,popi in enumerate(envdist.index):\n",
    "    for j,popj in enumerate(envdist.columns):\n",
    "        if i == j:\n",
    "            envdist.loc[popi,popj] = 0\n",
    "        elif math.isnan(envdist.loc[popi,popj]) == True:\n",
    "            envdist.loc[popi,popj] = envdist.loc[popj,popi]\n",
    "envdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/bayenv2/distance_matrices/environmental_distances.txt'\n",
    "envdist.to_csv(filE,header=True,index_col=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file made in 08_bayen2\n",
    "envdist = pd.read_csv('/home/lindb/wbp/bayenv2/distance_matrices/environmental_distances.txt',header=0,index_col=0,sep='\\t')\n",
    "envdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mantel shift vs total environmental distance\n",
    "skbio.stats.distance.mantel(shiftDF,envdist,permutations=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envdf = pd.read_csv('/home/lindb/wbp/bayenv2/ENVIRONFILE_headerIDX.txt',header=0,index_col=0,sep='\\t')\n",
    "envdf = envdf.loc[:,[col for col in envdf.columns[:8]]]\n",
    "envdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get distance matrices for each of the environmental variables\n",
    "envdDict = OrderedDict()\n",
    "for env in envdf.index:\n",
    "    envdDict[env] = pd.DataFrame(index=[pop for pop in shiftDF.index],columns=[pop for pop in shiftDF.columns])\n",
    "    for i,popi in enumerate(envdf.columns):\n",
    "        for j,popj in enumerate(envdf.columns):\n",
    "            if i != j:\n",
    "                dist = abs(envdf.loc[env,popi]-envdf.loc[env,popj])\n",
    "                envdDict[env].loc[popi,popj] = dist\n",
    "            elif i == j:\n",
    "                envdDict[env][popi][popj] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envdDict['Ann-ppt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/distance_matrices/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for env in envdDict.keys():\n",
    "    print env\n",
    "    filE = '/home/lindb/wbp/distance_matrices/%s_dist_symm.txt' % env\n",
    "    envdDict[env].to_csv(filE,header=True,index=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mantel vs individual envs\n",
    "for env in envdDict:\n",
    "    mant = mantel(shiftDF,envdDict[env],permutations = 9999)\n",
    "    print env,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "appt = []\n",
    "for i,popi in enumerate(envdDict['Ann-ppt'].index):\n",
    "    for j,popj in enumerate(envdDict['Ann-ppt'].columns):\n",
    "        if i>j:\n",
    "            appt.append(envdDict['Ann-ppt'].loc[popi,popj])\n",
    "plt.scatter(shifts,appt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envdDict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "longs = []\n",
    "for i,popi in enumerate(envdDict['Lon'].index):\n",
    "    for j,popj in enumerate(envdDict['Lon'].columns):\n",
    "        if i>j:\n",
    "            longs.append(envdDict['Lon'].loc[popi,popj])\n",
    "plt.scatter(shifts,longs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pearsonr(appt,shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prmi = []\n",
    "for i,popi in enumerate(envdDict['Max-rad-input'].index):\n",
    "    for j,popj in enumerate(envdDict['Max-rad-input'].columns):\n",
    "        if i>j:\n",
    "            prmi.append(envdDict['Max-rad-input'].loc[popi,popj])\n",
    "plt.scatter(prmi,shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sand = []\n",
    "for i,popi in enumerate(envdDict['Sand'].index):\n",
    "    for j,popj in enumerate(envdDict['Sand'].columns):\n",
    "        if i>j:\n",
    "            sand.append(envdDict['Sand'].loc[popi,popj])\n",
    "plt.scatter(sand,shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "silt = []\n",
    "for i,popi in enumerate(envdDict['Silt'].index):\n",
    "    for j,popj in enumerate(envdDict['Silt'].columns):\n",
    "        if i>j:\n",
    "            silt.append(envdDict['Silt'].loc[popi,popj])\n",
    "plt.scatter(silt,shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WC3rdbar = []\n",
    "for i,popi in enumerate(envdDict['WC3rdbar'].index):\n",
    "    for j,popj in enumerate(envdDict['WC3rdbar'].columns):\n",
    "        if i>j:\n",
    "            WC3rdbar.append(envdDict['WC3rdbar'].loc[popi,popj])\n",
    "plt.scatter(WC3rdbar,shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test against phenotypic distances\n",
    "DIR = '/home/lindb/wbp/gemma/distance_matrices'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "phendf = OrderedDict()\n",
    "for f in files:\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    phendf[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "phendf[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skbio.stats.distance import mantel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shiftDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a shiftDF with only those pops in common gardens\n",
    "pops = ['Dicks_Pass','Freel_Peak','Little_Round_Top','Mt_Rose_Ophir','Rifle_Peak','Snow_Valley_Peak']\n",
    "cgshiftDF = pd.DataFrame(shiftDF[[col for col in shiftDF.columns if col in pops]])\n",
    "cgshiftDF = cgshiftDF[cgshiftDF.index.isin(pops)]\n",
    "cgshiftDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test against phenotypic distances\n",
    "for pheno in phendf:\n",
    "    mant = mantel(cgshiftDF,phendf[pheno],permutations=9999)\n",
    "    if mant[1] <= 0.05:\n",
    "        print pheno,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using 95th percentile Dij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#where is the 95th percentile for 110 choose 2 Dij?\n",
    "math.floor(5995*0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise pops for empirical SNPs IDed by OutFLANK and calculate median Dij for each pop pair\n",
    "n5thshiftDict = OrderedDict() \n",
    "kcount = 0\n",
    "for m,popm in enumerate(impMAF.columns):\n",
    "    print popm\n",
    "    n5thshiftDict[popm] = OrderedDict()\n",
    "    for l,popl in enumerate(impMAF.columns):\n",
    "        if m>l:\n",
    "            dijlist = []\n",
    "            for i,locusi in enumerate(outliersnps):\n",
    "                for j,locusj in enumerate(outliersnps):\n",
    "                    if i > j: #i=row, j=col : lower triangle \n",
    "                        sums =0\n",
    "                        kcount += 1\n",
    "                        for popk in [popm,popl]:\n",
    "                            qik = impMAF.loc[locusi,popk] #get locusi maf for pop k\n",
    "                            qjk = impMAF.loc[locusj,popk] #get locusj maf for pop k\n",
    "                            nk = popDict[popk]            #N individuals in pop k\n",
    "                            \n",
    "                            globN = 2*(popDict[popm]+popDict[popl]) # number of alleles across 2 pops\n",
    "                            \n",
    "                            #get global mafs\n",
    "                            fqim = impMAF.loc[locusi,popm]        #minor allele locusi freq  in popm\n",
    "                            nqim = round(fqim*2*popDict[popm])    #minor allele locusi count in popm\n",
    "                            fqil = impMAF.loc[locusi,popl]        #minor allele locusi freq  in popl\n",
    "                            nqil = round(fqil*2*popDict[popl])    #minor allele locusi count in popl\n",
    "                            \n",
    "                            fqjm = impMAF.loc[locusj,popm]        #minor allele locusj freq  in popm\n",
    "                            nqjm = round(fqjm*2*popDict[popm])    #minor allele locusj count in popm\n",
    "                            fqjl = impMAF.loc[locusj,popl]        #minor allele locusj freq  in popl\n",
    "                            nqjl = round(fqjl*2*popDict[popl])    #minor allele locusj count in popl\n",
    "                            \n",
    "                            qi = (nqim+nqil)/((2*popDict[popm])+(2*popDict[popl])) #global maf locusi\n",
    "                            qj = (nqjm+nqjl)/((2*popDict[popm])+(2*popDict[popl])) #global maf locusj\n",
    "\n",
    "                            sums += (nk/(popDict[popm]+popDict[popl]))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                        dijlist.append(sums) #each pairwise pop comparison has a matrix of Dij\n",
    "                    else:\n",
    "                        pass #no redundancies, no diagonal.\n",
    "            n5thshiftDict[popm][popl] = sorted([abs(d) for d in dijlist])[5695]\n",
    "        else:\n",
    "            n5thshiftDict[popm][popl] = np.nan #no redundancies,no diagonal. will be faster to reflect across diag later\n",
    "#    if kcount > 1:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/freqshifts/pop_pairwise_95th_dij.text'\n",
    "with open(filE,'w') as o:\n",
    "    key0 = n5thshiftDict.keys()[0]\n",
    "    line = '\\t'.join(n5thshiftDict[key0].keys()) + str('\\n')\n",
    "    o.write(\"%s\" % line)\n",
    "    for popk in n5thshiftDict.keys():\n",
    "        text = str(popk)+'\\t'+'\\t'.join([str(d) for d in n5thshiftDict[popk].values()])+'\\n'\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/freqshifts/pop_pairwise_95th_dij.text'\n",
    "n5thshiftDF = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "for i,popi in enumerate(n5thshiftDF.index):\n",
    "    for j,popj in enumerate(n5thshiftDF.columns):\n",
    "        if i == j:\n",
    "            n5thshiftDF.loc[popi,popj] = 0\n",
    "        elif math.isnan(n5thshiftDF.loc[popi,popj]) == True:\n",
    "            n5thshiftDF.loc[popi,popj] = n5thshiftDF.loc[popj,popi]\n",
    "n5thshiftDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in envdDict.keys():\n",
    "    print env,skbio.stats.distance.mantel(n5thshiftDF,envdDict[env],permutations=9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using max abs Dij values instead of median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise pops for empirical SNPs IDed by OutFLANK and calculate median Dij for each pop pair\n",
    "maxshiftDict = OrderedDict() \n",
    "kcount = 0\n",
    "for m,popm in enumerate(impMAF.columns):\n",
    "    print popm\n",
    "    maxshiftDict[popm] = OrderedDict()\n",
    "    for l,popl in enumerate(impMAF.columns):\n",
    "        if m>l:\n",
    "            dijlist = []\n",
    "            for i,locusi in enumerate(outliersnps):\n",
    "                for j,locusj in enumerate(outliersnps):\n",
    "                    if i > j: #i=row, j=col : lower triangle \n",
    "                        sums =0\n",
    "                        kcount += 1\n",
    "                        for popk in [popm,popl]:\n",
    "                            qik = impMAF.loc[locusi,popk] #get locusi maf for pop k\n",
    "                            qjk = impMAF.loc[locusj,popk] #get locusj maf for pop k\n",
    "                            nk = popDict[popk]            #N individuals in pop k\n",
    "                            \n",
    "                            globN = 2*(popDict[popm]+popDict[popl]) # number of alleles across 2 pops\n",
    "                            \n",
    "                            #get global mafs\n",
    "                            fqim = impMAF.loc[locusi,popm]        #minor allele locusi freq  in popm\n",
    "                            nqim = round(fqim*2*popDict[popm])    #minor allele locusi count in popm\n",
    "                            fqil = impMAF.loc[locusi,popl]        #minor allele locusi freq  in popl\n",
    "                            nqil = round(fqil*2*popDict[popl])    #minor allele locusi count in popl\n",
    "                            \n",
    "                            fqjm = impMAF.loc[locusj,popm]        #minor allele locusj freq  in popm\n",
    "                            nqjm = round(fqjm*2*popDict[popm])    #minor allele locusj count in popm\n",
    "                            fqjl = impMAF.loc[locusj,popl]        #minor allele locusj freq  in popl\n",
    "                            nqjl = round(fqjl*2*popDict[popl])    #minor allele locusj count in popl\n",
    "                            \n",
    "                            qi = (nqim+nqil)/((2*popDict[popm])+(2*popDict[popl])) #global maf locusi\n",
    "                            qj = (nqjm+nqjl)/((2*popDict[popm])+(2*popDict[popl])) #global maf locusj\n",
    "\n",
    "                            sums += (nk/(popDict[popm]+popDict[popl]))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                        dijlist.append(sums) #each pairwise pop comparison has a matrix of Dij\n",
    "                    else:\n",
    "                        pass #no redundancies, no diagonal.\n",
    "            maxshiftDict[popm][popl] = max([abs(d) for d in dijlist])\n",
    "        else:\n",
    "            maxshiftDict[popm][popl] = np.nan #no redundancies,no diagonal. will be faster to reflect across diag later\n",
    "#    if kcount > 1:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/freqshifts/pop_pairwise_max_dij.text'\n",
    "with open(filE,'w') as o:\n",
    "    key0 = maxshiftDict.keys()[0]\n",
    "    line = '\\t'.join(maxshiftDict[key0].keys()) + str('\\n')\n",
    "    o.write(\"%s\" % line)\n",
    "    for popk in maxshiftDict.keys():\n",
    "        text = str(popk)+'\\t'+'\\t'.join([str(d) for d in maxshiftDict[popk].values()])+'\\n'\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/freqshifts/pop_pairwise_max_dij.text'\n",
    "maxshiftDF = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "for i,popi in enumerate(maxshiftDF.index):\n",
    "    for j,popj in enumerate(maxshiftDF.columns):\n",
    "        if i == j:\n",
    "            maxshiftDF.loc[popi,popj] = 0\n",
    "        elif math.isnan(maxshiftDF.loc[popi,popj]) == True:\n",
    "            maxshiftDF.loc[popi,popj] = maxshiftDF.loc[popj,popi]\n",
    "maxshiftDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in envdDict.keys():\n",
    "    print env,skbio.stats.distance.mantel(maxshiftDF,envdDict[env],permutations=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxs = []\n",
    "for i,popi in enumerate(maxshiftDF.index):\n",
    "    for j,popj in enumerate(maxshiftDF.columns):\n",
    "        if i > j:\n",
    "            maxs.append(maxshiftDF.loc[popi,popj])\n",
    "len(maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(maxs,appt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### using raw Dij values instead of absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise pops for empirical SNPs IDed by OutFLANK and calculate median Dij for each pop pair\n",
    "rawshiftDict = OrderedDict() \n",
    "kcount = 0\n",
    "for m,popm in enumerate(impMAF.columns):\n",
    "    print popm\n",
    "    rawshiftDict[popm] = OrderedDict()\n",
    "    for l,popl in enumerate(impMAF.columns):\n",
    "        if m>l:\n",
    "            dijlist = []\n",
    "            for i,locusi in enumerate(outliersnps):\n",
    "                for j,locusj in enumerate(outliersnps):\n",
    "                    if i > j: #i=row, j=col : lower triangle \n",
    "                        sums =0\n",
    "                        kcount += 1\n",
    "                        for popk in [popm,popl]:\n",
    "                            qik = impMAF.loc[locusi,popk] #get locusi maf for pop k\n",
    "                            qjk = impMAF.loc[locusj,popk] #get locusj maf for pop k\n",
    "                            nk = popDict[popk]            #N individuals in pop k\n",
    "                            \n",
    "                            globN = 2*(popDict[popm]+popDict[popl]) # number of alleles across 2 pops\n",
    "                            \n",
    "                            #get global mafs\n",
    "                            fqim = impMAF.loc[locusi,popm]        #minor allele locusi freq  in popm\n",
    "                            nqim = round(fqim*2*popDict[popm])    #minor allele locusi count in popm\n",
    "                            fqil = impMAF.loc[locusi,popl]        #minor allele locusi freq  in popl\n",
    "                            nqil = round(fqil*2*popDict[popl])    #minor allele locusi count in popl\n",
    "                            \n",
    "                            fqjm = impMAF.loc[locusj,popm]        #minor allele locusj freq  in popm\n",
    "                            nqjm = round(fqjm*2*popDict[popm])    #minor allele locusj count in popm\n",
    "                            fqjl = impMAF.loc[locusj,popl]        #minor allele locusj freq  in popl\n",
    "                            nqjl = round(fqjl*2*popDict[popl])    #minor allele locusj count in popl\n",
    "                            \n",
    "                            qi = (nqim+nqil)/((2*popDict[popm])+(2*popDict[popl])) #global maf locusi\n",
    "                            qj = (nqjm+nqjl)/((2*popDict[popm])+(2*popDict[popl])) #global maf locusj\n",
    "\n",
    "                            sums += (nk/(popDict[popm]+popDict[popl]))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                        dijlist.append(sums) #each pairwise pop comparison has a matrix of Dij\n",
    "                    else:\n",
    "                        pass #no redundancies, no diagonal. will be faster to reflect across diagonal later on\n",
    "            rawshiftDict[popm][popl] = np.median([d for d in dijlist])\n",
    "        else:\n",
    "            rawshiftDict[popm][popl] = np.nan #no redundancies,no diagonal. will be faster to reflect across diag later\n",
    "#    if kcount > 1:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write to file\n",
    "filE = '/home/lindb/wbp/OutFLANK/freqshifts/pop_pairwise_raw_median_dij.text'\n",
    "with open(filE,'w') as o:\n",
    "    key0 = rawshiftDict.keys()[0]\n",
    "    line = '\\t'.join(rawshiftDict[key0].keys()) + str('\\n')\n",
    "    o.write(\"%s\" % line)\n",
    "    for popk in rawshiftDict.keys():\n",
    "        text = str(popk)+'\\t'+'\\t'.join([str(d) for d in rawshiftDict[popk].values()])+'\\n'\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/OutFLANK/freqshifts/pop_pairwise_raw_median_dij.text'\n",
    "rawshiftDF = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "for i,popi in enumerate(rawshiftDF.index):\n",
    "    for j,popj in enumerate(rawshiftDF.columns):\n",
    "        if i == j:\n",
    "            rawshiftDF.loc[popi,popj] = 0\n",
    "        elif math.isnan(rawshiftDF.loc[popi,popj]) == True:\n",
    "            rawshiftDF.loc[popi,popj] = rawshiftDF.loc[popj,popi]\n",
    "rawshiftDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in envdDict.keys():\n",
    "    print env,skbio.stats.distance.mantel(rawshiftDF,envdDict[env],permutations=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#below scripts aren't used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# visualizing allele frequency shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxx = -1000\n",
    "for col in shiftDF.columns:\n",
    "    lst = [x for x in mirshiftDF[col].tolist() if math.isnan(x)==False]\n",
    "    if len(lst) >0:\n",
    "        m = max(lst)\n",
    "        if m > maxx:\n",
    "            maxx = m\n",
    "maxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#same geographic arrangement around lake tahoe\n",
    "pops = ['Heavenly',\n",
    "        'Freel_Peak',\n",
    "        'Little_Round_Top',\n",
    "        'Dicks_Pass',\n",
    "        'West_Shore_Peaks',\n",
    "        'Rifle_Peak',\n",
    "        'Mt_Rose_Ophir',\n",
    "        'Snow_Valley_Peak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "comps = []\n",
    "for i,popi in enumerate(pops):\n",
    "    for j,popj in enumerate(pops):\n",
    "        if i > j:\n",
    "            lst.append((i,j))\n",
    "            comps.append((popi,popj))\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = Graph(lst)\n",
    "g.vs[\"name\"] = pops\n",
    "g.vs[\"label\"] = g.vs[\"name\"]\n",
    "layout = g.layout_circle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style = {}\n",
    "#style[\"edge_width\"] = [(4*(shiftDF.loc[comps[i][0],comps[i][1]]/maxx)) for i in range(len(comps))]\n",
    "style[\"edge_width\"] = [100*(mirshiftDF.loc[comps[i][0],comps[i][1]]) for i in range(len(comps))]\n",
    "style[\"layout\"] = layout\n",
    "plot(g,**style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shiftDict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shiftDF.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = {\n",
    "    'nodes': {\n",
    "        'Dicks_Pass': {},\n",
    "        'Freel_Peak': {},\n",
    "        'Heavenly': {},\n",
    "        'Little_Round_Top': {},\n",
    "        'Mt_Rose_Ophir': {},\n",
    "        'Rifle_Peak': {},\n",
    "        'Snow_Valley_Peak': {},\n",
    "        'West_Shore_Peaks': {},\n",
    "    },\n",
    "    'edges': [\n",
    "        {'source': 'Dicks_Pass', 'target': 'Freel_Peak', 'size': shiftDF.loc['Freel_Peak','Dicks_Pass']/maxx},\n",
    "        {'source': 'Dicks_Pass', 'target': 'Heavenly', 'size': shiftDF.loc['Heavenly','Dicks_Pass']/maxx},\n",
    "        {'source': 'Dicks_Pass', 'target': 'Little_Round_Top', 'size': shiftDF.loc['Little_Round_Top','Dicks_Pass']/maxx},\n",
    "        {'source': 'Dicks_Pass', 'target': 'Mt_Rose_Ophir', 'size': shiftDF.loc['Little_Round_Top','Dicks_Pass']/maxx},\n",
    "        {'source': 'Dicks_Pass', 'target': 'Rifle_Peak', 'size': shiftDF.loc['Rifle_Peak','Dicks_Pass']/maxx},\n",
    "        {'source': 'Dicks_Pass', 'target': 'Snow_Valley_Peak', 'size': shiftDF.loc['Snow_Valley_Peak','Dicks_Pass']/maxx},\n",
    "        {'source': 'Dicks_Pass', 'target': 'West_Shore_Peaks', 'size': shiftDF.loc['West_Shore_Peaks','Dicks_Pass']/maxx},\n",
    "        {'source': 'Freel_Peak', 'target': 'Heavenly', 'size': shiftDF.loc['Heavenly','Freel_Peak']/maxx},\n",
    "        {'source': 'Freel_Peak', 'target': 'Little_Round_Top', 'size': shiftDF.loc['Little_Round_Top','Freel_Peak']/maxx},\n",
    "        {'source': 'Freel_Peak', 'target': 'Mt_Rose_Ophir', 'size': shiftDF.loc['Mt_Rose_Ophir','Freel_Peak']/maxx},\n",
    "        {'source': 'Freel_Peak', 'target': 'Rifle_Peak', 'size': shiftDF.loc['Rifle_Peak','Freel_Peak']/maxx},\n",
    "        {'source': 'Freel_Peak', 'target': 'Snow_Valley_Peak', 'size': shiftDF.loc['Snow_Valley_Peak','Freel_Peak']/maxx},\n",
    "        {'source': 'Freel_Peak', 'target': 'West_Shore_Peaks', 'size': shiftDF.loc['West_Shore_Peaks','Freel_Peak']/maxx},\n",
    "        {'source': 'Heavenly', 'target': 'Little_Round_Top', 'size': shiftDF.loc['Little_Round_Top','Heavenly']/maxx},\n",
    "        {'source': 'Heavenly', 'target': 'Mt_Rose_Ophir', 'size': shiftDF.loc['Mt_Rose_Ophir','Heavenly']/maxx},\n",
    "        {'source': 'Heavenly', 'target': 'Rifle_Peak', 'size': shiftDF.loc['Rifle_Peak','Heavenly']/maxx},\n",
    "        {'source': 'Heavenly', 'target': 'Snow_Valley_Peak', 'size': shiftDF.loc['Snow_Valley_Peak','Heavenly']/maxx},\n",
    "        {'source': 'Heavenly', 'target': 'West_Shore_Peaks', 'size': shiftDF.loc['West_Shore_Peaks','Heavenly']/maxx},\n",
    "        {'source': 'Little_Round_Top', 'target': 'Mt_Rose_Ophir', 'size': shiftDF.loc['Mt_Rose_Ophir','Little_Round_Top']/maxx},\n",
    "        {'source': 'Little_Round_Top', 'target': 'Rifle_Peak', 'size': shiftDF.loc['Rifle_Peak','Little_Round_Top']/maxx},\n",
    "        {'source': 'Little_Round_Top', 'target': 'Snow_Valley_Peak', 'size': shiftDF.loc['Snow_Valley_Peak','Little_Round_Top']/maxx},\n",
    "        {'source': 'Little_Round_Top', 'target': 'West_Shore_Peaks', 'size': shiftDF.loc['West_Shore_Peaks','Little_Round_Top']/maxx},\n",
    "        {'source': 'Mt_Rose_Ophir', 'target': 'Rifle_Peak', 'size': shiftDF.loc['Rifle_Peak','Mt_Rose_Ophir']/maxx},\n",
    "        {'source': 'Mt_Rose_Ophir', 'target': 'Snow_Valley_Peak', 'size': shiftDF.loc['Snow_Valley_Peak','Mt_Rose_Ophir']/maxx},\n",
    "        {'source': 'Mt_Rose_Ophir', 'target': 'West_Shore_Peaks', 'size': shiftDF.loc['West_Shore_Peaks','Mt_Rose_Ophir']/maxx},\n",
    "        {'source': 'Rifle_Peak', 'target': 'Snow_Valley_Peak', 'size': shiftDF.loc['Snow_Valley_Peak','Rifle_Peak']/maxx},\n",
    "        {'source': 'Rifle_Peak', 'target': 'West_Shore_Peaks', 'size': shiftDF.loc['West_Shore_Peaks','Rifle_Peak']/maxx},\n",
    "        {'source': 'Snow_Valley_Peak', 'target': 'West_Shore_Peaks', 'size': shiftDF.loc['West_Shore_Peaks','Snow_Valley_Peak']/maxx},\n",
    "        \n",
    "    ]\n",
    "}\n",
    "\n",
    "jgraph.draw(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layout = graph.layout(\"circle\")\n",
    "plot(graph,layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?jgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old way of calculating covariance using random snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this section was done before using H_exp (above)\n",
    "#the folders here were moved from /covariances to /covariances_unweighted_allele_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise to get D\n",
    "dDict = OrderedDict()\n",
    "icount = 0\n",
    "for i,locusi in enumerate(outliersnps):\n",
    "    dDict[locusi] = OrderedDict()\n",
    "    for j,locusj in enumerate(outliersnps):\n",
    "        if i > j: #i=row, j=col : lower triangle\n",
    "            asums = 0\n",
    "            bsums = 0\n",
    "            csums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                p_i = impMAF.loc[locusi,pop]\n",
    "                p_j = impMAF.loc[locusj,pop]\n",
    "\n",
    "                #calc \"a\"\n",
    "                product = p_i*p_j\n",
    "                asums = asums + product\n",
    "\n",
    "                #calc \"b\"\n",
    "                bsums = bsums + p_i\n",
    "\n",
    "                #calc \"c\" \n",
    "                csums = csums + p_j\n",
    "\n",
    "            a = asums/len(impMAF.columns)\n",
    "            b = bsums/len(impMAF.columns)\n",
    "            c = csums/len(impMAF.columns)\n",
    "\n",
    "            d = a - (b*c)\n",
    "            dDict[locusi][locusj] = d\n",
    "        else:\n",
    "            dDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount % 10 == 0:\n",
    "        print icount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write out the file\n",
    "rowcount = 0\n",
    "filE = '/home/lindb/wbp/OutFLANK/covariances/dvals/imputed_dvals.txt'\n",
    "if not opexists(opdirname(filE)):\n",
    "    os.makedirs(opdirname(filE))\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dDict.keys()[0]\n",
    "    line = '\\t'.join(dDict[key0].keys()) + str('\\n')\n",
    "    o.write(\"%s\" % line)\n",
    "    for locusi in dDict.keys():\n",
    "        line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dDict[locusi].values()]) + str('\\n')\n",
    "        o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvals = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "dvals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(loci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of snps not IDed as outliers\n",
    "bucket = set(loci) - set(outliersnps)\n",
    "len(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of snps not IDed as outliers\n",
    "filE = '/home/lindb/wbp/OutFLANK/covariances/drawbuckets/outflank_bucket.txt'\n",
    "snpbucket = pd.DataFrame([x for x in bucket])\n",
    "if not opexists(opdirname(filE)):\n",
    "    os.makedirs(opdirname(filE))\n",
    "snpbucket.to_csv(filE,header=True,index=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snpbucket = [x for x in snpbucket[0].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make 1000 dataframes with a set of snps == 110 = len(outliersnps)\n",
    "for i in range(20):                                  #make 20 .py files\n",
    "    for j in range(50):                              #each .py file makes 50 matrices\n",
    "        snps = random.sample(snpbucket,len(outliersnps))           #select random snps\n",
    "\n",
    "        DIR = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/randsnps'\n",
    "        if not opexists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = opjoin(DIR,\"outflank_%s_%s_randsnps.txt\" % (str(i).zfill(2),str(j).zfill(2)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(filE,header=None,sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make .py files\n",
    "for i in range(20):\n",
    "    for j in range(50):\n",
    "        text = '''from __future__ import division\n",
    "import sys, os, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "filE= '/home/lindb/wbp/OutFLANK/covariances/randmatrices/randsnps/outflank_%s_%s_randsnps.txt'\n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "snps = df[0].tolist()\n",
    "\n",
    "newdf = pd.read_csv('/home/lindb/wbp/OutFLANK/imputed_MAF.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "icount = 0\n",
    "rDict = OrderedDict()\n",
    "for i,locusi in enumerate(snps):\n",
    "    rDict[locusi] = OrderedDict()\n",
    "    for j,locusj in enumerate(snps):\n",
    "        if i > j: #i=row, j=col : lower tri\n",
    "            asums = 0\n",
    "            bsums = 0\n",
    "            csums = 0\n",
    "            for pop in newdf.columns:\n",
    "                p_i = newdf.loc[locusi,pop]\n",
    "                p_j = newdf.loc[locusj,pop]\n",
    "                \n",
    "                #calc \"a\"\n",
    "                product = p_i*p_j\n",
    "                asums = asums + product\n",
    "                \n",
    "                #calc \"b\"\n",
    "                bsums = bsums + p_i\n",
    "                \n",
    "                #calc \"c\"\n",
    "                csums = csums + p_j\n",
    "            \n",
    "            a = asums/len(newdf.columns)\n",
    "            b = bsums/len(newdf.columns)\n",
    "            c = csums/len(newdf.columns)\n",
    "            \n",
    "            d = a - (b*c)\n",
    "            rDict[locusi][locusj] = d\n",
    "        else:\n",
    "            rDict[locusi][locusj] = np.nan\n",
    "    icount += 1\n",
    "    if icount %% 10 == 0:\n",
    "        print icount\n",
    "\n",
    "filE = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/0outfiles/outflank_%s_%s_imputedDVALS.txt'\n",
    "DIR = opdirname(filE)\n",
    "if not opexists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\\\t'.join(snps) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in rDict.keys():\n",
    "        line = str(locusi) + '\\\\t' + '\\\\t'.join([str(x) for x in rDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "    \n",
    "''' % (str(i).zfill(2),str(j).zfill(2), \n",
    "       str(i).zfill(2),str(j).zfill(2))\n",
    "        \n",
    "        filE = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/0pyfiles/outflank_%s_%s_imputed.py' % (str(i).zfill(2),\n",
    "                                                                                                         str(j).zfill(2))\n",
    "        DIR = opdirname(filE)\n",
    "        if not opexists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(text)\n",
    "        o.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/0pyfiles/'\n",
    "files = ls(DIR)\n",
    "files = [opjoin(DIR,f) for f in files]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make <=198 sh files to include imputed py files too (my qsub limit is 200, I have 2 things going at the moment\n",
    "fcount =0\n",
    "shcount =0\n",
    "tcount =0\n",
    "newsh = True\n",
    "for f in sorted(files):\n",
    "    if newsh == True:\n",
    "        text = '''#!/bin/bash\n",
    "#$ -N run%s \n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "''' % str(shcount).zfill(3)\n",
    "    newtext = '''\n",
    "cd %s\n",
    "python %s\n",
    "''' % (opdirname(f),opbasename(f))\n",
    "    text = text + newtext\n",
    "    \n",
    "    fcount += 1\n",
    "    tcount += 1\n",
    "    newsh = False\n",
    "    if (fcount == 6) or (tcount == 1000):\n",
    "        newsh = True\n",
    "        fcount =0\n",
    "        filE = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/0runfiles/%s_run.sh' % str(shcount).zfill(3)\n",
    "        DIR = opdirname(filE)\n",
    "        if not opexists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(text)\n",
    "        o.close()\n",
    "        \n",
    "        shcount += 1\n",
    "    if '%max-rad-input_00_00_imputed.py' in f:\n",
    "        print \"shcount\",shcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opdirname(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check one to make sure it worked\n",
    "df = pd.read_csv('/home/lindb/wbp/OutFLANK/covariances/randmatrices/0outfiles/outflank_18_46_imputedDVALS.txt',header=0,\n",
    "                index_col = 0, sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# place observed median in distribution of medians made from random SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "DF = pd.read_csv('/home/lindb/wbp/OutFLANK/covariances/dvals/imputed_dvals.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "dvals = []\n",
    "for i,row in enumerate(DF.index):\n",
    "    for j,col in enumerate(DF.columns):\n",
    "        if i > j:\n",
    "            dvals.append(DF.loc[row,col])\n",
    "            \n",
    "DIR = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/0outfiles'\n",
    "files = ls(DIR)\n",
    "files = [f for f in files if f.startswith('outflank')]\n",
    "\n",
    "pvals = []\n",
    "fcount = 0\n",
    "allvals = []\n",
    "medvals = []\n",
    "for f in files:\n",
    "    df = pd.read_csv(opjoin(DIR,f),header=0,index_col=0,sep=\"\\t\")\n",
    "    rvals = [] #random dij values\n",
    "    for lst in df.values.tolist():\n",
    "        for x in lst:\n",
    "            if math.isnan(x) == False:\n",
    "                rvals.append(x)\n",
    "                allvals.append(x)\n",
    "    \n",
    "    medvals.append(np.median([abs(x) for x in rvals]))\n",
    "    pvals.append(ks_2samp(rvals,dvals)[1])\n",
    "    fcount += 1\n",
    "    if fcount % 10 == 0:\n",
    "        print fcount\n",
    "\n",
    "filE = '/home/lindb/wbp/OutFLANK/covariances/randmatrices/0OBSpvals/outflank_imputed_observedpvalues.txt'\n",
    "pvals = pd.DataFrame(pvals)\n",
    "pvals.to_csv(filE,header=None,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorts = sorted([abs(x) for x in medvals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(sorts)*.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorts[950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(sorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(medvals)[2] # this is the 1000 median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "med = np.median(dvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n5th = sorts[950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this is the distribution of median values of random SNPs\n",
    "#red line is the 95th percentile\n",
    "plt.hist(sorts)[2] \n",
    "plt.axvline(x=n5th,c=\"red\",linewidth=2,zorder=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this is the distribution of observed Dvals\n",
    "#red line is the median value\n",
    "fig = plt.hist([abs(x) for x in dvals])[2]\n",
    "plt.axvline(x=med,c=\"red\",linewidth=5,zorder=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.median(dvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.median(dvals) > max(sorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.median(dvals)/max(sorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#below what percentile of observed OBSdvals are the values less than the maximum randranddvals\n",
    "for i,med in enumerate(sorted(dvals)):\n",
    "    if not med < max(sorts):\n",
    "        print i,i/len(dvals)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorts[950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,med in enumerate(sorted(dvals)):\n",
    "    if not med < sorts[950]:\n",
    "        print i,i/len(dvals)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(loci),len(snpbucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ignore: effect distributions - effects pulled from 11_GEMMA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_dfs = {}\n",
    "DIR = '/home/lindb/wbp/gemma/infiles/bslmm/output/'\n",
    "for f in [op.join(DIR,f) for f in ls(DIR) if 'combined_df.txt' in f]:\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    combined_dfs[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "    print pheno\n",
    "    display(combined_dfs[pheno].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of effects across phenotypes for each of the outlier snps\n",
    "outfx = OrderedDict()\n",
    "for snp in outliersnps:\n",
    "    snp = str(snp)\n",
    "    outfx[snp] = OrderedDict()\n",
    "    outfx[snp]['alpha'] = []\n",
    "    outfx[snp]['beta'] = []\n",
    "    outfx[snp]['total'] = []\n",
    "    outfx[snp]['gamma'] = []\n",
    "    for pheno in combined_dfs:\n",
    "        outfx[snp]['alpha'].append(combined_dfs[pheno].loc[snp,'alpha_hmean'])\n",
    "        \n",
    "        outfx[snp]['beta'].append(combined_dfs[pheno].loc[snp,'beta_hmean'])\n",
    "        \n",
    "        outfx[snp]['total'].append(combined_dfs[pheno].loc[snp,'total_effect'])\n",
    "        \n",
    "        outfx[snp]['gamma'].append(combined_dfs[pheno].loc[snp,'gamma_hmean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get an average for each effect for each snp\n",
    "for snp in outfx:\n",
    "    outfx[snp]['mean alpha'] = np.mean([float(a) for a in outfx[snp]['alpha']])\n",
    "    outfx[snp]['mean beta']  = np.mean([b for b in outfx[snp]['beta']])\n",
    "    outfx[snp]['mean total'] = np.mean([t for t in outfx[snp]['total']])\n",
    "    outfx[snp]['mean gamma'] = np.mean([g for g in outfx[snp]['gamma']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outfx['NODE_1001690_length_90_cov_2.000000_37']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#put the means into a list\n",
    "outfx['total dist'] = []\n",
    "outfx['gamma dist'] = []\n",
    "outfx['alpha dist'] = []\n",
    "outfx['beta dist'] = []\n",
    "for snp in outfx:\n",
    "    if ' ' not in snp:\n",
    "        outfx['alpha dist'].append(outfx[snp]['mean alpha'])\n",
    "\n",
    "        outfx['beta dist'].append(outfx[snp]['mean beta'])\n",
    "\n",
    "        outfx['gamma dist'].append(outfx[snp]['mean gamma'])\n",
    "\n",
    "        outfx['total dist'].append(outfx[snp]['mean total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/OutFLANK/figures/effects_4_criteria.pdf') as pdf:\n",
    "#    crit = 'alpha_hmean'\n",
    "    \n",
    "#    plt.close('all')\n",
    "#    fig , ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "#    plotdict[count] = plt.subplot(int(\"23%s\" % count))\n",
    "    a = outfx['alpha dist']\n",
    "    b = outfx['beta dist']\n",
    "    c = outfx['gamma dist']\n",
    "    d = outfx['total dist']\n",
    "    bins=np.histogram(np.hstack((a,b,c,d)), bins=40)[1]\n",
    "    ax.hist(a,label='alpha (%s)' % str(round(np.median(a),8)),alpha=0.5,bins=bins)\n",
    "    ax.hist(b,label='beta (%s)' % str(round(np.median(b),5)),alpha=0.5,bins=bins)\n",
    "    ax.hist(c,label='gamma (%s)' % str(round(np.median(c),5)),alpha=0.5,bins=bins)\n",
    "    ax.hist(d,label='total (%s)' % str(round(np.median(d),5)),alpha=0.5,bins=bins)\n",
    "    \n",
    "#    plt.legend(['toppips','snpdict999','top alphas'])\n",
    "    plt.legend()\n",
    "#    plotdict[count].set_title('%s alpha' % pheno,y=.9,loc='left',fontsize=10,fontweight='bold')\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.set_xlabel('effect size')\n",
    "    ax.set_ylabel('count')\n",
    "    ax.locator_params(axis='x',nbins=4)\n",
    "\n",
    "    fig.set_size_inches(7,5)\n",
    "    pdf.savefig(fig,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(a,label='alpha dist',alpha=0.5,bins=bins)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(b,label='beta dist',alpha=0.5,bins=bins)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(c,label='gamma dist',alpha=0.5,bins=bins)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(d,label='total dist',alpha=0.5,bins=bins)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonoutlier = {}\n",
    "for pheno in combined_dfs:\n",
    "    loci = combined_dfs[pheno].index.tolist()\n",
    "    nonoutlier[pheno] = set(loci)-set(outliersnps)\n",
    "    print len(nonoutlier[pheno]),len(loci),len(loci)-110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(H.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonoutliers = set(H.index.tolist()) - set(outliersnps)\n",
    "len(nonoutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snpcount = 0\n",
    "nonfx = OrderedDict()\n",
    "for snp in nonoutliers:\n",
    "    if not snp in nonfx.keys():\n",
    "        nonfx[snp] = OrderedDict()\n",
    "        nonfx[snp]['alpha'] = []\n",
    "        nonfx[snp]['beta']  = []\n",
    "        nonfx[snp]['total'] = []\n",
    "        nonfx[snp]['gamma'] = []\n",
    "\n",
    "    [nonfx[snp]['alpha'].append(combined_dfs[pheno].loc[snp,'alpha_hmean']) for pheno in combined_dfs if snp in combined_dfs[pheno].index]\n",
    "\n",
    "    [nonfx[snp]['beta'].append(combined_dfs[pheno].loc[snp,'beta_hmean']) for pheno in combined_dfs if snp in combined_dfs[pheno].index]\n",
    "\n",
    "    [nonfx[snp]['total'].append(combined_dfs[pheno].loc[snp,'total_effect']) for pheno in combined_dfs if snp in combined_dfs[pheno].index]\n",
    "\n",
    "    [nonfx[snp]['gamma'].append(combined_dfs[pheno].loc[snp,'gamma_hmean']) for pheno in combined_dfs if snp in combined_dfs[pheno].index]\n",
    "    \n",
    "    snpcount += 1\n",
    "    if snpcount % 1000 == 0:\n",
    "        print snpcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write out the file since it took so GD long to make\n",
    "import pickle\n",
    "with open('/home/lindb/wbp/OutFLANK/nonoutlier_effects.pkl',\"wb\") as o:\n",
    "    pickle.dump(nonfx, o, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get an average for each effect for each snp\n",
    "for snp in nonfx:\n",
    "    nonfx[snp]['mean alpha'] = np.mean([float(a) for a in nonfx[snp]['alpha']])\n",
    "    nonfx[snp]['mean beta']  = np.mean([b for b in nonfx[snp]['beta']])\n",
    "    nonfx[snp]['mean total'] = np.mean([t for t in nonfx[snp]['total']])\n",
    "    nonfx[snp]['mean gamma'] = np.mean([g for g in nonfx[snp]['gamma']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put the means into a list\n",
    "nonfx['total dist'] = []\n",
    "nonfx['gamma dist'] = []\n",
    "nonfx['alpha dist'] = []\n",
    "nonfx['beta dist'] = []\n",
    "for snp in nonfx:\n",
    "    if ' ' not in snp:\n",
    "        nonfx['alpha dist'].append(nonfx[snp]['mean alpha'])\n",
    "\n",
    "        nonfx['beta dist'].append(nonfx[snp]['mean beta'])\n",
    "\n",
    "        nonfx['gamma dist'].append(nonfx[snp]['mean gamma'])\n",
    "\n",
    "        nonfx['total dist'].append(nonfx[snp]['mean total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/OutFLANK/figures/nonoutlier_effects_4_criteria.pdf') as pdf:\n",
    "#    crit = 'alpha_hmean'\n",
    "    \n",
    "#    plt.close('all')\n",
    "#    fig , ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "#    plotdict[count] = plt.subplot(int(\"23%s\" % count))\n",
    "    a = [A for A in nonfx['alpha dist'] if math.isnan(A)==False]\n",
    "    b = [B for B in nonfx['beta dist'] if math.isnan(B)==False]\n",
    "    c = [C for C in nonfx['gamma dist'] if math.isnan(C)==False]\n",
    "    d = [D for D in nonfx['total dist'] if math.isnan(D)==False]\n",
    "    bins=np.histogram(np.hstack((a,b,c,d)), bins=40)[1]\n",
    "    ax.hist(a,label='alpha (%s)' % str(round(np.median(a),8)),alpha=0.5,bins=bins)\n",
    "    ax.hist(b,label='beta (%s)' % str(round(np.median(b),5)),alpha=0.5,bins=bins)\n",
    "    ax.hist(c,label='gamma (%s)' % str(round(np.median(c),5)),alpha=0.5,bins=bins)\n",
    "    ax.hist(d,label='total (%s)' % str(round(np.median(d),5)),alpha=0.5,bins=bins)\n",
    "    \n",
    "#    plt.legend(['toppips','snpdict999','top alphas'])\n",
    "    plt.legend()\n",
    "#    plotdict[count].set_title('%s alpha' % pheno,y=.9,loc='left',fontsize=10,fontweight='bold')\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.set_xlabel('effect size')\n",
    "    ax.set_ylabel('count')\n",
    "    ax.locator_params(axis='x',nbins=4)\n",
    "\n",
    "    fig.set_size_inches(7,5)\n",
    "    pdf.savefig(fig,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for crit in ['alpha dist','beta dist','gamma dist','total dist']:\n",
    "    print crit\n",
    "    o = [B for B in outfx[crit] if math.isnan(B)==False]\n",
    "    n = [A for A in nonfx[crit] if math.isnan(A)==False]\n",
    "    print \"o=\",np.median(o)\n",
    "    print \"n=\",np.median(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(o),len(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import kruskalwallis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for crit in ['alpha dist','beta dist','gamma dist','total dist']:\n",
    "    print crit\n",
    "    o = [B for B in outfx[crit] if math.isnan(B)==False]\n",
    "    n = [A for A in nonfx[crit] if math.isnan(A)==False]\n",
    "    k = kruskalwallis(o,n)\n",
    "    print crit,k\n",
    "    print \"max o\",max(o)\n",
    "    print \"max n\",max(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/lindb/wbp/OutFLANK/OutFlank_results2.txt',header=0,sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min(df['He'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(df[df['OutlierFlag'] == True])\n",
    "len(df2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2['He'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

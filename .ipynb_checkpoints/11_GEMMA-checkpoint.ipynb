{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict,Counter,defaultdict\n",
    "import collections\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vcf\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create mean genotype file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of samples with phenotypes\n",
    "test = pd.read_csv('/home/lindb/wbp/WBP_IDS_MATCHED_POP_FINAL_02162016.csv',header=0,sep='\\t')\n",
    "cols = ['ID','Population_ID']\n",
    "[cols.append(col) for col in sorted(test.columns) if '_pop' in col]\n",
    "pheno_data = test[cols]\n",
    "samps = []\n",
    "rows = []\n",
    "for col in pheno_data.columns[-5:]:\n",
    "    for row in pheno_data.index:\n",
    "        if math.isnan(pheno_data.loc[row,col]) == False:\n",
    "            samp = pheno_data.loc[row,'ID']\n",
    "            if not samp in samps:\n",
    "                samps.append(samp)\n",
    "                rows.append(row)\n",
    "pheno_data = pheno_data.loc[[row for row in rows],:]\n",
    "pheno_data.index = pheno_data['ID'].tolist()\n",
    "pheno_data = pheno_data.loc[:,pheno_data.columns[1:]]\n",
    "pheno_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pheno_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "goodsamps = [x for x in pheno_data.index]\n",
    "len(goodsamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pDict = Counter()\n",
    "for row in pheno_data.index:\n",
    "    pDict[pheno_data.loc[row,'Population_ID']] += 1\n",
    "for pop in sorted(pDict.keys()):\n",
    "    print pop,pDict[pop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get freq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_dir = '/home/lindb/wbp/gemma/'\n",
    "VCFimputed = os.path.join(file_dir,'imputed_isect_one_per_contig.recode.vcf.gz_sorted.vcf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_genotype(gp): #GP is the list of likelihoods\n",
    "    total = 0\n",
    "    for i, val in enumerate(gp):\n",
    "        total += val*i\n",
    "    return np.round(total,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example of what happens\n",
    "imputed_reader = vcf.Reader(open(VCFimputed),'r')\n",
    "recCount = 0\n",
    "c = Counter()\n",
    "for rec in imputed_reader:\n",
    "    print rec\n",
    "    \n",
    "    sampDict = OrderedDict()\n",
    "    samps = 0\n",
    "    for sample in rec:   \n",
    "        #if sample.sample in goodsamps:\n",
    "        print sample\n",
    "        gt = sample['GT'].split('|')\n",
    "        if '.' not in gt:  #if '.' isn't in the sample genotype\n",
    "            c[gt[0]] += 1 #count the first allele using the allele as a key\n",
    "            c[gt[1]] += 1 #count the second allele \" \" \" \"\n",
    "            sampDict[sample.sample] = [gt, sample['GP']] #keep track of samp-spec gt and likelihood scores\n",
    "        else:\n",
    "            sampDict[sample.sample] = [gt, sample['GP']]\n",
    "\n",
    "        samps += 1\n",
    "        #if samps % 10 == 0:\n",
    "            #break\n",
    "    recCount += 1\n",
    "    if recCount % 10 == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#populations with seedlings in common gardens used to estimate phenotype\n",
    "pops = sorted(np.unique(pheno_data['Population_ID']).tolist())\n",
    "pops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(goodsamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the actual shindig\n",
    "recCount = 0\n",
    "locDict = OrderedDict()      #key = locus, value = lineDict\n",
    "imputed_reader = vcf.Reader(open(VCFimputed),'r')\n",
    "for rec in imputed_reader:   #for each locus\n",
    "    lineDict = OrderedDict()      #key = mean genotype file column name (eg 'locus', 'mnr', 'mjr', 'sampID_X')\n",
    "                                  #value = either locus, mjr_all, mnr_all, or samp-specific mean_genotype\n",
    "    sampDict = OrderedDict()      #key = sampID, value = [ gt , [PL_LST] ]\n",
    "        \n",
    "    ref = rec.REF\n",
    "    alt = rec.ALT[0]\n",
    "    locus = \"_\".join([str(rec.CHROM),str(rec.POS)])\n",
    "    lineDict['locus'] = locus\n",
    "    c = Counter()\n",
    "\n",
    "    samps = 0\n",
    "    for sample in rec: #get allele counts from a sorted list of sampleIDs\n",
    "        if sample.sample in goodsamps: #we only want allele freqs across the populations contributing to common gardens\n",
    "            gt = sample['GT'].split('|')\n",
    "            if '.' not in gt:  #if '.' isn't in the sample genotype\n",
    "                c[gt[0]] += 1 #count the first allele  using the allele as a key\n",
    "                c[gt[1]] += 1 #count the second allele \" \" \" \"\n",
    "                sampDict[sample.sample] = [gt, sample['GP']] #keep track of samp-spec gt and likelihood scores\n",
    "            else:\n",
    "                sampDict[sample.sample] = [gt, sample['GP']] #keep track of samp-spec gt and likelihood scores\n",
    "\n",
    "            samps += 1\n",
    "            #if samps % 20 == 0:\n",
    "                #break\n",
    "\n",
    "    if c['0'] < c['1']: #if the reference allele isn't the minor allele\n",
    "        lineDict['mnr'] = alt\n",
    "        lineDict['mjr'] = ref\n",
    "        \n",
    "        for samp in sampDict.keys(): \n",
    "            if '.' in sampDict[samp][0]: #if the genotype has a '.' (shouldn't be the case for imputed)\n",
    "                mean_genotype = \"NA\"\n",
    "                print \"stop you idiot, you've done something wrong\" #imputed data has missing values\n",
    "            else:\n",
    "                #calc mean genotype from sample['GP']\n",
    "                sampDict[samp][1] = list(reversed(sampDict[samp][1])) #reverse the sample['GP']\n",
    "                mean_genotype = get_mean_genotype(sampDict[samp][1])\n",
    "            \n",
    "            lineDict[samp] = mean_genotype #append the mean genotype score for each sample\n",
    "    else:\n",
    "        lineDict['mnr'] = ref\n",
    "        lineDict['mjr'] = alt\n",
    "        \n",
    "        for samp in sampDict.keys():\n",
    "            if '.' in sampDict[samp][0]: #if the genotype has a '.'\n",
    "                mean_genotype = \"NA\"\n",
    "            else:\n",
    "                #calc mean genotype from sample['GP']\n",
    "                mean_genotype = get_mean_genotype(sampDict[samp][1])\n",
    "                \n",
    "            lineDict[samp] = mean_genotype\n",
    "            \n",
    "        \n",
    "    locDict[locus] = lineDict\n",
    "    recCount += 1\n",
    "    #break\n",
    "    if recCount % 1000 == 0:\n",
    "        print recCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(locDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loc0 = locDict.keys()[0]\n",
    "keys = locDict[loc0].keys() #len = 88 samps + 'locus' + 'mjr' + 'mnr' = 91\n",
    "print len(keys),keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/mean_genotypes.txt'\n",
    "with open(filE,'w') as o:\n",
    "    loc0 = locDict.keys()[0]\n",
    "    keys = locDict[loc0].keys()\n",
    "    text = '\\t'.join([key for key in keys])+'\\n'\n",
    "    o.write(\"%s\" % text)\n",
    "    for locus in locDict.keys():\n",
    "        text = '\\t'.join([str(val) for val in locDict[locus].values()])+'\\n'\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/mean_genotypes.txt'\n",
    "meangene = pd.read_csv(filE,header=0,sep=\"\\t\")\n",
    "cols = ['locus','mnr','mjr']\n",
    "for col in sorted(meangene.columns):\n",
    "    if 'compiled' in col:\n",
    "        cols.append(col)\n",
    "meangene = meangene[cols]\n",
    "meangene.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(meangene.columns) #88 + 3 = 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/mean_genotypes.txt'\n",
    "meangene.to_csv(filE,header=True,index=False,sep=\"\\t\")\n",
    "filE2 = '/home/lindb/wbp/gemma/mean_genotypes_noHEADERIDX.txt'\n",
    "meangene.to_csv(filE2,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/infiles/mean_genotypes_noHEADERIDX.txt'\n",
    "if os.path.exists(filE):\n",
    "    os.remove(filE)\n",
    "os.symlink('/home/lindb/wbp/gemma/mean_genotypes_noHEADERIDX.txt',filE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get PCAs before making phenotype file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1) first need to verify order 0,1,2 as counts of minor allele\n",
    "# 2) then center and standardize each genotype\n",
    "# 3) then get PCAs from prcomp.R\n",
    "# 4) use Tracy-Widom to get sig PCs\n",
    "# 5) mult reg (get residuals): phen ~ PCs\n",
    "# 6) normal quantile transformation\n",
    "# 7) create GEMMA phenotype infiles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - assert order 0,1,2 as counts of minor allele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#z12 made in 6_pca.ipynb, symlinked from /gemma (012 refers to the count of the minor allele)\n",
    "#I need to go through these again since I've reduced the sample size and recode 012 for minor allele\n",
    "\n",
    "z12 = pd.read_csv('/home/lindb/wbp/gemma/imputed_z12_maf_swp_trans_z12.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "z12 = z12.ix[z12.index.isin(goodsamps),:] #reduce to only include samps from common garden pops\n",
    "z12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z12['NODE_1002790_length_94_cov_1.010638_30'] #before loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nz12 = pd.DataFrame(z12)\n",
    "goodcols=[]\n",
    "for col in nz12.columns:\n",
    "    zero = nz12[col].tolist().count(0)\n",
    "    one = nz12[col].tolist().count(1)\n",
    "    two = nz12[col].tolist().count(2)\n",
    "    \n",
    "    p = (2*zero)+ one\n",
    "    q = (2*two) + one\n",
    "    \n",
    "    if not p > q: #if 012 is counting minor allele, there will be more instances of 0s\n",
    "        print col\n",
    "        cols.append(col)\n",
    "        \n",
    "        nz12[col].replace({0:2,2:0},inplace =True)\n",
    "    else:\n",
    "        goodcols.append(col)\n",
    "    #print zero,one,two\n",
    "    #print zero+one+two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z12['NODE_1002790_length_94_cov_1.010638_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nz12['NODE_1002790_length_94_cov_1.010638_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check reordering\n",
    "for col in nz12.columns:\n",
    "    zero = nz12[col].tolist().count(0)\n",
    "    one = nz12[col].tolist().count(1)\n",
    "    two = nz12[col].tolist().count(2)\n",
    "    \n",
    "    p = (2*zero)+ one\n",
    "    q = (2*two) + one\n",
    "    \n",
    "    if not p >= q: #if 012 is counting minor allele, there will be more instances of 0s\n",
    "        print \"crap\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(nz12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/imputed_z12_maf_swp_trans_z12_6pop_maf_z12.txt'\n",
    "nz12.to_csv(filE,header=True,index=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/imputed_z12_maf_swp_trans_z12_6pop_maf_z12.txt'\n",
    "nz12 = pd.read_csv(filE,header=0,index_col=0,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check for fixed alleles\n",
    "fixed = []\n",
    "for locus in nz12.columns:\n",
    "    if len(np.unique(nz12[locus].tolist())) == 1:\n",
    "        fixed.append(locus)\n",
    "len(fixed)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newcols = [x for x in nz12.columns if x not in fixed]\n",
    "nnz12 = nz12.loc[:,newcols]\n",
    "nnz12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "116231-115632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/imputed_z12_maf_swp_trans_z12_6pop_maf_z12_nofixed.txt'\n",
    "nnz12.to_csv(filE,header=True,index=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/imputed_z12_maf_swp_trans_z12_6pop_maf_z12_nofixed.txt'\n",
    "nnz12 = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "nnz12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnz12.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - center and standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#call it z12 again\n",
    "filE = '/home/lindb/wbp/gemma/imputed_z12_maf_swp_trans_z12_6pop_maf_z12_nofixed.txt'\n",
    "z12 = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "z12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(z12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z12 = z12.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newz12 = preprocessing.scale(z12)\n",
    "imp_std = pd.DataFrame(newz12,columns=z12.columns,index=z12.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.std(imp_std['NODE_1000013_length_91_cov_1.802198_37'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#make sure all have std = 1\n",
    "for col in new2.columns:\n",
    "    if not round(np.std(new2[col]),1) == 1:\n",
    "        print 'crap',np.std(new2[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(imp_std['NODE_1000013_length_91_cov_1.802198_37'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.std(imp_std['NODE_1000013_length_91_cov_1.802198_37'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.std(imp_std['NODE_1000031_length_98_cov_2.000000_30'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(np.unique(imp_std).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(np.unique(imp_std).tolist()),min(np.unique(imp_std).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp_std.shape #88 x 115632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/imputed_z12_maf_swp_trans_z12_6pop_maf_z12_nofixed_std.txt' #bayenv\n",
    "imp_std.to_csv(filE,header=True,index=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/imputed_z12_maf_swp_trans_z12_6pop_maf_z12_nofixed_std_noIDX.txt' #bayenv\n",
    "imp_std.to_csv(filE,header=True,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/imputed_z12_maf_swp_trans_z12_6pop_maf_z12_nofixed_std.txt' #bayenv\n",
    "imp_std = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - get the PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_r(): \n",
    "    os.environ['R_HOME'] = '/home/lindb/g/R3/lib64/R/' \n",
    "    os.environ['LD_LIBRARY_PATH'] = \"%s/lib:%s\" % (os.environ['R_HOME'], \n",
    "                                                   os.environ['LD_LIBRARY_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "setup_r()\n",
    "import readline\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri as pd2ri\n",
    "pd2ri.activate()\n",
    "r = robjects.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext rpy2.ipython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(data.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary = r('summary')\n",
    "prcomp = r('prcomp')\n",
    "dframe = r('data.frame')\n",
    "fread = r('fread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time prcomp_res = prcomp(fread('/home/lindb/wbp/gemma/imputed_z12_maf_swp_trans_z12_6pop_maf_z12_nofixed_std_noIDX.txt'),scale=False,center=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r('saveRDS')(prcomp_res, \"/home/lindb/wbp/gemma/6pop_maf_z12_nofixed_std_noIDX_prcomp_res.rds\") #bayenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time imp_res = r('readRDS')(\"/home/lindb/wbp/gemma/6pop_maf_z12_nofixed_std_noIDX_prcomp_res.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcas = pd2ri.ri2py_dataframe(imp_res.rx2(\"x\"))\n",
    "pcas.index = imp_std.index\n",
    "pcas.columns = imp_res.rx2(\"x\").names.rx2(2)\n",
    "pcas = pd.DataFrame(pcas)\n",
    "pcas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pheno_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pheno_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phen_data = pheno_data[[col for col in pheno_data.columns if '_pop' in col]]\n",
    "phen_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impPCs = pd.merge(phen_data,pcas,left_index=True,right_index=True)\n",
    "impPCs.sort_index(inplace =True)\n",
    "impPCs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impPCs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/6pop_maf_z12_nofixed_std_noIDX_prcomp_res_mergedpheno.txt'\n",
    "impPCs.to_csv(filE,header=True,index=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/6pop_maf_z12_nofixed_std_noIDX_prcomp_res_mergedpheno.txt'\n",
    "impPCs = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "impPCs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impPCs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pops = ['Dicks_Pass',\n",
    " 'Freel_Peak',\n",
    " 'Little_Round_Top',\n",
    " 'Mt_Rose_Ophir',\n",
    " 'Rifle_Peak',\n",
    " 'Snow_Valley_Peak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popids = OrderedDict()\n",
    "for i,pop in enumerate(pops):\n",
    "    popids[pop] = i+1\n",
    "    print pop,popids[pop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impPCs['PC1'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stpDF = pd.read_csv('/home/lindb/wbp/sampsTOpop.txt',header=0,sep='\\t')\n",
    "stpDict = OrderedDict()\n",
    "for row in stpDF.index:\n",
    "    samp = stpDF.loc[row,'sampID']\n",
    "    if samp in goodsamps:\n",
    "        stpDict[samp] = stpDF.loc[row,'pop']\n",
    "len(stpDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm = mcolors.Normalize(min(popids.values()), max(popids.values()))\n",
    "legend = {}\n",
    "for row in impPCs.index:\n",
    "    samp = row\n",
    "    pop = stpDict[samp]\n",
    "    n = norm(popids[pop])\n",
    "    color = cm.rainbow(n)\n",
    "    legend[pop] = color\n",
    "    plt.scatter(impPCs.loc[row,'PC1'], \n",
    "                impPCs.loc[row,'PC2'], \n",
    "                s=50, \n",
    "                c=color)\n",
    "fig = plt.gcf()\n",
    "ax = plt.gca()\n",
    "cmap = plt.get_cmap()\n",
    "fig.set_size_inches(10,8)\n",
    "plt.title(\"PCA of n=%d imputed samples for gemma on %d loci across 6 populations\" % (len(impPCs), len(imp_std.columns)))\n",
    "#0.05294  0.01079\n",
    "#0.02051  0.01083\n",
    "imp = summary(imp_res).rx(\"importance\")[0]\n",
    "plt.xlabel(\"PC1 (%g)\" % imp.rx(2,1)[0])\n",
    "plt.ylabel(\"PC2 (%g)\" % imp.rx(2,2)[0])\n",
    "handles = []\n",
    "for pop in sorted(legend):\n",
    "    handles.append(mpatches.Patch(color=legend[pop], label=pop))\n",
    "plt.legend(handles=sorted(handles), loc=2,bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare with the full imputed PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gemma = r('readRDS')('/home/lindb/wbp/gemma/6pop_maf_z12_nofixed_std_noIDX_prcomp_res.rds')\n",
    "gemmapcas = pd2ri.ri2py_dataframe(imp_res.rx2(\"x\"))\n",
    "gemmapcas.index = imp_std.index\n",
    "gemmapcas.columns = imp_res.rx2(\"x\").names.rx2(2)\n",
    "gemmapcas = pd.DataFrame(gemmapcas)\n",
    "gemmapcas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gemmapcas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pca_x(res):\n",
    "    x = pd.DataFrame(pd2ri.ri2py(res.rx2(\"x\")))\n",
    "    x.index = res.rx2(\"x\").names[0]\n",
    "    x.columns = res.rx2(\"x\").names[1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beagle = r('readRDS')('/home/lindb/wbp/workingsnps/imputed/pca_res.rds')\n",
    "beaglepcas = get_pca_x(beagle)\n",
    "beaglepcas = beaglepcas[beaglepcas.index.isin(gemmapcas.index)]\n",
    "beaglepcas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beaglepcas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(beaglepcas),len(gemmapcas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(gemmapcas['PC1'],beaglepcas['PC1'])\n",
    "plt.title(\"beaglePC1 ~ gemmapPC1\")\n",
    "plt.xlabel(\"gemma\")\n",
    "plt.ylabel(\"beagle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(gemmapcas['PC2'],beaglepcas['PC2'])\n",
    "plt.title(\"beaglePC2 ~ gemmapPC2\")\n",
    "plt.xlabel(\"gemma\")\n",
    "plt.ylabel(\"beagle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter([x for x in range(0,10,1)],[y for y in range(10,20,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Use Tracy-Widom to get significant PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(data.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "data_imp = readRDS('/home/lindb/wbp/gemma/6pop_maf_z12_nofixed_std_noIDX_prcomp_res.rds') #bayenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "data_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/6pop_maf_z12_nofixed_std_noIDX_prcomp_res_PCs.txt' #bayenv\n",
    "PCs = impPCs.loc[:,[col for col in impPCs.columns if 'PC' in col]]\n",
    "PCs.to_csv(filE,header=True,index=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/6pop_maf_z12_nofixed_std_noIDX_prcomp_res_PCs_noIDX.txt' #bayenv\n",
    "PCs.to_csv(filE,header=True,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "source(\"/home/lindb/g/ipython/tw_calc.R\")\n",
    "twtable=read.table(\"/home/lindb/g/ipython/twTable.txt\", header=F)\n",
    "tw_imp = TWcalc(as.matrix(fread('/home/lindb/wbp/gemma/6pop_maf_z12_nofixed_std_noIDX_prcomp_res_PCs_noIDX.txt')),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw_imp = r(\"tw_imp[[2]]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sig_tracywidom(tw_p):\n",
    "    ps = []\n",
    "    for i, p in enumerate(tw_p):\n",
    "        if p > 0.05:\n",
    "            print(i, p)\n",
    "            break\n",
    "        else:\n",
    "            ps.append(p)\n",
    "    return len(ps), ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw_num = get_sig_tracywidom(tw_imp)\n",
    "tw_num #4 sig axes of structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - linear reg (get residuals): phen ~ PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impPCs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x for x in np.unique(impPCs).tolist() if math.isnan(x) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos = sorted(impPCs.columns[:5])\n",
    "phenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impStatDict = OrderedDict()\n",
    "phenCount = 0\n",
    "for phen in phenos:\n",
    "    data = pd.DataFrame()\n",
    "    data[phen] = impPCs[phen][[i for i,x in enumerate(impPCs[phen]) if not math.isnan(x)]]\n",
    "    data['PC1']  = impPCs['PC1'][[i for i,x in enumerate(impPCs[phen]) if not math.isnan(x)]]\n",
    "    data['PC2']  = impPCs['PC2'][[i for i,x in enumerate(impPCs[phen]) if not math.isnan(x)]]  \n",
    "    data = data.astype(float)\n",
    "    formula = str(\"%s~PC1+PC2\" % phen)\n",
    "    impStatDict[phen] = smf.ols(formula, data).fit()\n",
    "    phenCount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impResidDict = OrderedDict()\n",
    "for k,v in impStatDict.items():\n",
    "    data = impStatDict[k]\n",
    "    impResidDict[k] = data.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impResidDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impResidDict[phenos[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = phenos[0]\n",
    "for i,x in enumerate(impResidDict[p]):\n",
    "    print impResidDict[p].keys()[i]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impResidDf = pd.DataFrame()\n",
    "impResidDf[0] = [\"\" for x in imp_std.index]\n",
    "impResidDf.index = imp_std.index\n",
    "leng = 0\n",
    "count = 0\n",
    "for p in phenos:\n",
    "    for i,x in enumerate(impResidDict[p]):\n",
    "        SAMP = impResidDict[p].keys()[i]\n",
    "        #print i, SAMP\n",
    "        if SAMP.endswith('compiled'):\n",
    "            impResidDf.loc[SAMP,p] = x\n",
    "    #break\n",
    "impResidDf = impResidDf.loc[:,[col for col in phenos]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impResidDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x for x in np.unique(impResidDf).tolist() if math.isnan(x) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/6pop_maf_z12_nofixed_std_noIDX_prcomp_res_PCs_std.txt' #bayenv\n",
    "impResidDf.to_csv(filE,header=True,index=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - normal quantile transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%R -i impResidDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "htpopx  = qqnorm(impResidDf$ht_pop,plot.it=F)$x\n",
    "bfpopx  = qqnorm(impResidDf$bf_pop,plot.it=F)$x\n",
    "rspopx  = qqnorm(impResidDf$rs_pop,plot.it=F)$x\n",
    "c13popx = qqnorm(impResidDf$c13_pop,plot.it=F)$x\n",
    "n15popx = qqnorm(impResidDf$n15_pop,plot.it=F)$x\n",
    "#htfamx  = qqnorm(impResidDf$ht_fam,plot.it=F)$x\n",
    "#bffamx  = qqnorm(impResidDf$bf_fam,plot.it=F)$x\n",
    "#rsfamx  = qqnorm(impResidDf$rs_fam,plot.it=F)$x\n",
    "#c13famx = qqnorm(impResidDf$c13_fam,plot.it=F)$x\n",
    "#n15famx = qqnorm(impResidDf$n15_fam,plot.it=F)$x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impResidDf['htpopx'] = r('htpopx')\n",
    "impResidDf['bfpopx'] = r('bfpopx')\n",
    "impResidDf['rspopx'] = r('rspopx')\n",
    "impResidDf['c13popx'] = r('c13popx')\n",
    "impResidDf['n15popx'] = r('n15popx')\n",
    "#impResidDf['htfamx'] = r('htfamx')\n",
    "#impResidDf['bffamx'] = r('bffamx')\n",
    "#impResidDf['rsfamx'] = r('rsfamx')\n",
    "#impResidDf['c13famx'] = r('c13famx')\n",
    "#impResidDf['n15famx'] = r('n15famx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impResidDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/6pop_maf_z12_nofixed_std_noIDX_prcomp_res_PCs_std_qqnorm.txt'\n",
    "impResidDf.to_csv(filE,header=True,index=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - make phenotypic infiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/6pop_maf_z12_nofixed_std_noIDX_prcomp_res_PCs_std_qqnorm.txt'\n",
    "impResidDF = pd.read_csv(filE,header=0,index_col=0,sep=\"\\t\")\n",
    "impResidDF.fillna(\"NA\",inplace=True)\n",
    "impResidDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x for x in np.unique(impResidDF).tolist() if x == 'NA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filEs = []\n",
    "for col in impResidDF.columns[-5:]:\n",
    "    print col\n",
    "    filE = '/home/lindb/wbp/gemma/%s_infile_noHEADERIDX.txt' % col\n",
    "    filEs.append(filE)\n",
    "    print filE\n",
    "    df = pd.DataFrame(impResidDF[col])\n",
    "    df.to_csv(filE,header=False,index=False,sep=\"\\t\")\n",
    "    filE2 = '/home/lindb/wbp/gemma/%s_infile.txt' % col\n",
    "    df.to_csv(filE2,header=True,index=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meangene = pd.read_csv('/home/lindb/wbp/gemma/mean_genotypes.txt',header=0,sep=\"\\t\")\n",
    "meangene.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make sure index is same order as columns in meangene file (index = columns = sorted sample names)\n",
    "for i,samp in enumerate(df.index):\n",
    "    if samp != meangene.columns[i+3]:\n",
    "        print samp,meangene.columns[i+3]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in filEs:\n",
    "    print f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in filEs:\n",
    "    print f\n",
    "    bname = os.path.basename(f)\n",
    "    os.symlink(f,os.path.join('/home/lindb/wbp/gemma/infiles/',bname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make SNP annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/snp_annotation.txt'\n",
    "with open(filE,'w') as o:\n",
    "    for i,locus in enumerate(z12.columns):\n",
    "        #print locus\n",
    "        contig = '_'.join([x for x in locus.split(\"_\")[:-1]])\n",
    "        pos = locus.split('_')[-1]\n",
    "        #print contig\n",
    "        #print pos\n",
    "        text = '\\t'.join([contig,str(pos),str(i+1)])+'\\n'\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snp_ano = pd.read_csv(filE,header=None,sep='\\t')\n",
    "snp_ano.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snp_ano.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/home/lindb/wbp/gemma/infiles/snp_annotation.txt'):\n",
    "    os.remove('/home/lindb/wbp/gemma/infiles/snp_annotation.txt')\n",
    "os.symlink('/home/lindb/wbp/gemma/snp_annotation.txt','/home/lindb/wbp/gemma/infiles/snp_annotation.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make all_pheno file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('/home/lindb/wbp/gemma/')\n",
    "files = [os.path.join('/home/lindb/wbp/gemma/',f) for f in files if 'infile_noHEADERIDX' in f]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF = impResidDF[sorted(impResidDF.columns[-5:])]\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/all_pheno.txt'\n",
    "DF.to_csv(filE,header=True,index=True,sep='\\t')\n",
    "filE = '/home/lindb/wbp/gemma/all_pheno_noHEADERIDX.txt'\n",
    "DF.to_csv(filE,header=False,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/home/lindb/wbp/gemma/infiles/all_pheno_noHEADERIDX.txt'):\n",
    "    os.remove('/home/lindb/wbp/gemma/infiles/all_pheno_noHEADERIDX.txt')\n",
    "os.symlink('/home/lindb/wbp/gemma/all_pheno_noHEADERIDX.txt', '/home/lindb/wbp/gemma/infiles/all_pheno_noHEADERIDX.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allpheno = pd.read_csv('/home/lindb/wbp/gemma/all_pheno.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "allpheno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a2 = pd.read_csv('/home/lindb/wbp/gemma/infiles/all_pheno_noHEADERIDX.txt',header=None,sep=\"\\t\")\n",
    "a2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confirming infile requirements, updating if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locfile = pd.read_csv('/home/lindb/wbp/gemma/infiles/snp_annotation.txt',header=None,sep='\\t')\n",
    "locfile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locfile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a list of SNPs\n",
    "locs = ['_'.join([locfile.loc[row,0],str(locfile.loc[row,1])]) for row in locfile.index]\n",
    "len(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load meangenotype file created above, edit to only include SNPs that are not fixed across the 6 pops\n",
    "meangeno = pd.read_csv('/home/lindb/wbp/gemma/mean_genotypes.txt',header=0,sep='\\t')\n",
    "meangeno = meangeno.loc[meangeno['locus'].isin(locs)]\n",
    "meangeno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meangeno.shape #88 + 3 = 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/mean_genotypes.txt'\n",
    "meangeno.to_csv(filE,header=True,index=True,sep='\\t')\n",
    "filE = '/home/lindb/wbp/gemma/mean_genotypes_noHEADERIDX.txt'\n",
    "meangeno.to_csv(filE,header=False,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#how many fixed SNPs - because we called snps with 8 populations, not 6\n",
    "fixed = 0\n",
    "for col in z12.columns:\n",
    "    if len(np.unique(z12[col]).tolist()) == 1:\n",
    "        fixed += 1\n",
    "fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#how many samples do I expect?\n",
    "for col in ex.columns:\n",
    "    l = len(ex[ex[col] > -10000000]) #another way to filer NA, np.nan, or NaN\n",
    "    print col,l\n",
    "#I should use ht,c13, or n15 for matrix estimation because they have all of the individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GEMMA commands\n",
    "## Estimate relatedness matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "./gemma -g [filename] -p [filename] -gk [num] -o [prefix]\n",
    "\n",
    "    cd /home/lindb/wbp/gemma/infiles/\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p n15popx_infile_noHEADERIDX.txt \\\n",
    "    -gk 1 -o relatednessmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigen decomposition of relatedness matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "./gemma -g [filename] -p [filename] -k [filename] -eigen -o [prefix]\n",
    "\n",
    "    cd /home/lindb/wbp/gemma/infiles/\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p n15popx_infile_noHEADERIDX.txt \\\n",
    "    -k ./output/relatednessmatrix.cXX.txt -eigen -o decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Tests with Univariate Linear Mixed Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "./gemma -g [filename] -p [filename] -a [filename] -k [filename] -lmm [num] -o [prefix]\n",
    "\n",
    "    cd /home/lindb/wbp/gemma/infiles/\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p bfpopx_infile_noHEADERIDX.txt\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -o bfpopx_lmm\n",
    "\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p c13popx_infile_noHEADERIDX.txt\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -o c13popx_lmm\n",
    "\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p htpopx_infile_noHEADERIDX.txt\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -o htpopx_lmm\n",
    "\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p n15popx_infile_noHEADERIDX.txt\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -o n15popx_lmm\n",
    "\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p rspopx_infile_noHEADERIDX.txt\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -o rspopx_lmm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Tests with Multivariate Linear Mixed Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "./gemma -g [filename] -p [filename] -a [filename] -k [filename] -lmm [num] -n [num1] [num2] [num3] -o [prefix]\n",
    "\n",
    "    cd /home/lindb/wbp/gemma/infiles/\n",
    "    ###### bfpopx vs rest\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p all_pheno_noHEADERIDX.txt \\\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -n 1 2 -o bfpopx_c13popx\n",
    "\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p all_pheno_noHEADERIDX.txt \\\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -n 1 3 -o bfpopx_htpopx\n",
    "\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p all_pheno_noHEADERIDX.txt \\\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -n 1 4 -o bfpopx_n15popx    \n",
    "\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p all_pheno_noHEADERIDX.txt \\\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -n 1 5 -o bfpopx_rspopx\n",
    "\n",
    "    ###### c13popx vs rest\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p all_pheno_noHEADERIDX.txt \\\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -n 2 3 -o c13popx_htpopx\n",
    "\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p all_pheno_noHEADERIDX.txt \\\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -n 2 4 -o c13popx_n15popx    \n",
    "\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p all_pheno_noHEADERIDX.txt \\\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -n 2 5 -o c13popx_rspopx\n",
    "\n",
    "    ###### htpopx vs rest\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p all_pheno_noHEADERIDX.txt \\\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -n 3 4 -o htpopx_n15popx\n",
    "\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p all_pheno_noHEADERIDX.txt \\\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -n 3 5 -o htpopx_rspopx    \n",
    "\n",
    "    ###### n15popx vs rest\n",
    "    /home/lindb/g/src/gemma-0.94/bin/gemma -g mean_genotypes_noHEADERIDX.txt -p all_pheno_noHEADERIDX.txt \\\n",
    "    -a snp_annotation.txt -k ./output/relatednessmatrix.cXX.txt -lmm 4 -n 4 5 -o n15popx_rspopx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fit a Bayesian sparse linear mixed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ./gemma -g [filename] -p [filename] -a [filename] -k [filename] -bslmm [num] -o [prefix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_qsub(tokens):\n",
    "    filename, pheno, out, name = tokens\n",
    "    num = filename.split(\".\")[0][-1:]\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N BSLMM%s\n",
    "#$ -V\n",
    "#$ -cwd\n",
    "#$ -j y\n",
    "#$ -l mem_free=20G\n",
    "\n",
    "/home/lindb/g/src/gemma-0.94/bin/gemma \\\n",
    "-g /home/lindb/wbp/gemma/infiles/mean_genotypes_noHEADERIDX.txt \\\n",
    "-p %s \\\n",
    "-a /home/lindb/wbp/gemma/infiles/snp_annotation.txt \\\n",
    "-k /home/lindb/wbp/gemma/infiles/output/relatednessmatrix.cXX.txt \\\n",
    "-bslmm 1 \\\n",
    "-o %s \\\n",
    "-w 1000000 \\\n",
    "-s 100000000 \\\n",
    "-smin 1 \\\n",
    "-smax 300 \\\n",
    "-hmin 0.01 \\\n",
    "-hmax 0.9 \\\n",
    "-pmin -3 \\\n",
    "-pmax 0 \\\n",
    "-rpace 1000\n",
    "''' % (\"%s%s\" % (name[:3],num),\n",
    "       pheno,\n",
    "       out)\n",
    "    with open(filename,'w') as o:\n",
    "        o.write(\"%s\" % text)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos = [col for col in impResidDF.columns if 'x' in col]\n",
    "phenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    j = i+1\n",
    "    for pheno in phenos:\n",
    "        fname = '/home/lindb/wbp/gemma/infiles/bslmm/%s_bslmm_%s.sh'  % (pheno,j)\n",
    "        out =   '%s_bslmm_%s_out' % (pheno,j)\n",
    "        p =     '/home/lindb/wbp/gemma/infiles/%s_infile_noHEADERIDX.txt' % pheno\n",
    "        name =  op.basename(p).split(\"_\")[0]\n",
    "        tokens = [fname,p,out,name]\n",
    "        make_qsub(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_qsub_10mil(tokens):\n",
    "    filename, pheno, out, name = tokens\n",
    "    num = filename.split(\".\")[0][-1:]\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N BSLMM%s\n",
    "#$ -V\n",
    "#$ -cwd\n",
    "#$ -j y\n",
    "#$ -l mem_free=20G\n",
    "\n",
    "/home/lindb/g/src/gemma-0.94/bin/gemma \\\n",
    "-g /home/lindb/wbp/gemma/infiles/mean_genotypes_noHEADERIDX.txt \\\n",
    "-p %s \\\n",
    "-a /home/lindb/wbp/gemma/infiles/snp_annotation.txt \\\n",
    "-k /home/lindb/wbp/gemma/infiles/output/relatednessmatrix.cXX.txt \\\n",
    "-bslmm 1 \\\n",
    "-o %s \\\n",
    "-w 1000000 \\\n",
    "-s 10000000 \\\n",
    "-smin 1 \\\n",
    "-smax 300 \\\n",
    "-hmin 0.01 \\\n",
    "-hmax 0.9 \\\n",
    "-pmin -3 \\\n",
    "-pmax 0 \\\n",
    "-rpace 1000\n",
    "''' % (\"%s%s\" % (name[:3],num),\n",
    "       pheno,\n",
    "       out)\n",
    "    with open(filename,'w') as o:\n",
    "        o.write(\"%s\" % text)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_qsub_20mil(tokens):\n",
    "    filename, pheno, out, name = tokens\n",
    "    num = filename.split(\".\")[0][-1:]\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N BSLMM%s\n",
    "#$ -V\n",
    "#$ -cwd\n",
    "#$ -j y\n",
    "#$ -l mem_free=20G\n",
    "\n",
    "/home/lindb/g/src/gemma-0.94/bin/gemma \\\n",
    "-g /home/lindb/wbp/gemma/infiles/mean_genotypes_noHEADERIDX.txt \\\n",
    "-p %s \\\n",
    "-a /home/lindb/wbp/gemma/infiles/snp_annotation.txt \\\n",
    "-k /home/lindb/wbp/gemma/infiles/output/relatednessmatrix.cXX.txt \\\n",
    "-bslmm 1 \\\n",
    "-o %s \\\n",
    "-w 1000000 \\\n",
    "-s 20000000 \\\n",
    "-smin 1 \\\n",
    "-smax 300 \\\n",
    "-hmin 0.01 \\\n",
    "-hmax 0.9 \\\n",
    "-pmin -3 \\\n",
    "-pmax 0 \\\n",
    "-rpace 1000\n",
    "''' % (\"%s%s\" % (name[:3],num),\n",
    "       pheno,\n",
    "       out)\n",
    "    with open(filename,'w') as o:\n",
    "        o.write(\"%s\" % text)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_qsub_50mil(tokens):\n",
    "    filename, pheno, out, name = tokens\n",
    "    num = filename.split(\".\")[0][-1:]\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N BSLMM%s\n",
    "#$ -V\n",
    "#$ -cwd\n",
    "#$ -j y\n",
    "#$ -l mem_free=20G\n",
    "\n",
    "/home/lindb/g/src/gemma-0.94/bin/gemma \\\n",
    "-g /home/lindb/wbp/gemma/infiles/mean_genotypes_noHEADERIDX.txt \\\n",
    "-p %s \\\n",
    "-a /home/lindb/wbp/gemma/infiles/snp_annotation.txt \\\n",
    "-k /home/lindb/wbp/gemma/infiles/output/relatednessmatrix.cXX.txt \\\n",
    "-bslmm 1 \\\n",
    "-o %s \\\n",
    "-w 1000000 \\\n",
    "-s 50000000 \\\n",
    "-smin 1 \\\n",
    "-smax 300 \\\n",
    "-hmin 0.01 \\\n",
    "-hmax 0.9 \\\n",
    "-pmin -3 \\\n",
    "-pmax 0 \\\n",
    "-rpace 1000\n",
    "''' % (\"%s%s\" % (name[:3],num),\n",
    "       pheno,\n",
    "       out)\n",
    "    with open(filename,'w') as o:\n",
    "        o.write(\"%s\" % text)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    j = i+1\n",
    "    for pheno in phenos:\n",
    "        fname = '/home/lindb/wbp/gemma/infiles/bslmm/%s_bslmm_%s_10mil.sh'  % (pheno,j)\n",
    "        out =   '%s_bslmm_%s_out_10mil' % (pheno,j)\n",
    "        p =     '/home/lindb/wbp/gemma/infiles/%s_infile_noHEADERIDX.txt' % pheno\n",
    "        name =  op.basename(p).split(\"_\")[0]\n",
    "        tokens = [fname,p,out,name]\n",
    "        make_qsub_10mil(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    j = i+1\n",
    "    for pheno in phenos:\n",
    "        fname = '/home/lindb/wbp/gemma/infiles/bslmm/%s_bslmm_%s_20mil.sh'  % (pheno,j)\n",
    "        out =   '%s_bslmm_%s_out_20mil' % (pheno,j)\n",
    "        p =     '/home/lindb/wbp/gemma/infiles/%s_infile_noHEADERIDX.txt' % pheno\n",
    "        name =  op.basename(p).split(\"_\")[0]\n",
    "        tokens = [fname,p,out,name]\n",
    "        make_qsub_20mil(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    j = i+1\n",
    "    for pheno in phenos:\n",
    "        fname = '/home/lindb/wbp/gemma/infiles/bslmm/%s_bslmm_%s_50mil.sh'  % (pheno,j)\n",
    "        out =   '%s_bslmm_%s_out_50mil' % (pheno,j)\n",
    "        p =     '/home/lindb/wbp/gemma/infiles/%s_infile_noHEADERIDX.txt' % pheno\n",
    "        name =  op.basename(p).split(\"_\")[0]\n",
    "        tokens = [fname,p,out,name]\n",
    "        make_qsub_50mil(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### univariate association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/infiles/output/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'lmm.assoc' in f]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lmm = {}\n",
    "for f in files:\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    df = pd.read_csv(f,header=0,sep='\\t')\n",
    "    lmm[pheno] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lmm[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in lmm:\n",
    "    p = 0.05/len(lmm[pheno].index)\n",
    "    sigs = lmm[pheno][lmm[pheno]['p_wald'] <= p]\n",
    "    snps = sigs['rs'].tolist()\n",
    "    print pheno,p,len(snps),min(lmm[pheno]['p_wald']),min(lmm[pheno]['p_wald'])/p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/infiles/output/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'lmm.log' in f]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log = {}\n",
    "for f in files:\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    df = open(f,'r')\n",
    "    g = df.readlines()\n",
    "    for line in g:\n",
    "        if '## pve' in line:\n",
    "            pve = line.split(\"=\")[1].strip()\n",
    "        if '## se(pve)' in line:\n",
    "            se = line.split(\"=\")[1].strip()\n",
    "            log[pheno] = (pve,se)\n",
    "            break\n",
    "    print pheno,log[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line.split(\"=\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = 0.05/116231\n",
    "log10p = -1*math.log(p,10)\n",
    "lnp = -1*math.log(p)\n",
    "log10p,lnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pheno in lmm:\n",
    "    print pheno\n",
    "    filE = '/home/lindb/wbp/figures/%s_lmm_manhattan_log10.pdf' % pheno\n",
    "    with PdfPages(filE) as pdf:\n",
    "        fig = plt.figure()\n",
    "        plt.scatter(range(len(lmm[pheno].index)),[-1*math.log(x,10) for x in lmm[pheno]['p_wald'].tolist()])\n",
    "        plt.xlim(0,len(lmm[pheno].index))\n",
    "        plt.ylim(0,16)\n",
    "        plt.axhline(y=log10p,c=\"red\",linewidth=0.5,zorder=1,ls='dashed')\n",
    "        plt.ylabel(r'-$\\log_{10}(p)$',fontsize=14)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lmm[pheno]['p_wald'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pheno in lmm:\n",
    "    print pheno\n",
    "    filE = '/home/lindb/wbp/figures/%s_lmm_manhattan_ln.pdf' % pheno\n",
    "    with PdfPages(filE) as pdf:\n",
    "        fig = plt.figure()\n",
    "        plt.scatter(range(len(lmm[pheno].index)),[-1*math.log(x) for x in lmm[pheno]['p_wald'].tolist()])\n",
    "        plt.xlim(0,len(lmm[pheno].index))\n",
    "        plt.ylim(0,16)\n",
    "        plt.axhline(y=lnp,c=\"red\",linewidth=0.5,zorder=1,ls='dashed')\n",
    "        plt.ylabel(r'-$\\ln(p)$',fontsize=14)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the loci that kind of stand out (lnloci)\n",
    "lmmloci = {}\n",
    "outloci = {}\n",
    "lnloci = {}\n",
    "for pheno in lmm:\n",
    "    print pheno\n",
    "    lmmloci = {}\n",
    "    lmmloci[pheno] = {}\n",
    "    lmmloci[pheno]['log10'] = {}\n",
    "    lmmloci[pheno]['ln'] = {}\n",
    "    lnloci[pheno] = []\n",
    "    for i,snp in enumerate(lmm[pheno]['rs'].tolist()):\n",
    "        x = lmm[pheno].loc[i,'p_wald']\n",
    "        if -1*math.log(x) >=10:\n",
    "            lmmloci[pheno]['log10'][snp] = -1*math.log(x,10)\n",
    "            lmmloci[pheno]['ln'][snp] = -1*math.log(x)\n",
    "            lnloci[pheno].append(snp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in sorted(lnloci):\n",
    "    print pheno,len(lnloci[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outloci = []\n",
    "for pheno in sorted(lnloci):\n",
    "    for snp in lnloci[pheno]:\n",
    "        outloci.append(snp)\n",
    "len(outloci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pheno in lnloci:\n",
    "    filE  = '/home/lindb/wbp/gemma/lnloci_%s.txt' % pheno\n",
    "    df = pd.DataFrame(lnloci[pheno])\n",
    "    df.to_csv(filE,header=False,index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in toppips:\n",
    "    print pheno,len(toppips[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snplst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#intersection with top 0.999 PIPs\n",
    "for i,phenoi in enumerate(toppips):\n",
    "    for j,phenoj in enumerate(lnloci):\n",
    "        snps = set(toppips[phenoi]).intersection(set(lnloci[phenoj]))\n",
    "        if len(snps) > 0:\n",
    "            [snplst.append(snp) for snp in snps if snp not in snplst]\n",
    "            print phenoi,phenoj,len(snps),len(lnloci[phenoj]),'\\n',snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in topint:\n",
    "    print pheno, len(topint[pheno][0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#intersection with top 0.999 PIPs and 0.999 modfx\n",
    "for i,phenoi in enumerate(topint):\n",
    "    for j,phenoj in enumerate(lnloci):\n",
    "        snps = set(topint[phenoi][0.999]).intersection(set(lnloci[phenoj]))\n",
    "        if len(snps) > 0:\n",
    "            [snplst.append(snp) for snp in snps if snp not in snplst]\n",
    "            print phenoi,phenoj,len(snps),len(lnloci[phenoj]),'\\n',snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#intersection with top 0.998 PIPs\n",
    "for i,phenoi in enumerate(pip998):\n",
    "    for j,phenoj in enumerate(lnloci):\n",
    "        snps = set(pip998[phenoi]).intersection(set(lnloci[phenoj]))\n",
    "        if len(snps) > 0:\n",
    "            print phenoi,phenoj,len(snps),len(lnloci[phenoj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(snplst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in topint:\n",
    "    print pheno,len(topint[pheno][0.995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#intersection with top 0.995 PIPs and 0.995 modfx\n",
    "for i,phenoi in enumerate(topint):\n",
    "    for j,phenoj in enumerate(lnloci):\n",
    "        snps = set(topint[phenoi][0.995]).intersection(set(lnloci[phenoj]))\n",
    "        if len(snps) > 0:\n",
    "            [snplst.append(snp) for snp in snps if snp not in snplst]\n",
    "            print phenoi,phenoj,len(snps),len(lnloci[phenoj]),'\\n',snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(snplst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "math.e**(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " bfpopx 1\n",
    "c13popx 5\n",
    "htpopx 4\n",
    "n15popx 1\n",
    "rspopx 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(snplst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pheno in sorted(lmm):\n",
    "    print pheno\n",
    "    df = lmm[pheno][lmm[pheno]['p_wald'] <= math.e**(-10)]\n",
    "    for row in df.index:\n",
    "        print df.loc[row,'rs']\n",
    "        print df.loc[row,'beta']\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,phenoi in enumerate(lnloci):\n",
    "    for j,phenoj in enumerate(lnloci):\n",
    "        if i>j:\n",
    "            snps = set(lnloci[phenoi]).intersection(set(lnloci[phenoj]))\n",
    "            if len(snps)>0:\n",
    "                print phenoi,phenoj,len(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lmm[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst = ['A','B','C','D','E','F']\n",
    "with PdfPages('/home/lindb/wbp/figures/all_manhattan_ln.pdf') as pdf:\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "    \n",
    "    plotdict = {}\n",
    "    count = 1\n",
    "    for pheno in sorted(lmm):\n",
    "        plotdict[count] = plt.subplot(int(\"23%s\" % count))\n",
    "\n",
    "        plotdict[count].scatter(range(len(lmm[pheno].index)),[-1*math.log(x) for x in lmm[pheno]['p_wald'].tolist()])\n",
    "\n",
    "        plotdict[count].set_title(lst[count-1],y=.9,loc='right',fontsize=17,fontweight='bold')\n",
    "        \n",
    "        plotdict[count].set_xlim(0,len(lmm[pheno].index))\n",
    "        plotdict[count].set_ylim(0,16)\n",
    "        plotdict[count].axhline(y=lnp,c=\"red\",linewidth=0.5,zorder=1,ls='dashed')\n",
    "        plotdict[count].locator_params(axis='x',nbins=5)\n",
    "        for tick in plotdict[count].xaxis.get_major_ticks():\n",
    "                tick.label.set_fontsize(8)\n",
    "        if count in [1,4,7]:\n",
    "            plotdict[count].set_ylabel(r'-$\\ln(p)$',fontsize=14)\n",
    "        \n",
    "        if count == 6:\n",
    "            plotdict[count].spines['right'].set_visible(False)\n",
    "            plotdict[count].spines['left'].set_visible(False)\n",
    "            plotdict[count].spines['top'].set_visible(False)\n",
    "            plotdict[count].spines['bottom'].set_visible(False)\n",
    "            plotdict[count].axes.get_yaxis().set_visible(False)\n",
    "            plotdict[count].axes.get_xaxis().set_ticks([])\n",
    "            plotdict[count].axes.get_yaxis().set_ticks([])\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "    fig.set_size_inches(13,10)\n",
    "    pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blslmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outdir = '/home/lindb/wbp/gemma/infiles/bslmm/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bslmm = [op.join(outdir,f) for f in ls(outdir) if '50mil' in f]\n",
    "len(bslmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(bslmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(coda)\n",
    "library(data.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mcmc = r('plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makehash():\n",
    "    return collections.defaultdict(makehash)\n",
    "\n",
    "def split_bslmm_by_pheno(bslmm):\n",
    "    h = makehash()\n",
    "    for f in bslmm:\n",
    "        d = os.path.basename(f).split(\"_\")\n",
    "        pheno = d[0]\n",
    "        o = d[-1].split(\".\")\n",
    "        out = o[1]\n",
    "        num = d[2]\n",
    "        h[pheno][out][num] = f\n",
    "    return h\n",
    "bslmm_dict = split_bslmm_by_pheno(bslmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_files(key):\n",
    "    d = defaultdict(list)\n",
    "    for pheno, data in bslmm_dict.items():\n",
    "        for n in data[key]:\n",
    "            d[pheno].append(data[key][n])\n",
    "    return d\n",
    "    \n",
    "hyp_files = collect_files(\"hyp\")\n",
    "param_files = collect_files(\"param\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyp_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r('eff_size=list()')\n",
    "r('mcmc_summary=list()')\n",
    "r('mcmc_lists=list()')\n",
    "for pheno, files in hyp_files.items():\n",
    "    print(pheno)\n",
    "    r(\"m_list=list()\")\n",
    "    %R -i pheno\n",
    "    for i, hyp_file in enumerate(files):\n",
    "        lines = open(hyp_file).readlines()\n",
    "        if len(lines) < 10:\n",
    "            continue\n",
    "        data = []\n",
    "        for l in lines:\n",
    "            l = l.strip().split(\"\\t\")\n",
    "            data.append(l)\n",
    "        hyp = pd.DataFrame(data[1:], columns=data[0], dtype=float)\n",
    "        hyp.columns = [x.strip() for x in hyp.columns]\n",
    "        hyp.to_csv(hyp_file, sep=\"\\t\", header=True, index=False)\n",
    "        %R -i hyp_file\n",
    "        r(\"m = mcmc(fread('%s', sep='\\t', , header=T, data.table=F), thin=1000)\" % hyp_file)\n",
    "        r(\"m_list$%s = m\" % os.path.basename(hyp_file))\n",
    "    r(\"mcmc_list = mcmc.list(m_list)\")\n",
    "    r(\"mcmc_lists$%s = mcmc_list\" % pheno) \n",
    "    r(\"eff_size$%s = effectiveSize(mcmc_list)\" % pheno)\n",
    "    r(\"mcmc_summary$%s = summary(mcmc_list)\" % pheno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R -i outdir\n",
    "r(\"saveRDS(mcmc_lists, file='%s')\" % os.path.join(outdir, \"mcmc_lists_50mil.rds\"));\n",
    "r(\"saveRDS(mcmc_summary, file='%s')\" % os.path.join(outdir, \"mcmc_summary_50mil.rds\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print '\\t effective sizes'\n",
    "print(r('eff_size'))\n",
    "print '\\t mcmc summaries'\n",
    "print(r('mcmc_summary'))\n",
    "print '\\t stuff'\n",
    "for pheno in hyp_files:\n",
    "    print(pheno, r('gelman.diag(mcmc_lists$%s, autoburnin=F)' % pheno))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "print(\"bfpopx\")\n",
    "mcmc_lists = readRDS(paste(outdir, '/mcmc_lists_50mil.rds', sep=''))\n",
    "plot(mcmc_lists$bfpopx)\n",
    "print(\"c13popx\")\n",
    "plot(mcmc_lists$c13popx)\n",
    "print(\"n15popx\")\n",
    "plot(mcmc_lists$n15popx)\n",
    "print(\"htpopx\")\n",
    "plot(mcmc_lists$htpopx)\n",
    "print(\"rspopx\")\n",
    "plot(mcmc_lists$rspopx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_dfs['bfpopx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_dfs['bfpopx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyp_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#delete a useless key\n",
    "newhyp_files = {}\n",
    "for pheno in hyp_files.keys():\n",
    "    if (pheno !=0) and (pheno!='0'):\n",
    "        flz = hyp_files[pheno]\n",
    "        newhyp_files[pheno] = flz\n",
    "newhyp_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_hyp_dfs(files):\n",
    "    dfs = {}\n",
    "    for pheno, filelist in files.items():\n",
    "        dfs[pheno] = pd.DataFrame()\n",
    "        df = None\n",
    "        fcount = 0\n",
    "        for f in sorted(filelist):\n",
    "            fdata = os.path.basename(f).split(\".\")[0].split(\"_\")\n",
    "            num = int(fdata[2])\n",
    "            \n",
    "            if fcount == 0:\n",
    "                df = pd.read_csv(f, sep=\"\\t\")\n",
    "                df.columns = ['%s_%s' % (col,num) for col in df.columns]\n",
    "            else:\n",
    "                newdf = pd.read_csv(f, sep=\"\\t\")\n",
    "                newdf.columns = [\"%s_%s\" % (col,num) for col in newdf.columns]\n",
    "                df = pd.merge(df,newdf,left_index=True,right_index=True)\n",
    "            fcount += 1\n",
    "        df = df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "        dfs[pheno] = df\n",
    "    return dfs\n",
    "hyp_dfs = get_hyp_dfs(newhyp_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyp_dfs['bfpopx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hmean_row(row):\n",
    "    try:\n",
    "        return sp.stats.hmean(row)\n",
    "    except ValueError as e:\n",
    "        return np.nan   \n",
    "\n",
    "def get_hmean(param):\n",
    "    d = {}\n",
    "    for pheno in param_dfs:\n",
    "        df = param_dfs[pheno]\n",
    "        g = pd.DataFrame(df[[col for col in df.columns if param in col]])\n",
    "        #m = g.apply(get_hmean_row, axis=1)\n",
    "        m = g.apply(np.mean, axis=1)\n",
    "        g['%s_hmean' % param] = m\n",
    "        d[pheno] = g\n",
    "    return d\n",
    "\n",
    "gamma_dfs = get_hmean('gamma')\n",
    "beta_dfs = get_hmean('beta')\n",
    "alpha_dfs = get_hmean('alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = 'gamma'\n",
    "for pheno in param_dfs:\n",
    "    df = param_dfs[pheno]\n",
    "    g = pd.DataFrame(df[[col for col in df.columns if 'gamma' in col]])\n",
    "    m = g.apply(np.mean,axis=1)\n",
    "    g['%s_hmean' % 'gamma']  = m\n",
    "    d[pheno] = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in gamma_dfs:\n",
    "    print pheno,gamma_dfs[pheno]['gamma_hmean'].describe()['max']\n",
    "    maxx = gamma_dfs[pheno]['gamma_hmean'].describe()['max']\n",
    "    minn = gamma_dfs[pheno]['gamma_hmean'].describe()['min']\n",
    "    print maxx/minn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.arange(0,maxx,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d[x].quantile(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pheno in gamma_dfs:\n",
    "    print pheno\n",
    "    print gamma_dfs[pheno]['gamma_hmean'].describe()\n",
    "    maxx = max(gamma_dfs[pheno]['gamma_hmean'])\n",
    "    minn = min(gamma_dfs[pheno]['gamma_hmean'])\n",
    "    mean = round(gamma_dfs[pheno]['gamma_hmean'].describe()['mean'],5)\n",
    "    std = gamma_dfs[pheno]['gamma_hmean'].describe()['std']\n",
    "    quant = gamma_dfs[pheno]['gamma_hmean'].quantile(0.999)\n",
    "    fth = gamma_dfs[pheno]['gamma_hmean'].quantile(0.5)\n",
    "    plt.hist(gamma_dfs[pheno]['gamma_hmean'], bins = np.arange(0,maxx,0.0001))\n",
    "    plt.axvline(x=quant,c=\"red\",linewidth=0.5,zorder=0,ls='dashed')\n",
    "    plt.suptitle(str(pheno)+':'+' min='+str(minn)+' max='+str(maxx)+' std='+str(round(std,6))+' mean='+str(mean)+' med='+str(fth),fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma_dfs['bfpopx']['gamma_hmean'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get dataframes with absolute values of effect \n",
    "combined_dfs = {}\n",
    "for pheno in gamma_dfs:\n",
    "    a = alpha_dfs[pheno]['alpha_hmean']\n",
    "    b = beta_dfs[pheno]['beta_hmean']\n",
    "    g = gamma_dfs[pheno]['gamma_hmean']\n",
    "    t = pd.concat((a, b, g), axis=1)\n",
    "    plt.scatter(t['alpha_hmean'], t['beta_hmean'])\n",
    "    plt.xlim(np.min(a), np.max(a))\n",
    "    plt.ylim(np.min(b), np.max(b))\n",
    "    plt.ylabel(\"beta mean\")\n",
    "    plt.xlabel(\"alpha mean\")\n",
    "    plt.title(pheno)\n",
    "    plt.show()\n",
    "    t = np.abs(t)\n",
    "    t['total_effect'] = t.apply(lambda x: x.alpha_hmean + x.beta_hmean, axis=1)\n",
    "    t['model_averaged'] = t.apply(lambda x: x.alpha_hmean + (x.beta_hmean*x.gamma_hmean),axis=1) #sensu gompert et al. 2015\n",
    "    combined_dfs[pheno] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get data frames with original sign from GEMMA (pos or neg effect)\n",
    "combined_fx_dfs = {}\n",
    "for pheno in gamma_dfs:\n",
    "    a = alpha_dfs[pheno]['alpha_hmean']\n",
    "    b = beta_dfs[pheno]['beta_hmean']\n",
    "    g = gamma_dfs[pheno]['gamma_hmean']\n",
    "    t = pd.concat((a, b, g), axis=1)\n",
    "    t['total_effect'] = t.apply(lambda x: x.alpha_hmean + x.beta_hmean, axis=1)\n",
    "    t['model_averaged'] = t.apply(lambda x: x.alpha_hmean + (x.beta_hmean*x.gamma_hmean), axis=1) #sensu gompert et al. 2015\n",
    "    combined_fx_dfs[pheno] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_fx_dfs[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_dfs[pheno].loc['NODE_1000031_length_98_cov_2.000000_30',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pheno in combined_dfs:\n",
    "    print(pheno)\n",
    "    display(combined_dfs[pheno].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3, venn3_unweighted, venn3_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write the combined_dfs to file\n",
    "for pheno in combined_dfs:\n",
    "    filE = '/home/lindb/wbp/gemma/infiles/bslmm/output/%s_combined_df.txt' % pheno\n",
    "    combined_dfs[pheno].to_csv(filE,header=True,index=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#when I restart my notebook\n",
    "DIR = '/home/lindb/wbp/gemma/infiles/bslmm/output/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'x_combined_df' in f]\n",
    "combined_dfs = {}\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    combined_dfs[pheno] = pd.read_csv(f,header=0,index_col='rs',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_dfs[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "effect_snps = {}\n",
    "for pheno in combined_dfs:\n",
    "    d = combined_dfs[pheno]\n",
    "    x = 'gamma_hmean'\n",
    "    for q in [0.995, 0.999]:\n",
    "        x99_cutoff = d[x].quantile(q)\n",
    "        xvals = d[x][d[x] >= x99_cutoff] \n",
    "        for y in ['alpha_hmean', 'beta_hmean', 'total_effect']:\n",
    "            y99_cutoff = d[y].quantile(q)\n",
    "            yvals = d[y][d[y] >= y99_cutoff]\n",
    "            isect = set(xvals.index).intersection(set(yvals.index))\n",
    "            effect_snps[pheno, x, y, q] = isect\n",
    "            print(pheno, x, y, q, len(isect))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outdir, \"effect_snps_50mil.pkl\"), \"wb\") as o:\n",
    "    pickle.dump(effect_snps, o, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in sorted(effect_snps):\n",
    "    print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in effect_snps:\n",
    "    k = [str(x) for x in key]\n",
    "    out = os.path.join(outdir, \"%s_effect_50mil.txt\" % \"-\".join(k))\n",
    "    with open(out, \"w\") as o:\n",
    "        o.write(\"\\n\".join(effect_snps[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outdir, \"combined_dfs_50mil.pkl\"), \"wb\") as o:\n",
    "    pickle.dump(combined_dfs, o, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "venn_data = {}\n",
    "for pheno in combined_dfs:\n",
    "    d = combined_dfs[pheno]\n",
    "    x = 'gamma_hmean'\n",
    "    venn_data[pheno] = {}\n",
    "    for q in [0.995, 0.999]:\n",
    "        venn_data[pheno][q] = []\n",
    "        x99_cutoff = d[x].quantile(q)\n",
    "        xvals = d[x][d[x] >= x99_cutoff] \n",
    "        venn_data[pheno][q].append(set(xvals.index))\n",
    "        for y in ['alpha_hmean', 'beta_hmean']:\n",
    "            y99_cutoff = d[y].quantile(q)\n",
    "            yvals = d[y][d[y] >= y99_cutoff]\n",
    "            venn_data[pheno][q].append(set(yvals.index))\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "for pheno, d in venn_data.items():\n",
    "    for q in d:\n",
    "        venn3(d[q], (\"gamma\", \"alpha\", \"beta\"))\n",
    "        plt.title(\"%s_%.3f\" % (pheno, q))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pheno in combined_dfs:\n",
    "    d = combined_dfs[pheno]\n",
    "    x = 'gamma_hmean'\n",
    "    y = 'total_effect'\n",
    "    sns.lmplot(x, y, d)\n",
    "    plt.xlim(np.min(d[x]), np.max(d[x]))\n",
    "    plt.ylim(np.min(d[y]), np.max(d[y]))\n",
    "    plt.title(pheno)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/lindb/wbp/hierfstat/imputed/imputed_hierarchical_Fstats.txt',header=0,index_col=0,sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loci_fst = pd.DataFrame(df['pop_total'])\n",
    "loci_fst.columns = ['Fst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pheno in combined_dfs:\n",
    "    d = combined_dfs[pheno]\n",
    "    j = d.join(loci_fst, how=\"inner\")\n",
    "    x = 'Fst'\n",
    "    y = 'total_effect'\n",
    "    sns.lmplot(x, y, j)\n",
    "    plt.xlim(np.min(j[x]), np.max(j[x]))\n",
    "    plt.ylim(np.min(j[y]), np.max(j[y]))\n",
    "    plt.title(pheno)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/infiles/bslmm/output'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if '0.999_effect_50mil' in f and 'swp' not in f]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdict = {}\n",
    "for f in files:\n",
    "    pheno = op.basename(f).split(\"-\")[0]\n",
    "    if pheno not in fdict.keys():\n",
    "        fdict[pheno] = []\n",
    "    fdict[pheno].append(f)\n",
    "fdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sets = {}\n",
    "for pheno in fdict.keys():\n",
    "    fcount = 0\n",
    "    for f in fdict[pheno]:\n",
    "        if fcount == 0:\n",
    "            df = pd.read_csv(f,header=None,sep='\\t')\n",
    "            snps = df[0].tolist()\n",
    "            sets[pheno] = snps\n",
    "        else:\n",
    "            df = pd.read_csv(f,header=None,sep='\\t')\n",
    "            snps = df[0].tolist()\n",
    "            sets[pheno] = set(sets[pheno]).intersection(set(snps))\n",
    "        fcount += 1\n",
    "for pheno in sorted(sets.keys()):\n",
    "    print pheno,len(sets[pheno])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariances intersection of 99.9th percentile of PIPs and total effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/sampsTOpop.txt'\n",
    "stpdf = pd.read_csv(filE,header=0,sep='\\t')\n",
    "stpdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stpDict = OrderedDict()\n",
    "for row in stpdf.index:\n",
    "    pop = stpdf.loc[row,'pop']\n",
    "    if not pop in ['Heavenly','West_Shore_Peaks']:\n",
    "        if not pop in stpDict.keys():\n",
    "            stpDict[pop] =[]\n",
    "        stpDict[pop].append(stpdf.loc[row,'sampID'])\n",
    "len(stpDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get 012 file\n",
    "filE = '/home/lindb/wbp/gemma/imputed_z12_maf_swp_trans_z12_6pop_maf_z12_nofixed.txt'\n",
    "nnz12 = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "nnz12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(nnz12.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get a dictionary of dataframes for each pop\n",
    "popDict = OrderedDict()\n",
    "for pop in stpDict.keys():\n",
    "    df = nnz12[nnz12.index.isin(stpDict[pop])]\n",
    "    popDict[pop] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get maf for each locus for each pop\n",
    "mafDict = OrderedDict()\n",
    "lcount = 0\n",
    "for locus in nnz12.columns:\n",
    "    mafDict[locus] = OrderedDict()\n",
    "    for pop in stpDict.keys():\n",
    "        df = pd.DataFrame(popDict[pop])\n",
    "        \n",
    "        zero = df[locus].tolist().count(0)\n",
    "        one = df[locus].tolist().count(1)\n",
    "        two = df[locus].tolist().count(2)\n",
    "        \n",
    "        p = ((2*zero)+one)/(2*len(df.index))\n",
    "        q = ((2*two)+one)/(2*len(df.index))\n",
    "        \n",
    "        assert p > q\n",
    "        \n",
    "        maf = q #want pop freq to be representative of the globally minor allele\n",
    "        mafDict[locus][pop] = maf\n",
    "    lcount += 1\n",
    "    if lcount % 10000 == 0:\n",
    "        print lcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mafDict.keys()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/6pop_pop_maf.txt'\n",
    "with open(filE,'w') as o:\n",
    "    key0 = mafDict.keys()[0]\n",
    "    line = '\\t'.join(mafDict[key0].keys())+'\\n'\n",
    "    o.write(\"%s\" % line)\n",
    "    for locus in mafDict.keys():\n",
    "        line = str(locus)+'\\t'+'\\t'.join([str(x) for x in mafDict[locus].values()]) + '\\n'\n",
    "        o.write(\"%s\" % line)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/6pop_pop_maf.txt'\n",
    "mafDF = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "mafDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mafDict['NODE_1000013_length_91_cov_1.802198_37']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in mafDF.columns:\n",
    "    print col, max(mafDF[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get global maf for each locus\n",
    "glob = OrderedDict()\n",
    "lcount = 0\n",
    "for locus in nnz12.columns:\n",
    "    zero = nnz12[locus].tolist().count(0)\n",
    "    one = nnz12[locus].tolist().count(1)\n",
    "    two = nnz12[locus].tolist().count(2)\n",
    "    \n",
    "    p = ((2*zero)+one)/(2*len(nnz12.index))\n",
    "    q = ((2*two)+one)/(2*len(nnz12.index))\n",
    "    \n",
    "    assert p >= q\n",
    "    \n",
    "    glob[locus] = min(p,q)\n",
    "    lcount += 1\n",
    "    if lcount % 10000 == 0:\n",
    "        print lcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "with open(filE,'w') as o:\n",
    "    line = '\\t'.join(['locus','maf'])+'\\n'\n",
    "    o.write(\"%s\" % line)\n",
    "    for locus in glob.keys():\n",
    "        line = '\\t'.join([locus,str(glob[locus])])+'\\n'\n",
    "        o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glob['NODE_1000042_length_93_cov_1.118280_100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "df = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "glob = OrderedDict()\n",
    "for locus in df.index:\n",
    "    glob[locus] = df.loc[locus,'maf']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#population sizes (only for individuals in pops with phenotypic measurements)\n",
    "popsizeDict = {'Dicks_Pass':15,\n",
    "'Freel_Peak':19,\n",
    "'Little_Round_Top':14,\n",
    "'Mt_Rose_Ophir':11,\n",
    "'Rifle_Peak':15,\n",
    "'Snow_Valley_Peak':14,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get files with significant SNPs\n",
    "DIR = '/home/lindb/wbp/gemma/infiles/bslmm/output/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'total_effect-0.999_effect_50mil.txt' in f]\n",
    "len(files)\n",
    "snpdict999 = {}\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"-\")[0]\n",
    "    df = pd.read_csv(f,header=None,sep='\\t')\n",
    "    snpdict999[pheno] = df[0].tolist()\n",
    "    print pheno,len(snpdict999[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get files with significant SNPs\n",
    "DIR = '/home/lindb/wbp/gemma/infiles/bslmm/output/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'total_effect-0.995_effect_50mil.txt' in f]\n",
    "snpdict995 = {}\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"-\")[0]\n",
    "    df = pd.read_csv(f,header=None,sep='\\t')\n",
    "    snpdict995[pheno] = df[0].tolist()\n",
    "    print pheno,len(snpdict995[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ipyparallel import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rc = Client(profile=\"xmn\")\n",
    "dview = rc[:]\n",
    "lview = rc.load_balanced_view()\n",
    "len(lview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sums = 0\n",
    "for pop in popsizeDict.keys():\n",
    "    sums = sums + popsizeDict[pop]\n",
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#change total pop size if necessary\n",
    "#do pairwise to get Dij\n",
    "def get_dijdict(tokens):\n",
    "    from collections import OrderedDict\n",
    "    from os import path as op\n",
    "    from os import listdir as ls\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    pheno,outliersnps,glob,mafDict,popDict = tokens\n",
    "    dijDict = OrderedDict() \n",
    "    dijDict[pheno] = OrderedDict() \n",
    "    for i,locusi in enumerate(outliersnps):\n",
    "        dijDict[pheno][locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(outliersnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popDict.keys():\n",
    "                    qik = mafDict[locusi][pop] #get pop maf\n",
    "                    qjk = mafDict[locusj][pop] #get pop maf\n",
    "                    nk = popDict[pop]\n",
    "\n",
    "                    sums += (float(nk)/float(88))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[pheno][locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[pheno][locusi][locusj] = np.nan\n",
    "    #write out the file\n",
    "    for pheno in dijDict.keys():\n",
    "        filE = '/home/lindb/wbp/gemma/covariances_total_effect_999/dvals/%s_dvals.txt' % pheno\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            key0 = dijDict[pheno].keys()[0]\n",
    "            line = '\\t'.join(dijDict[pheno][key0].keys()) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "            for locusi in dijDict[pheno].keys():\n",
    "                line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[pheno][locusi].values()]) + str('\\n')\n",
    "                o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jobs = []\n",
    "for pheno in snpdict995.keys():\n",
    "    print pheno\n",
    "    jobs.append(lview.apply_async(get_dijdict,[pheno,snpdict999[pheno],glob,mafDict,popsizeDict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for j in jobs:\n",
    "    if j.ready():\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in jobs:\n",
    "    if j.ready():\n",
    "        print j.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariances_total_effect_999/dvals'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "dijDict = OrderedDict()\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    dijDict[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "dijDict[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(dijDict[pheno].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get expected heterozygosity\n",
    "Hexp = OrderedDict()\n",
    "for locus in glob.keys():\n",
    "    Hexp[locus] = 2*glob[locus]*(1-glob[locus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/covariances_total_effect_999/6pops_Hexp_by_snp.txt'\n",
    "with open(filE,'w') as o:\n",
    "    text = 'locus\\tH_exp\\n'\n",
    "    o.write(\"%s\" % text)\n",
    "    for snp in Hexp.keys():\n",
    "        text = '\\t'.join([snp,str(Hexp[snp])])+'\\n'\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/covariances_total_effect_999/6pops_Hexp_by_snp.txt'\n",
    "H = pd.read_csv(filE,header=0,sep='\\t')\n",
    "H.index = [snp for snp in H['locus'].tolist()]\n",
    "H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#assign bins to samps\n",
    "#1st bin is the j=0th bin, 50th bin is the j=49th bin\n",
    "count = 0\n",
    "binDict = OrderedDict()\n",
    "for row in H.index:\n",
    "    h = np.round(H.loc[row,'H_exp'],decimals=3)\n",
    "    binDict[row] = 0 #because 2pq will never be equal to 0 for a SNP, but may be less than 0.01\n",
    "    for Bin,j in enumerate(np.arange(0,0.51,0.01)): #1st bin is the j=0th bin, 50th bin is the j=49th bin\n",
    "        if h>j: #binDict[row] will constantly replace the value, which is good. don't want 2pq=0.5 having its own group\n",
    "            binDict[row] = Bin\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print count\n",
    "H['bin'] = binDict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(H['H_exp'],bins = [x for x in np.arange(0,.51,0.01)])[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dijDict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataframe with the outlier loci and their bins\n",
    "outlierdata = OrderedDict()\n",
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(snpdict999[pheno])\n",
    "    outliersnps = snpdict999[pheno]\n",
    "\n",
    "    outlierdata[pheno] = pd.DataFrame(H[H.index.isin(outliersnps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/grid_imp_hexp_hist_by_pheno_999.pdf') as pdf:\n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "\n",
    "    a1 = plt.subplot(231)\n",
    "    a1.hist(outlierdata['bfpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a1.set_title('A',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a1.spines['right'].set_visible(False)\n",
    "    a1.spines['top'].set_visible(False)\n",
    "    a1.yaxis.set_ticks_position('left')\n",
    "    a1.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a2 = plt.subplot(232)\n",
    "    a2.hist(outlierdata['c13popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a2.set_title('B',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a2.spines['right'].set_visible(False)\n",
    "    a2.spines['top'].set_visible(False)\n",
    "    a2.yaxis.set_ticks_position('left')\n",
    "    a2.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a3 = plt.subplot(233)\n",
    "    a3.hist(outlierdata['htpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a3.set_title('C',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a3.spines['right'].set_visible(False)\n",
    "    a3.spines['top'].set_visible(False)\n",
    "    a3.yaxis.set_ticks_position('left')\n",
    "    a3.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a4 = plt.subplot(234)\n",
    "    a4.hist(outlierdata['n15popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a4.set_title('D',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a4.spines['right'].set_visible(False)\n",
    "    a4.spines['top'].set_visible(False)\n",
    "    a4.yaxis.set_ticks_position('left')\n",
    "    a4.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a5 = plt.subplot(235)\n",
    "    a5.hist(outlierdata['rspopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a5.set_title('E',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a5.spines['right'].set_visible(False)\n",
    "    a5.spines['top'].set_visible(False)\n",
    "    a5.yaxis.set_ticks_position('left')\n",
    "    a5.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a6.spines['right'].set_visible(False)\n",
    "    a6.spines['left'].set_visible(False)\n",
    "    a6.spines['top'].set_visible(False)\n",
    "    a6.spines['bottom'].set_visible(False)\n",
    "    a6.axes.get_yaxis().set_visible(False)\n",
    "    a6.axes.get_xaxis().set_ticks([])\n",
    "    a6.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    fig.set_size_inches(13,10)\n",
    "    pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get sets of SNPs to make random draws\n",
    "nonsigsnps = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    nonsigs = list(set(H.index.tolist()) - set(outlierdata[pheno].index.tolist()))\n",
    "    nonsigsnps[pheno] = pd.DataFrame(H[H.index.isin(nonsigs)])\n",
    "    print len(outlierdata[pheno].index.tolist())+len(nonsigsnps[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many random snps to draw from each bin?\n",
    "binCounter = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    binCounter[pheno] = Counter()\n",
    "    for row in outlierdata[pheno].index:\n",
    "        binCounter[pheno][outlierdata[pheno].loc[row,'bin']] += 1\n",
    "    for b in binCounter[pheno].keys():\n",
    "        print pheno,b,binCounter[pheno][b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices')\n",
    "os.makedirs('/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randsnps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each pheno make 1000 dataframes with a set of snps == len(outliersnps[pheno])\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    for i in range(1000):  \n",
    "        snps = []        \n",
    "        for binn in binCounter[pheno].keys():\n",
    "            data = nonsigsnps[pheno][nonsigsnps[pheno]['bin'] == binn]\n",
    "                \n",
    "            [snps.append(snp) for snp in random.sample(data.index,binCounter[pheno][binn])]\n",
    "        \n",
    "        assert len(np.unique(snps).tolist()) == sum(binCounter[pheno].values())\n",
    "\n",
    "        DIR = '/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randsnps/%s' % pheno\n",
    "        if not op.exists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = op.join(DIR,\"%s_%s_randsnps.txt\" % (pheno,str(i).zfill(3)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quicker to do it this way\n",
    "#get dvals for the sets of random snps for each pheno\n",
    "for pheno in outlierdata.keys():\n",
    "    for i in range(1000):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "#population sizes (only for individuals in pops with phenotypic measurements)\n",
    "popDict = {'Dicks_Pass':15,\n",
    "'Freel_Peak':19,\n",
    "'Little_Round_Top':14,\n",
    "'Mt_Rose_Ophir':11,\n",
    "'Rifle_Peak':15,\n",
    "'Snow_Valley_Peak':14,}\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/gemma/6pop_pop_maf.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randsnps/%s/%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/88)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randDVALS/%s/%s_%s_randDVALS.txt'\n",
    "if not op.exists('/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randDVALS/'):\n",
    "    os.makedirs('/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randDVALS/')\n",
    "DIR = op.dirname(filE)\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "\n",
    "\n",
    "''' % (pheno,pheno,str(i).zfill(3),\n",
    "       pheno,pheno,str(i).zfill(3))\n",
    "        filE = '/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randpyfiles/get_rand_dvals_%s_%s.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(3)\n",
    "                                                                                                     )\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randpyfiles/'\n",
    "pyfiles = [op.join(DIR,f) for f in ls(DIR)]\n",
    "len(pyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "5000/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#spread out the jobs across 25 files\n",
    "#18000/25=720\n",
    "count = 0\n",
    "fcount = 0\n",
    "for f in sorted(pyfiles):\n",
    "    if count == 0:\n",
    "        text = 'python %s\\n' % f\n",
    "    else:\n",
    "        newtext = 'python %s\\n' % f\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    if count == 40:\n",
    "        count = 0\n",
    "        fcount += 1\n",
    "        filE = '/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/runpyfiles/%s_cmds.txt' % str(fcount).zfill(3)\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make 125 qsub files\n",
    "for i in range(125):\n",
    "    j = i+1\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N pyfile%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cat %s_cmds.txt | parallel --progress --\n",
    "\n",
    "''' % (str(j).zfill(3),\n",
    "       str(j).zfill(3)\n",
    "      )\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/runpyfiles/%s_run.sh' % str(j).zfill(3)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#qsub the qsubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randDVALS/'\n",
    "ds = [op.join(DIR,d) for d in ls(DIR)]\n",
    "count = 0\n",
    "for DIR in ds:\n",
    "    count = count + len(ls(DIR))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "dvals = OrderedDict()\n",
    "medvals = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariances_total_effect_999/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "\n",
    "    DIR = '/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randDVALS/%s/' % pheno\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "\n",
    "    fcount = 0\n",
    "    medvals[pheno] = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        rvals = [] #random dij values\n",
    "        for lst in df.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    rvals.append(x)\n",
    "\n",
    "\n",
    "        medvals[pheno].append(np.median([abs(val) for val in rvals]))\n",
    "\n",
    "        fcount += 1\n",
    "        if fcount % 100 == 0:\n",
    "            print fcount\n",
    "\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    if not op.exists(op.dirname(filE)):\n",
    "        os.makedirs(op.dirname(filE))\n",
    "    medvaldf = pd.DataFrame(medvals[pheno])\n",
    "    medvaldf.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#in case I restart my notebook\n",
    "medvals = {}\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariances_total_effect_999/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    df = pd.read_csv(filE,header=None,sep='\\t')\n",
    "    medvals[pheno] = df[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno,'has',len(snpdict999[pheno]),'SNPs in intersection of top PIPs and top effects (alpha+beta)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I moved the /covariances/ directory to /covariances_total_effect_999\n",
    "#I went back through to confirm analyses, replaced (in script)\n",
    "    #covariance with covariances_total_effect_999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariance for intersection of top 0.995 PIPs and tot effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get files with significant SNPs\n",
    "DIR = '/home/lindb/wbp/gemma/infiles/bslmm/output/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'total_effect-0.995_effect_50mil.txt' in f]\n",
    "snpdict995 = {}\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"-\")[0]\n",
    "    df = pd.read_csv(f,header=None,sep='\\t')\n",
    "    snpdict995[pheno] = df[0].tolist()\n",
    "    print pheno,len(snpdict995[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popsizeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do pairwise to get Dij\n",
    "for pheno in snpdict995.keys():\n",
    "    outliersnps = snpdict995[pheno]\n",
    "    popDict = OrderedDict(popsizeDict)\n",
    "\n",
    "    dijDict = OrderedDict() \n",
    "    dijDict[pheno] = OrderedDict() \n",
    "    for i,locusi in enumerate(outliersnps):\n",
    "        dijDict[pheno][locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(outliersnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popDict.keys():\n",
    "                    qik = mafDict[locusi][pop] #get pop maf\n",
    "                    qjk = mafDict[locusj][pop] #get pop maf\n",
    "                    nk = popDict[pop]\n",
    "\n",
    "                    sums += (float(nk)/float(88))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[pheno][locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[pheno][locusi][locusj] = np.nan\n",
    "    #write out the file\n",
    "    for pheno in dijDict.keys(): #indented by accident, would just overwrite and take longer than necessary\n",
    "        filE = '/home/lindb/wbp/gemma/covariances_total_effect_995/dvals/%s_dvals.txt' % pheno\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            key0 = dijDict[pheno].keys()[0]\n",
    "            line = '\\t'.join(dijDict[pheno][key0].keys()) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "            for locusi in dijDict[pheno].keys():\n",
    "                line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[pheno][locusi].values()]) + str('\\n')\n",
    "                o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariances_total_effect_995/dvals'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "dijDict = OrderedDict()\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    dijDict[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "dijDict[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(dijDict[pheno].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(Hexp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataframe with the outlier loci and their bins\n",
    "outlierdata = OrderedDict()\n",
    "for pheno in dijDict.keys():\n",
    "    outliersnps = snpdict995[pheno]\n",
    "\n",
    "    outlierdata[pheno] = pd.DataFrame(H[H.index.isin(outliersnps)])\n",
    "    print pheno,len(outlierdata[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(H['bin'],bins =np.arange(0,51,1))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/grid_imp_hexp_hist_by_pheno_top_PIPs_top_effects_995.pdf') as pdf:\n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "\n",
    "    a1 = plt.subplot(231)\n",
    "    a1.hist(outlierdata['bfpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a1.set_title('A',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a1.spines['right'].set_visible(False)\n",
    "    a1.spines['top'].set_visible(False)\n",
    "    a1.yaxis.set_ticks_position('left')\n",
    "    a1.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a2 = plt.subplot(232)\n",
    "    a2.hist(outlierdata['c13popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a2.set_title('B',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a2.spines['right'].set_visible(False)\n",
    "    a2.spines['top'].set_visible(False)\n",
    "    a2.yaxis.set_ticks_position('left')\n",
    "    a2.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a3 = plt.subplot(233)\n",
    "    a3.hist(outlierdata['htpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a3.set_title('C',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a3.spines['right'].set_visible(False)\n",
    "    a3.spines['top'].set_visible(False)\n",
    "    a3.yaxis.set_ticks_position('left')\n",
    "    a3.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a4 = plt.subplot(234)\n",
    "    a4.hist(outlierdata['n15popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a4.set_title('D',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a4.spines['right'].set_visible(False)\n",
    "    a4.spines['top'].set_visible(False)\n",
    "    a4.yaxis.set_ticks_position('left')\n",
    "    a4.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a5 = plt.subplot(235)\n",
    "    a5.hist(outlierdata['rspopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a5.set_title('E',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a5.spines['right'].set_visible(False)\n",
    "    a5.spines['top'].set_visible(False)\n",
    "    a5.yaxis.set_ticks_position('left')\n",
    "    a5.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a6.spines['right'].set_visible(False)\n",
    "    a6.spines['left'].set_visible(False)\n",
    "    a6.spines['top'].set_visible(False)\n",
    "    a6.spines['bottom'].set_visible(False)\n",
    "    a6.axes.get_yaxis().set_visible(False)\n",
    "    a6.axes.get_xaxis().set_ticks([])\n",
    "    a6.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    fig.set_size_inches(13,10)\n",
    "    pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get sets of SNPs to make random draws\n",
    "nonsigsnps = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    nonsigs = list(set(H.index.tolist()) - set(outlierdata[pheno].index.tolist()))\n",
    "    nonsigsnps[pheno] = pd.DataFrame(H[H.index.isin(nonsigs)])\n",
    "    print len(outlierdata[pheno].index.tolist())+len(nonsigsnps[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many random snps to draw from each bin?\n",
    "binCounter = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    binCounter[pheno] = Counter()\n",
    "    for row in outlierdata[pheno].index:\n",
    "        binCounter[pheno][outlierdata[pheno].loc[row,'bin']] += 1\n",
    "    for b in binCounter[pheno].keys():\n",
    "        print pheno,b,binCounter[pheno][b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices')\n",
    "os.makedirs('/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/randsnps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in outlierdata.keys():\n",
    "    print pheno,len(outlierdata[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each pheno make 1000 dataframes with a set of snps == len(outliersnps[pheno])\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    for i in range(1000):  \n",
    "        snps = []        \n",
    "        for binn in binCounter[pheno].keys():\n",
    "            data = nonsigsnps[pheno][nonsigsnps[pheno]['bin'] == binn]\n",
    "                \n",
    "            [snps.append(snp) for snp in random.sample(data.index,binCounter[pheno][binn])]\n",
    "        \n",
    "        assert len(np.unique(snps).tolist()) == sum(binCounter[pheno].values())\n",
    "\n",
    "        DIR = '/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/randsnps/%s' % pheno\n",
    "        if not op.exists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = op.join(DIR,\"%s_%s_randsnps.txt\" % (pheno,str(i).zfill(3)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quicker to do it this way\n",
    "#get dvals for the sets of random snps for each pheno\n",
    "for pheno in outlierdata.keys():\n",
    "    for i in range(1000):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "#population sizes (only for individuals in pops with phenotypic measurements)\n",
    "popDict = {'Dicks_Pass':15,\n",
    "'Freel_Peak':19,\n",
    "'Little_Round_Top':14,\n",
    "'Mt_Rose_Ophir':11,\n",
    "'Rifle_Peak':15,\n",
    "'Snow_Valley_Peak':14,}\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/gemma/6pop_pop_maf.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/randsnps/%s/%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/88)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/randDVALS/%s/%s_%s_randDVALS.txt'\n",
    "if not op.exists('/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/randDVALS/'):\n",
    "    os.makedirs('/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/randDVALS/')\n",
    "DIR = op.dirname(filE)\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "        \n",
    "''' % (pheno,pheno,str(i).zfill(3),\n",
    "       pheno,pheno,str(i).zfill(3))\n",
    "        filE = '/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/randpyfiles/get_rand_dvals_%s_%s.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(3)\n",
    "                                                                                                     )\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the python scripts\n",
    "DIR = '/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/randpyfiles/'\n",
    "pyfiles = [op.join(DIR,f) for f in ls(DIR)]\n",
    "len(pyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spread out the jobs across 25 files\n",
    "#5000/25=200\n",
    "count = 0\n",
    "fcount = 0\n",
    "for f in sorted(pyfiles):\n",
    "    if count == 0:\n",
    "        text = 'python %s\\n' % f\n",
    "    else:\n",
    "        newtext = 'python %s\\n' % f\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    if count == 200:\n",
    "        count = 0\n",
    "        fcount += 1\n",
    "        filE = '/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/runpyfiles/%s_cmds.txt' % str(fcount).zfill(3)\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make 125 qsub files\n",
    "for i in range(25):\n",
    "    j = i+1\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N pyfile%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cat %s_cmds.txt | parallel --progress --\n",
    "\n",
    "''' % (str(j).zfill(3),\n",
    "       str(j).zfill(3)\n",
    "      )\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/runpyfiles/%s_run.sh' % str(j).zfill(3)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#qsub the qsubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/randDVALS/'\n",
    "ds = [op.join(DIR,d) for d in ls(DIR)]\n",
    "count = 0\n",
    "for DIR in ds:\n",
    "    count = count + len(ls(DIR))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#double check data\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno, len(outlierdata[pheno].index),len(snpdict995[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "dvals = OrderedDict()\n",
    "medvals = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariances_total_effect_995/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "\n",
    "    DIR = '/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/randDVALS/%s/' % pheno\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "\n",
    "    fcount = 0\n",
    "    medvals[pheno] = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        rvals = [] #random dij values\n",
    "        for lst in df.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    rvals.append(x)\n",
    "\n",
    "\n",
    "        medvals[pheno].append(np.median([abs(val) for val in rvals]))\n",
    "\n",
    "        fcount += 1\n",
    "        if fcount % 100 == 0:\n",
    "            print fcount\n",
    "\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    if not op.exists(op.dirname(filE)):\n",
    "        os.makedirs(op.dirname(filE))\n",
    "    medvaldf = pd.DataFrame(medvals[pheno])\n",
    "    medvaldf.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#in case I restart my notebook\n",
    "medvals = {}\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariances_total_effect_995/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_total_effect_995/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    df = pd.read_csv(filE,header=None,sep='\\t')\n",
    "    medvals[pheno] = df[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno,'has',len(snpdict995[pheno]),'SNPs in intersection of top PIPs and top effects (alpha+beta)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariance among top PIPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "effect_snps = {}\n",
    "for pheno in phenos:\n",
    "    d = combined_dfs[pheno]\n",
    "    x = 'gamma_hmean'\n",
    "    effect_snps[pheno] = {}\n",
    "    for q in [0.999]:\n",
    "        x99_cutoff = d[x].quantile(q)\n",
    "        xvals = d[x][d[x] >= x99_cutoff] \n",
    "        effect_snps[pheno][q] = xvals.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in effect_snps.keys():\n",
    "    for q in effect_snps[pheno].keys():\n",
    "        print pheno,q,len(effect_snps[pheno][q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(outdir, \"effect_snps_top_PIPs_50mil.pkl\"), \"wb\") as o:\n",
    "    pickle.dump(effect_snps, o, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write out the files\n",
    "for pheno in effect_snps.keys():\n",
    "    for q in effect_snps[pheno].keys():\n",
    "        out = op.join(outdir,\"%s_effect_top_PIPs_50mil.txt\" % \"-\".join([pheno,str(q)]))\n",
    "        with open(out,'w') as o:\n",
    "            o.write(\"\\n\".join(effect_snps[pheno][q]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toppips = {}\n",
    "x = 'gamma_hmean'\n",
    "q = 0.999\n",
    "for pheno in phenos:\n",
    "    d = combined_dfs[pheno]\n",
    "    cutoff = d[x].quantile(q)\n",
    "    xvals = d[x][d[x]>=cutoff]\n",
    "    toppips[pheno] = xvals.index\n",
    "    print pheno,len(toppips[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rc = Client(profile=\"xmn\")\n",
    "dview = rc[:]\n",
    "lview = rc.load_balanced_view()\n",
    "len(lview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "op.makedirs(\"/home/lindb/wbp/gemma/covariances_top_PIPs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do pairwise to get Dij\n",
    "def get_dijdict(tokens):\n",
    "    from collections import OrderedDict\n",
    "    from os import path as op\n",
    "    from os import listdir as ls\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    pheno,outliersnps,glob,mafDict,popDict = tokens\n",
    "    dijDict = OrderedDict() \n",
    "    dijDict[pheno] = OrderedDict() \n",
    "    for i,locusi in enumerate(outliersnps):\n",
    "        dijDict[pheno][locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(outliersnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popDict.keys():\n",
    "                    qik = mafDict[locusi][pop] #get pop maf\n",
    "                    qjk = mafDict[locusj][pop] #get pop maf\n",
    "                    nk = popDict[pop]\n",
    "\n",
    "                    sums += (float(nk)/float(88))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[pheno][locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[pheno][locusi][locusj] = np.nan\n",
    "    #write out the file\n",
    "    for pheno in dijDict.keys():\n",
    "        filE = '/home/lindb/wbp/gemma/covariances_top_PIPs/dvals/%s_dvals.txt' % pheno\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            key0 = dijDict[pheno].keys()[0]\n",
    "            line = '\\t'.join(dijDict[pheno][key0].keys()) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "            for locusi in dijDict[pheno].keys():\n",
    "                line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[pheno][locusi].values()]) + str('\\n')\n",
    "                o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jobs = []\n",
    "for pheno in toppips.keys():\n",
    "    print pheno\n",
    "    jobs.append(lview.apply_async(get_dijdict,[pheno,toppips[pheno],glob,mafDict,popsizeDict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in jobs:\n",
    "    if j.ready():\n",
    "        print j.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariances_top_PIPs/dvals'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "dijDict = OrderedDict()\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    dijDict[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "dijDict[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(dijDict[pheno].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataframe with the outlier loci and their bins\n",
    "outlierdata = OrderedDict()\n",
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(toppips[pheno])\n",
    "    outliersnps = toppips[pheno]\n",
    "\n",
    "    outlierdata[pheno] = pd.DataFrame(H[H.index.isin(outliersnps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(outlierdata[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/grid_imp_hexp_hist_by_pheno_top_PIPs.pdf') as pdf:\n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "\n",
    "    a1 = plt.subplot(231)\n",
    "    a1.hist(outlierdata['bfpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a1.set_title('A',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a1.spines['right'].set_visible(False)\n",
    "    a1.spines['top'].set_visible(False)\n",
    "    a1.yaxis.set_ticks_position('left')\n",
    "    a1.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a2 = plt.subplot(232)\n",
    "    a2.hist(outlierdata['c13popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a2.set_title('B',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a2.spines['right'].set_visible(False)\n",
    "    a2.spines['top'].set_visible(False)\n",
    "    a2.yaxis.set_ticks_position('left')\n",
    "    a2.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a3 = plt.subplot(233)\n",
    "    a3.hist(outlierdata['htpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a3.set_title('C',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a3.spines['right'].set_visible(False)\n",
    "    a3.spines['top'].set_visible(False)\n",
    "    a3.yaxis.set_ticks_position('left')\n",
    "    a3.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a4 = plt.subplot(234)\n",
    "    a4.hist(outlierdata['n15popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a4.set_title('D',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a4.spines['right'].set_visible(False)\n",
    "    a4.spines['top'].set_visible(False)\n",
    "    a4.yaxis.set_ticks_position('left')\n",
    "    a4.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a5 = plt.subplot(235)\n",
    "    a5.hist(outlierdata['rspopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a5.set_title('E',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a5.spines['right'].set_visible(False)\n",
    "    a5.spines['top'].set_visible(False)\n",
    "    a5.yaxis.set_ticks_position('left')\n",
    "    a5.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a6.spines['right'].set_visible(False)\n",
    "    a6.spines['left'].set_visible(False)\n",
    "    a6.spines['top'].set_visible(False)\n",
    "    a6.spines['bottom'].set_visible(False)\n",
    "    a6.axes.get_yaxis().set_visible(False)\n",
    "    a6.axes.get_xaxis().set_ticks([])\n",
    "    a6.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    fig.set_size_inches(13,10)\n",
    "    pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get sets of SNPs to make random draws\n",
    "nonsigsnps = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    nonsigs = list(set(H.index.tolist()) - set(outlierdata[pheno].index.tolist()))\n",
    "    nonsigsnps[pheno] = pd.DataFrame(H[H.index.isin(nonsigs)])\n",
    "    print len(outlierdata[pheno].index.tolist())+len(nonsigsnps[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many random snps to draw from each bin?\n",
    "binCounter = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    binCounter[pheno] = Counter()\n",
    "    for row in outlierdata[pheno].index:\n",
    "        binCounter[pheno][outlierdata[pheno].loc[row,'bin']] += 1\n",
    "    for b in binCounter[pheno].keys():\n",
    "        print pheno,b,binCounter[pheno][b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices')\n",
    "os.makedirs('/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/randsnps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in outlierdata.keys():\n",
    "    print pheno,len(outlierdata[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each pheno make 1000 dataframes with a set of snps == len(outliersnps[pheno])\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    for i in range(1000):  \n",
    "        snps = []        \n",
    "        for binn in binCounter[pheno].keys():\n",
    "            data = nonsigsnps[pheno][nonsigsnps[pheno]['bin'] == binn]\n",
    "                \n",
    "            [snps.append(snp) for snp in random.sample(data.index,binCounter[pheno][binn])]\n",
    "        \n",
    "        assert len(np.unique(snps).tolist()) == sum(binCounter[pheno].values())\n",
    "\n",
    "        DIR = '/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/randsnps/%s' % pheno\n",
    "        if not op.exists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = op.join(DIR,\"%s_%s_randsnps.txt\" % (pheno,str(i).zfill(3)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quicker to do it this way\n",
    "#get dvals for the sets of random snps for each pheno\n",
    "for pheno in outlierdata.keys():\n",
    "    for i in range(1000):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "#population sizes (only for individuals in pops with phenotypic measurements)\n",
    "popDict = {'Dicks_Pass':15,\n",
    "'Freel_Peak':19,\n",
    "'Little_Round_Top':14,\n",
    "'Mt_Rose_Ophir':11,\n",
    "'Rifle_Peak':15,\n",
    "'Snow_Valley_Peak':14,}\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/gemma/6pop_pop_maf.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/randsnps/%s/%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/88)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/randDVALS/%s/%s_%s_randDVALS.txt'\n",
    "if not op.exists('/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/randDVALS/'):\n",
    "    os.makedirs('/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/randDVALS/')\n",
    "DIR = op.dirname(filE)\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "        \n",
    "''' % (pheno,pheno,str(i).zfill(3),\n",
    "       pheno,pheno,str(i).zfill(3))\n",
    "        filE = '/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/randpyfiles/get_rand_dvals_%s_%s.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(3)\n",
    "                                                                                                     )\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/randpyfiles/'\n",
    "pyfiles = [op.join(DIR,f) for f in ls(DIR)]\n",
    "len(pyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spread out the jobs across 25 files\n",
    "count = 0\n",
    "fcount = 0\n",
    "for f in sorted(pyfiles):\n",
    "    if count == 0:\n",
    "        text = 'python %s\\n' % f\n",
    "    else:\n",
    "        newtext = 'python %s\\n' % f\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    if count == 200:\n",
    "        count = 0\n",
    "        fcount += 1\n",
    "        filE = '/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/runpyfiles/%s_cmds.txt' % str(fcount).zfill(3)\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make 125 qsub files\n",
    "for i in range(25):\n",
    "    j = i+1\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N pyfile%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cat %s_cmds.txt | parallel --progress --\n",
    "\n",
    "''' % (str(j).zfill(3),\n",
    "       str(j).zfill(3)\n",
    "      )\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/runpyfiles/%s_run.sh' % str(j).zfill(3)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#qsub the qsubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/randDVALS/'\n",
    "ds = [op.join(DIR,d) for d in ls(DIR)]\n",
    "count = 0\n",
    "for DIR in ds:\n",
    "    count = count + len(ls(DIR))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos = ['bfpopx','c13popx','htpopx','n15popx','rspopx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "dvals = OrderedDict()\n",
    "medvals = OrderedDict()\n",
    "for pheno in phenos:\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariances_top_PIPs/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "\n",
    "    DIR = '/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/randDVALS/%s/' % pheno\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "\n",
    "    fcount = 0\n",
    "    medvals[pheno] = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        rvals = [] #random dij values\n",
    "        for lst in df.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    rvals.append(x)\n",
    "\n",
    "\n",
    "        medvals[pheno].append(np.median([abs(val) for val in rvals]))\n",
    "\n",
    "        fcount += 1\n",
    "        if fcount % 100 == 0:\n",
    "            print fcount\n",
    "\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    if not op.exists(op.dirname(filE)):\n",
    "        os.makedirs(op.dirname(filE))\n",
    "    medvaldf = pd.DataFrame(medvals[pheno])\n",
    "    medvaldf.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#in case I restart my notebook\n",
    "medvals = {}\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariances_top_PIPs/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_top_PIPs/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    df = pd.read_csv(filE,header=None,sep='\\t')\n",
    "    medvals[pheno] = df[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in phenos:\n",
    "    print pheno,'has',len(toppips[pheno]),'SNPs in top PIPs (gamma)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I moved the /covariances directory to /covariances_top_PIPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariance of top 0.998 PIPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_dfs['bfpopx'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pip998 = {}\n",
    "x = 'gamma_hmean'\n",
    "q = 0.998\n",
    "for pheno in phenos:\n",
    "    d = combined_dfs[pheno]\n",
    "    cutoff = d[x].quantile(q)\n",
    "    xvals = d[x][d[x]>=cutoff]\n",
    "    pip998[pheno] = xvals.index\n",
    "    print pheno,len(pip998[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toppips[pheno][0] in pip998[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in toppips:\n",
    "    snps = set(toppips[pheno]).intersection(set(pip998[pheno]))\n",
    "    print pheno,len(snps),len(toppips[pheno]),len(pip998[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariance_pips_998')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do pairwise to get Dij\n",
    "for pheno in pip998.keys():\n",
    "    outliersnps = pip998[pheno]\n",
    "    popDict = OrderedDict(popsizeDict)\n",
    "\n",
    "    dijDict = OrderedDict() \n",
    "    dijDict[pheno] = OrderedDict() \n",
    "    for i,locusi in enumerate(outliersnps):\n",
    "        dijDict[pheno][locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(outliersnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popDict.keys():\n",
    "                    qik = mafDict[locusi][pop] #get pop maf\n",
    "                    qjk = mafDict[locusj][pop] #get pop maf\n",
    "                    nk = popDict[pop]\n",
    "\n",
    "                    sums += (float(nk)/float(88))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[pheno][locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[pheno][locusi][locusj] = np.nan\n",
    "    #write out the file\n",
    "    for pheno in dijDict.keys(): #indented by accident, would just overwrite and take longer than necessary\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_pips_998/dvals/%s_dvals.txt' % pheno\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            key0 = dijDict[pheno].keys()[0]\n",
    "            line = '\\t'.join(dijDict[pheno][key0].keys()) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "            for locusi in dijDict[pheno].keys():\n",
    "                line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[pheno][locusi].values()]) + str('\\n')\n",
    "                o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariance_pips_998/dvals'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "dijDict = OrderedDict()\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    dijDict[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "dijDict[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(dijDict[pheno].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataframe with the outlier loci and their bins\n",
    "outlierdata = OrderedDict()\n",
    "for pheno in dijDict.keys():\n",
    "    outliersnps = pip998[pheno]\n",
    "\n",
    "    outlierdata[pheno] = pd.DataFrame(H[H.index.isin(outliersnps)])\n",
    "    print pheno,len(outlierdata[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get sets of SNPs to make random draws\n",
    "nonsigsnps = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    nonsigs = list(set(H.index.tolist()) - set(outlierdata[pheno].index.tolist()))\n",
    "    nonsigsnps[pheno] = pd.DataFrame(H[H.index.isin(nonsigs)])\n",
    "    print len(outlierdata[pheno].index.tolist())+len(nonsigsnps[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many random snps to draw from each bin?\n",
    "binCounter = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    binCounter[pheno] = Counter()\n",
    "    for row in outlierdata[pheno].index:\n",
    "        binCounter[pheno][outlierdata[pheno].loc[row,'bin']] += 1\n",
    "    for b in binCounter[pheno].keys():\n",
    "        print pheno,b,binCounter[pheno][b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariance_pips_998/randmatrices')\n",
    "os.makedirs('/home/lindb/wbp/gemma/covariance_pips_998/randmatrices/randsnps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in outlierdata.keys():\n",
    "    print pheno,len(outlierdata[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each pheno make 1000 dataframes with a set of snps == len(outliersnps[pheno])\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    for i in range(1000):  \n",
    "        snps = []        \n",
    "        for binn in binCounter[pheno].keys():\n",
    "            data = nonsigsnps[pheno][nonsigsnps[pheno]['bin'] == binn]\n",
    "                \n",
    "            [snps.append(snp) for snp in random.sample(data.index,binCounter[pheno][binn])]\n",
    "        \n",
    "        assert len(np.unique(snps).tolist()) == sum(binCounter[pheno].values())\n",
    "\n",
    "        DIR = '/home/lindb/wbp/gemma/covariance_pips_998/randmatrices/randsnps/%s' % pheno\n",
    "        if not op.exists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = op.join(DIR,\"%s_%s_randsnps.txt\" % (pheno,str(i).zfill(3)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quicker to do it this way\n",
    "#get dvals for the sets of random snps for each pheno\n",
    "for pheno in outlierdata.keys():\n",
    "    for i in range(1000):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "#population sizes (only for individuals in pops with phenotypic measurements)\n",
    "popDict = {'Dicks_Pass':15,\n",
    "'Freel_Peak':19,\n",
    "'Little_Round_Top':14,\n",
    "'Mt_Rose_Ophir':11,\n",
    "'Rifle_Peak':15,\n",
    "'Snow_Valley_Peak':14,}\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/gemma/6pop_pop_maf.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/gemma/covariance_pips_998/randmatrices/randsnps/%s/%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/88)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/covariance_pips_998/randmatrices/randDVALS/%s/%s_%s_randDVALS.txt'\n",
    "if not op.exists('/home/lindb/wbp/gemma/covariance_pips_998/randmatrices/randDVALS/'):\n",
    "    os.makedirs('/home/lindb/wbp/gemma/covariance_pips_998/randmatrices/randDVALS/')\n",
    "DIR = op.dirname(filE)\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "        \n",
    "''' % (pheno,pheno,str(i).zfill(3),\n",
    "       pheno,pheno,str(i).zfill(3))\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_pips_998/randmatrices/randpyfiles/get_rand_dvals_%s_%s.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(3)\n",
    "                                                                                                     )\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the python scripts\n",
    "DIR = '/home/lindb/wbp/gemma/covariance_pips_998/randmatrices/randpyfiles/'\n",
    "pyfiles = [op.join(DIR,f) for f in ls(DIR)]\n",
    "len(pyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spread out the jobs \n",
    "count = 0\n",
    "fcount = 0\n",
    "for f in sorted(pyfiles):\n",
    "    if count == 0:\n",
    "        text = 'python %s\\n' % f\n",
    "    else:\n",
    "        newtext = 'python %s\\n' % f\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    if count == 200:\n",
    "        count = 0\n",
    "        fcount += 1\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_pips_998/randmatrices/runpyfiles/%s_cmds.txt' % str(fcount).zfill(3)\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make qsub files\n",
    "for i in range(25):\n",
    "    j = i+1\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N pyfile%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cat %s_cmds.txt | parallel --progress --\n",
    "\n",
    "''' % (str(j).zfill(3),\n",
    "       str(j).zfill(3)\n",
    "      )\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_pips_998/randmatrices/runpyfiles/%s_run.sh' % str(j).zfill(3)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/gemma/covariance_pips_998/randmatrices/randDVALS/'\n",
    "ds = [op.join(DIR,d) for d in ls(DIR)]\n",
    "count = 0\n",
    "for DIR in ds:\n",
    "    count = count + len(ls(DIR))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "dvals = OrderedDict()\n",
    "medvals = OrderedDict()\n",
    "for pheno in phenos:\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariance_pips_998/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "\n",
    "    DIR = '/home/lindb/wbp/gemma/covariance_pips_998/randmatrices/randDVALS/%s/' % pheno\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "\n",
    "    fcount = 0\n",
    "    medvals[pheno] = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        rvals = [] #random dij values\n",
    "        for lst in df.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    rvals.append(x)\n",
    "\n",
    "\n",
    "        medvals[pheno].append(np.median([abs(val) for val in rvals]))\n",
    "\n",
    "        fcount += 1\n",
    "        if fcount % 100 == 0:\n",
    "            print fcount\n",
    "\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_total_effect_999/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    if not op.exists(op.dirname(filE)):\n",
    "        os.makedirs(op.dirname(filE))\n",
    "    medvaldf = pd.DataFrame(medvals[pheno])\n",
    "    medvaldf.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in pip998.keys():\n",
    "    print pheno,'has',len(pip998[pheno]),'SNPs in intersection of top PIPs and top effects (alpha+beta)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariance top PIPs true mafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in effect_snps.keys():\n",
    "    for q in effect_snps[pheno].keys():\n",
    "        print pheno,q,len(effect_snps[pheno][q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in toppips.keys():\n",
    "    print pheno, len(toppips[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rc = Client(profile=\"xmn\")\n",
    "dview = rc[:]\n",
    "lview = rc.load_balanced_view()\n",
    "len(lview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"/home/lindb/wbp/gemma/covariances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file made in 09_OutFLANK.ipynb\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/OutFLANK/imputed_MAF.txt',header=0,index_col=0,sep=\"\\t\")\n",
    "impMAF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#file made in 09_OutFLANK.ipynb\n",
    "globmafs = pd.read_csv('/home/lindb/wbp/OutFLANK/global_mafs.txt',header=0,sep='\\t')\n",
    "globmafs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create dictionary, key = SNP, val = maf\n",
    "glob = OrderedDict()\n",
    "for row in globmafs.index:\n",
    "    locus = globmafs.loc[row,'locus']\n",
    "    glob[locus] = globmafs.loc[row,'maf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#population sizes\n",
    "popDict = {'Dicks_Pass':25,\n",
    "'Freel_Peak':48,\n",
    "'Little_Round_Top':25,\n",
    "'Heavenly':25,\n",
    "'Mt_Rose_Ophir':49,\n",
    "'Rifle_Peak':24,\n",
    "'Snow_Valley_Peak':24,\n",
    "'West_Shore_Peaks':24}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#do pairwise to get Dij\n",
    "dijDict = OrderedDict() \n",
    "for pheno in toppips.keys():\n",
    "    outliersnps = toppips[pheno]\n",
    "    \n",
    "    dijDict[pheno] = OrderedDict() \n",
    "    icount = 0\n",
    "    for i,locusi in enumerate(outliersnps):\n",
    "        dijDict[pheno][locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(outliersnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in impMAF.columns:\n",
    "                    qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                    qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                    nk = popDict[pop]\n",
    "\n",
    "                    sums += (nk/244)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[pheno][locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[pheno][locusi][locusj] = np.nan\n",
    "        icount += 1\n",
    "        if icount % 10 == 0:\n",
    "            print pheno,icount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariances/dvals/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write out the file\n",
    "for pheno in dijDict.keys():\n",
    "    filE = '/home/lindb/wbp/gemma/covariances/dvals/%s_dvals.txt' % pheno\n",
    "    with open(filE,'w') as o:\n",
    "        key0 = dijDict[pheno].keys()[0]\n",
    "        line = '\\t'.join(dijDict[pheno][key0].keys()) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "        for locusi in dijDict[pheno].keys():\n",
    "            line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[pheno][locusi].values()]) + str('\\n')\n",
    "            o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariances/dvals'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "dijDict = OrderedDict()\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    dijDict[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "dijDict[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dijDict[pheno].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(dijDict[pheno].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get H_exp for each snp\n",
    "#file made in 09_OutFLANK.ipynb\n",
    "filE = '/home/lindb/wbp/OutFLANK/Hexp_by_snp_withbins.txt'\n",
    "H = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "H = pd.DataFrame(H.loc[:,['h_exp','bin']])\n",
    "H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(H.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataframe with the outlier loci and their bins\n",
    "outlierdata = OrderedDict()\n",
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(toppips[pheno])\n",
    "    outliersnps = toppips[pheno]\n",
    "\n",
    "    outlierdata[pheno] = pd.DataFrame(H[H.index.isin(outliersnps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/grid_imp_hexp_hist_by_pheno_top_PIPs_true_freqs.pdf') as pdf:\n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "\n",
    "    a1 = plt.subplot(231)\n",
    "    a1.hist(outlierdata['bfpopx']['h_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a1.set_title('A',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a1.spines['right'].set_visible(False)\n",
    "    a1.spines['top'].set_visible(False)\n",
    "    a1.yaxis.set_ticks_position('left')\n",
    "    a1.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a2 = plt.subplot(232)\n",
    "    a2.hist(outlierdata['c13popx']['h_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a2.set_title('B',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a2.spines['right'].set_visible(False)\n",
    "    a2.spines['top'].set_visible(False)\n",
    "    a2.yaxis.set_ticks_position('left')\n",
    "    a2.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a3 = plt.subplot(233)\n",
    "    a3.hist(outlierdata['htpopx']['h_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a3.set_title('C',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a3.spines['right'].set_visible(False)\n",
    "    a3.spines['top'].set_visible(False)\n",
    "    a3.yaxis.set_ticks_position('left')\n",
    "    a3.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a4 = plt.subplot(234)\n",
    "    a4.hist(outlierdata['n15popx']['h_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a4.set_title('D',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a4.spines['right'].set_visible(False)\n",
    "    a4.spines['top'].set_visible(False)\n",
    "    a4.yaxis.set_ticks_position('left')\n",
    "    a4.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a5 = plt.subplot(235)\n",
    "    a5.hist(outlierdata['rspopx']['h_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a5.set_title('E',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a5.spines['right'].set_visible(False)\n",
    "    a5.spines['top'].set_visible(False)\n",
    "    a5.yaxis.set_ticks_position('left')\n",
    "    a5.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a6.spines['right'].set_visible(False)\n",
    "    a6.spines['left'].set_visible(False)\n",
    "    a6.spines['top'].set_visible(False)\n",
    "    a6.spines['bottom'].set_visible(False)\n",
    "    a6.axes.get_yaxis().set_visible(False)\n",
    "    a6.axes.get_xaxis().set_ticks([])\n",
    "    a6.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    fig.set_size_inches(13,10)\n",
    "    pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get sets of SNPs to make random draws\n",
    "nonsigsnps = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    nonsigs = list(set(H.index.tolist()) - set(outlierdata[pheno].index.tolist()))\n",
    "    nonsigsnps[pheno] = pd.DataFrame(H[H.index.isin(nonsigs)])\n",
    "    print len(outlierdata[pheno].index.tolist())+len(nonsigsnps[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many random snps to draw from each bin?\n",
    "binCounter = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    binCounter[pheno] = Counter()\n",
    "    for row in outlierdata[pheno].index:\n",
    "        binCounter[pheno][outlierdata[pheno].loc[row,'bin']] += 1\n",
    "    for b in binCounter[pheno].keys():\n",
    "        print pheno,b,binCounter[pheno][b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariances/randmatrices')\n",
    "os.makedirs('/home/lindb/wbp/gemma/covariances/randmatrices/randsnps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each pheno make 1000 dataframes with a set of snps == len(outliersnps[pheno])\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    for i in range(1000):  \n",
    "        snps = []        \n",
    "        for binn in binCounter[pheno].keys():\n",
    "            data = nonsigsnps[pheno][nonsigsnps[pheno]['bin'] == binn]\n",
    "                \n",
    "            [snps.append(snp) for snp in random.sample(data.index,binCounter[pheno][binn])]\n",
    "        \n",
    "        assert len(np.unique(snps).tolist()) == sum(binCounter[pheno].values())\n",
    "\n",
    "        DIR = '/home/lindb/wbp/gemma/covariances/randmatrices/randsnps/%s' % pheno\n",
    "        if not op.exists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = op.join(DIR,\"%s_%s_randsnps.txt\" % (pheno,str(i).zfill(3)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quicker to do it this way\n",
    "#get dvals for the sets of random snps for each pheno\n",
    "for pheno in outlierdata.keys():\n",
    "    for i in range(1000):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "#get pop assignment for each samp\n",
    "filE = '/home/lindb/wbp/sampsTOpop.txt'\n",
    "stp = pd.read_csv(filE,header=0,sep=\"\\t\")\n",
    "\n",
    "\n",
    "#pops matched to samps\n",
    "ptsDict = OrderedDict() #pop to samp dictionary\n",
    "for row in stp.index:\n",
    "    pop = stp.loc[row,'pop']\n",
    "    if not pop in ptsDict.keys():\n",
    "        ptsDict[pop] = []\n",
    "    ptsDict[pop].append(stp.loc[row,'sampID'])\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "popDict = OrderedDict()\n",
    "total = 0\n",
    "for pop in ptsDict.keys():\n",
    "    popDict[pop] = len(ptsDict[pop])\n",
    "    print pop,popDict[pop]\n",
    "    total += popDict[pop]\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/OutFLANK/global_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/OutFLANK/imputed_MAF.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/gemma/covariances/randmatrices/randsnps/%s/%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/244)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/covariances/randmatrices/randDVALS/%s/%s_%s_randDVALS.txt'\n",
    "if not op.exists('/home/lindb/wbp/gemma/covariances/randmatrices/randDVALS/'):\n",
    "    os.makedirs('/home/lindb/wbp/gemma/covariances/randmatrices/randDVALS/')\n",
    "DIR = op.dirname(filE)\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "\n",
    "\n",
    "''' % (pheno,pheno,str(i).zfill(3),\n",
    "       pheno,pheno,str(i).zfill(3))\n",
    "        filE = '/home/lindb/wbp/gemma/covariances/randmatrices/randpyfiles/get_rand_dvals_%s_%s.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(3)\n",
    "                                                                                                     )\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariances/randmatrices/randpyfiles/'\n",
    "pyfiles = [op.join(DIR,f) for f in ls(DIR)]\n",
    "len(pyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "5000/180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spread out the jobs across 180 files\n",
    "#18000/25=720\n",
    "tcount  = 0\n",
    "fcount = 0\n",
    "tcount = 0\n",
    "for f in sorted(pyfiles):\n",
    "    if count == 0:\n",
    "        text = 'python %s\\n' % f\n",
    "    else:\n",
    "        newtext = 'python %s\\n' % f\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    tcount +=1\n",
    "    if (count == 28) or (tcount ==5000):\n",
    "        count = 0\n",
    "        fcount += 1\n",
    "        filE = '/home/lindb/wbp/gemma/covariances/randmatrices/runpyfiles/%s_cmds.txt' % str(fcount).zfill(3)\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make 180 qsub files\n",
    "for i in range(179):\n",
    "    j = i+1\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N pyfile%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cat %s_cmds.txt | parallel --progress --\n",
    "\n",
    "''' % (str(j).zfill(3),\n",
    "       str(j).zfill(3)\n",
    "      )\n",
    "    filE = '/home/lindb/wbp/gemma/covariances/randmatrices/runpyfiles/%s_run.sh' % str(j).zfill(3)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#qsub the qsubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/gemma/covariances/randmatrices/randDVALS/'\n",
    "ds = [op.join(DIR,d) for d in ls(DIR)]\n",
    "count = 0\n",
    "for DIR in ds:\n",
    "    count = count + len(ls(DIR))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "dvals = OrderedDict()\n",
    "medvals = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariances/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "\n",
    "    DIR = '/home/lindb/wbp/gemma/covariances/randmatrices/randDVALS/%s/' % pheno\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "\n",
    "    fcount = 0\n",
    "    medvals[pheno] = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        rvals = [] #random dij values\n",
    "        for lst in df.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    rvals.append(x)\n",
    "\n",
    "\n",
    "        medvals[pheno].append(np.median([abs(val) for val in rvals]))\n",
    "\n",
    "        fcount += 1\n",
    "        if fcount % 100 == 0:\n",
    "            print fcount\n",
    "\n",
    "    filE = '/home/lindb/wbp/gemma/covariances/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    if not op.exists(op.dirname(filE)):\n",
    "        os.makedirs(op.dirname(filE))\n",
    "    medvaldf = pd.DataFrame(medvals[pheno])\n",
    "    medvaldf.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#in case I restart my notebook\n",
    "medvals = {}\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariances_top_PIPs_true_mafs/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_top_PIPs_true_mafs/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    df = pd.read_csv(filE,header=None,sep='\\t')\n",
    "    medvals[pheno] = df[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno,'has',len(toppips[pheno]),'SNPs in intersection of top PIPs and top effects (alpha+beta)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,i/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,i/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at effect distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(combined_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this dictionary was made elsewhere in 11_GEMMA.ipynb; I came back to remake soem of the distribution charts\n",
    "for pheno in alphasnps:\n",
    "    print pheno,len(alphasnps999[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in toppips:\n",
    "    print pheno,len(toppips[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in pip998:\n",
    "    print pheno,len(pip998[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "effects = OrderedDict()\n",
    "for pheno in combined_dfs.keys():\n",
    "    effects[pheno] = OrderedDict()\n",
    "    effects[pheno]['toppips'] = pd.DataFrame(combined_dfs[pheno][combined_dfs[pheno].index.isin(toppips[pheno])])\n",
    "#    effects[pheno]['snpdict999'] = pd.DataFrame(combined_dfs[pheno][combined_dfs[pheno].index.isin(snpdict999[pheno])])\n",
    "#    effects[pheno]['snpdict995'] = pd.DataFrame(combined_dfs[pheno][combined_dfs[pheno].index.isin(snpdict995[pheno])])\n",
    "#    effects[pheno]['top alphas'] = pd.DataFrame(combined_dfs[pheno][combined_dfs[pheno].index.isin(alphasnps999[pheno])])\n",
    "    effects[pheno]['pip998']     = pd.DataFrame(combined_dfs[pheno][combined_dfs[pheno].index.isin(pip998[pheno])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in effects:\n",
    "    for crit in effects[pheno]:\n",
    "        print pheno,crit,len(effects[pheno][crit].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/alpha_effects_2_criteria_by_pheno.pdf') as pdf:\n",
    "    crit = 'alpha_hmean'\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "    \n",
    "    plotdict = {}\n",
    "    count = 1\n",
    "    for pheno in effects:\n",
    "        plotdict[count] = plt.subplot(int(\"23%s\" % count))\n",
    "        a = effects[pheno]['toppips'][crit].tolist()\n",
    "        b = effects[pheno]['pip998'][crit].tolist()\n",
    "        bins=np.histogram(np.hstack((a,b)), bins=40)[1]\n",
    "        sns.kdeplot(np.asarray(a),shade=True,legend=True)\n",
    "        sns.kdeplot(np.asarray(b),shade=True,legend=True)\n",
    "        plt.legend(['99.9th PIPs','99.8th PIPs'])\n",
    "        plotdict[count].set_title('%s alpha' % pheno,y=.9,loc='left',fontsize=10,fontweight='bold')\n",
    "        plotdict[count].spines['right'].set_visible(False)\n",
    "        plotdict[count].spines['top'].set_visible(False)\n",
    "        plotdict[count].yaxis.set_ticks_position('left')\n",
    "        plotdict[count].xaxis.set_ticks_position('bottom')\n",
    "        plotdict[count].locator_params(axis='x',nbins=4)\n",
    "\n",
    "        fig.set_size_inches(13,10)\n",
    "        pdf.savefig(fig,bbox_inches='tight')\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/alpha_effects_2_criteria_by_pheno.pdf') as pdf:\n",
    "    crit = 'alpha_hmean'\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "    \n",
    "    plotdict = {}\n",
    "    count = 1\n",
    "    for pheno in effects:\n",
    "        plotdict[count] = plt.subplot(int(\"23%s\" % count))\n",
    "        a = effects[pheno]['toppips'][crit].tolist()\n",
    "        b = effects[pheno]['pip998'][crit].tolist()\n",
    "        bins=np.histogram(np.hstack((a,b)), bins=40)[1]\n",
    "        plotdict[count].hist(a,label='99.9th PIPs',alpha=0.5,bins=bins)\n",
    "        plotdict[count].hist(b,label='99.8th PIPs',alpha=0.5,bins=bins)\n",
    "        plt.legend()\n",
    "        plotdict[count].set_title('%s alpha' % pheno,y=.9,loc='left',fontsize=10,fontweight='bold')\n",
    "        plotdict[count].spines['right'].set_visible(False)\n",
    "        plotdict[count].spines['top'].set_visible(False)\n",
    "        plotdict[count].yaxis.set_ticks_position('left')\n",
    "        plotdict[count].xaxis.set_ticks_position('bottom')\n",
    "        if pheno == 'c13popx':\n",
    "            plotdict[count].locator_params(axis='x',nbins=3)\n",
    "        else:\n",
    "            plotdict[count].locator_params(axis='x',nbins=4)\n",
    "\n",
    "        fig.set_size_inches(13,10)\n",
    "        pdf.savefig(fig,bbox_inches='tight')\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in toppips:\n",
    "    snps = set(toppips[pheno]).intersection(set(pip998[pheno]))\n",
    "    print pheno,len(snps),len(toppips[pheno]),len(pip998[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/gamma_dist_2_criteria_by_pheno.pdf') as pdf:\n",
    "    crit = 'gamma_hmean'\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "    \n",
    "    plotdict = {}\n",
    "    count = 1\n",
    "    for pheno in effects:\n",
    "        plotdict[count] = plt.subplot(int(\"23%s\" % count))\n",
    "        a = effects[pheno]['toppips'][crit].tolist()\n",
    "        b = effects[pheno]['pip998'][crit].tolist()\n",
    "        bins=np.histogram(np.hstack((a,b)), bins=40)[1]\n",
    "        plotdict[count].hist(a,label='99.9th PIPs',alpha=0.5,bins=bins)\n",
    "        plotdict[count].hist(b,label='99.8th PIPs',alpha=0.5,bins=bins)\n",
    "        plotdict[count].set_title('%s %s' % (pheno,crit.split(\"_\")[0]),y=.9,loc='left',fontsize=10,fontweight='bold')\n",
    "        plotdict[count].spines['right'].set_visible(False)\n",
    "        plotdict[count].spines['top'].set_visible(False)\n",
    "        plotdict[count].yaxis.set_ticks_position('left')\n",
    "        plotdict[count].xaxis.set_ticks_position('bottom')\n",
    "        plotdict[count].locator_params(axis='x',nbins=4)\n",
    "        plt.legend(['99.9th PIPs','99.8th PIPs'])\n",
    "\n",
    "        fig.set_size_inches(13,10)\n",
    "        pdf.savefig(fig,bbox_inches='tight')\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "effects[pheno]['toppips'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/modavg_effect_dist_2_criteria_by_pheno.pdf') as pdf:\n",
    "    crit = 'model_averaged' #criteria\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "    \n",
    "    plotdict = {}\n",
    "    count = 1\n",
    "    for pheno in effects:\n",
    "        plotdict[count] = plt.subplot(int(\"23%s\" % count))\n",
    "        a = effects[pheno]['toppips'][crit].tolist()\n",
    "        b = effects[pheno]['pip998'][crit].tolist()\n",
    "        bins=np.histogram(np.hstack((a,b)), bins=40)[1]\n",
    "        plotdict[count].hist(a,label='99.9th PIPs',alpha=0.5,bins=bins)\n",
    "        plotdict[count].hist(b,label='99.8th PIPs',alpha=0.5,bins=bins)\n",
    "        plt.legend()\n",
    "        plotdict[count].set_title('%s %s' % (pheno,crit.split(\"_\")[0]),y=.9,loc='left',fontsize=10,fontweight='bold')\n",
    "        plotdict[count].spines['right'].set_visible(False)\n",
    "        plotdict[count].spines['top'].set_visible(False)\n",
    "        plotdict[count].yaxis.set_ticks_position('left')\n",
    "        plotdict[count].xaxis.set_ticks_position('bottom')\n",
    "        plotdict[count].locator_params(axis='x',nbins=4)\n",
    "\n",
    "        fig.set_size_inches(13,10)\n",
    "        pdf.savefig(fig,bbox_inches='tight')\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/beta_dist_3_criteria_by_pheno.pdf') as pdf:\n",
    "    crit = 'beta_hmean' #criteria\n",
    "    \n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "    \n",
    "    plotdict = {}\n",
    "    count = 1\n",
    "    for pheno in effects:\n",
    "        plotdict[count] = plt.subplot(int(\"23%s\" % count))\n",
    "        a = effects[pheno]['toppips'][crit].tolist()\n",
    "        b = effects[pheno]['pip998'][crit].tolist()\n",
    "        bins=np.histogram(np.hstack((a,b)), bins=40)[1]\n",
    "        plotdict[count].hist(a,label='99.9th PIPs',alpha=0.5,bins=bins)\n",
    "        plotdict[count].hist(b,label='99.8th PIPs',alpha=0.5,bins=bins)\n",
    "        plt.legend()\n",
    "        plotdict[count].set_title('%s %s' % (pheno,crit.split(\"_\")[0]),y=.9,loc='left',fontsize=10,fontweight='bold')\n",
    "        plotdict[count].spines['right'].set_visible(False)\n",
    "        plotdict[count].spines['top'].set_visible(False)\n",
    "        plotdict[count].yaxis.set_ticks_position('left')\n",
    "        plotdict[count].xaxis.set_ticks_position('bottom')\n",
    "        plotdict[count].locator_params(axis='x',nbins=4)\n",
    "\n",
    "        fig.set_size_inches(13,10)\n",
    "        pdf.savefig(fig,bbox_inches='tight')\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# covariance among top 0.99 PIPs and 0.99 top effects for ht and n15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "effect_snps = {}\n",
    "for pheno in phenos:\n",
    "    d = combined_dfs[pheno]\n",
    "    x = 'gamma_hmean'\n",
    "    for q in [0.990]:\n",
    "        x99_cutoff = d[x].quantile(q)\n",
    "        xvals = d[x][d[x] >= x99_cutoff] \n",
    "        for y in ['total_effect']:\n",
    "            y99_cutoff = d[y].quantile(q)\n",
    "            yvals = d[y][d[y] >= y99_cutoff]\n",
    "            isect = set(xvals.index).intersection(set(yvals.index))\n",
    "            effect_snps[pheno] = isect\n",
    "            print(pheno, x, y, q, len(isect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/bp/gemma/covariance_90th_n15_ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popsizeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mafs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mafDict['NODE_1000013_length_91_cov_1.802198_37']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(popsizeDict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise to get Dij\n",
    "for pheno in effect_snps.keys():\n",
    "    print pheno\n",
    "    outliersnps = effect_snps[pheno]\n",
    "\n",
    "    dijDict = OrderedDict() \n",
    "    dijDict[pheno] = OrderedDict() \n",
    "    for i,locusi in enumerate(outliersnps):\n",
    "        dijDict[pheno][locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(outliersnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popsizeDict.keys():\n",
    "                    qik = mafDict[locusi][pop] #get pop maf\n",
    "                    qjk = mafDict[locusj][pop] #get pop maf\n",
    "                    nk = popsizeDict[pop]\n",
    "\n",
    "                    sums += (float(nk)/float(88))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[pheno][locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[pheno][locusi][locusj] = np.nan\n",
    "    #write out the file\n",
    "    for pheno in dijDict.keys():\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_90th_n15_ht/dvals/%s_dvals.txt' % pheno\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            key0 = dijDict[pheno].keys()[0]\n",
    "            line = '\\t'.join(dijDict[pheno][key0].keys()) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "            for locusi in dijDict[pheno].keys():\n",
    "                line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[pheno][locusi].values()]) + str('\\n')\n",
    "                o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariance_90th_n15_ht/dvals'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "dijDict = OrderedDict()\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    dijDict[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "dijDict['n15popx'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(dijDict[pheno].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(Hexp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(H.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in effect_snps:\n",
    "    print pheno,len(effect_snps[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataframe with the outlier loci and their bins\n",
    "outlierdata = OrderedDict()\n",
    "for pheno in dijDict.keys():\n",
    "    outliersnps = effect_snps[pheno]\n",
    "\n",
    "    outlierdata[pheno] = pd.DataFrame(H[H.index.isin(outliersnps)])\n",
    "    print pheno,len(outlierdata[pheno]),len(effect_snps[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/grid_imp_hexp_hist_by_pheno_990.pdf') as pdf:\n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "\n",
    "    a1 = plt.subplot(231)\n",
    "    a1.hist(outlierdata['bfpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a1.set_title('A',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a1.spines['right'].set_visible(False)\n",
    "    a1.spines['top'].set_visible(False)\n",
    "    a1.yaxis.set_ticks_position('left')\n",
    "    a1.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a2 = plt.subplot(232)\n",
    "    a2.hist(outlierdata['c13popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a2.set_title('B',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a2.spines['right'].set_visible(False)\n",
    "    a2.spines['top'].set_visible(False)\n",
    "    a2.yaxis.set_ticks_position('left')\n",
    "    a2.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a3 = plt.subplot(233)\n",
    "    a3.hist(outlierdata['htpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a3.set_title('C',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a3.spines['right'].set_visible(False)\n",
    "    a3.spines['top'].set_visible(False)\n",
    "    a3.yaxis.set_ticks_position('left')\n",
    "    a3.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a4 = plt.subplot(234)\n",
    "    a4.hist(outlierdata['n15popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a4.set_title('D',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a4.spines['right'].set_visible(False)\n",
    "    a4.spines['top'].set_visible(False)\n",
    "    a4.yaxis.set_ticks_position('left')\n",
    "    a4.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a5 = plt.subplot(235)\n",
    "    a5.hist(outlierdata['rspopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a5.set_title('E',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a5.spines['right'].set_visible(False)\n",
    "    a5.spines['top'].set_visible(False)\n",
    "    a5.yaxis.set_ticks_position('left')\n",
    "    a5.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a6.spines['right'].set_visible(False)\n",
    "    a6.spines['left'].set_visible(False)\n",
    "    a6.spines['top'].set_visible(False)\n",
    "    a6.spines['bottom'].set_visible(False)\n",
    "    a6.axes.get_yaxis().set_visible(False)\n",
    "    a6.axes.get_xaxis().set_ticks([])\n",
    "    a6.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    fig.set_size_inches(13,10)\n",
    "    pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get sets of SNPs to make random draws\n",
    "nonsigsnps = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    nonsigs = list(set(H.index.tolist()) - set(outlierdata[pheno].index.tolist()))\n",
    "    nonsigsnps[pheno] = pd.DataFrame(H[H.index.isin(nonsigs)])\n",
    "    print len(outlierdata[pheno].index.tolist())+len(nonsigsnps[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#how many random snps to draw from each bin?\n",
    "binCounter = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    binCounter[pheno] = Counter()\n",
    "    for row in outlierdata[pheno].index:\n",
    "        binCounter[pheno][outlierdata[pheno].loc[row,'bin']] += 1\n",
    "    for b in binCounter[pheno].keys():\n",
    "        print pheno,b,binCounter[pheno][b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices')\n",
    "os.makedirs('/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/randsnps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each pheno make 1000 dataframes with a set of snps == len(outliersnps[pheno])\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    for i in range(1000):  \n",
    "        snps = []        \n",
    "        for binn in binCounter[pheno].keys():\n",
    "            data = nonsigsnps[pheno][nonsigsnps[pheno]['bin'] == binn]\n",
    "                \n",
    "            [snps.append(snp) for snp in random.sample(data.index,binCounter[pheno][binn])]\n",
    "        \n",
    "        assert len(np.unique(snps).tolist()) == sum(binCounter[pheno].values())\n",
    "\n",
    "        DIR = '/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/randsnps/%s' % pheno\n",
    "        if not op.exists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = op.join(DIR,\"%s_%s_randsnps.txt\" % (pheno,str(i).zfill(3)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quicker to do it this way\n",
    "#get dvals for the sets of random snps for each pheno\n",
    "for pheno in outlierdata.keys():\n",
    "    for i in range(1000):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "#population sizes (only for individuals in pops with phenotypic measurements)\n",
    "popDict = {'Dicks_Pass':15,\n",
    "'Freel_Peak':19,\n",
    "'Little_Round_Top':14,\n",
    "'Mt_Rose_Ophir':11,\n",
    "'Rifle_Peak':15,\n",
    "'Snow_Valley_Peak':14,}\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/gemma/6pop_pop_maf.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/randsnps/%s/%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/88)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/randDVALS/%s/%s_%s_randDVALS.txt'\n",
    "if not op.exists('/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/randDVALS/'):\n",
    "    os.makedirs('/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/randDVALS/')\n",
    "DIR = op.dirname(filE)\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "\n",
    "\n",
    "''' % (pheno,pheno,str(i).zfill(3),\n",
    "       pheno,pheno,str(i).zfill(3))\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/randpyfiles/get_rand_dvals_%s_%s.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(3)\n",
    "                                                                                                     )\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/randpyfiles/'\n",
    "pyfiles = [op.join(DIR,f) for f in ls(DIR)]\n",
    "len(pyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "5000/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spread out the jobs\n",
    "count = 0\n",
    "fcount = 0\n",
    "tcount = 0\n",
    "for f in sorted(pyfiles):\n",
    "    if count == 0:\n",
    "        text = 'python %s\\n' % f\n",
    "    else:\n",
    "        newtext = 'python %s\\n' % f\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    tcount += 1\n",
    "    if (count == 100) or (tcount == 5000):\n",
    "        count = 0\n",
    "        fcount += 1\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/runpyfiles/%s_cmds.txt' % str(fcount).zfill(3)\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make qsub files\n",
    "for i in range(50):\n",
    "    j = i+1\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N pyfile%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cat %s_cmds.txt | parallel --progress --\n",
    "\n",
    "''' % (str(j).zfill(3),\n",
    "       str(j).zfill(3)\n",
    "      )\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/runpyfiles/%s_run.sh' % str(j).zfill(3)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/randDVALS/'\n",
    "ds = [op.join(DIR,d) for d in ls(DIR)]\n",
    "count = 0\n",
    "for DIR in ds:\n",
    "    count = count + len(ls(DIR))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outlierdata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "dvals = OrderedDict()\n",
    "medvals = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariance_90th_n15_ht/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "\n",
    "    DIR = '/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/randDVALS/%s/' % pheno\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "\n",
    "    fcount = 0\n",
    "    medvals[pheno] = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        rvals = [] #random dij values\n",
    "        for lst in df.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    rvals.append(x)\n",
    "\n",
    "        medvals[pheno].append(np.median([abs(val) for val in rvals]))\n",
    "\n",
    "        fcount += 1\n",
    "        if fcount % 100 == 0:\n",
    "            print fcount\n",
    "\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_90th_n15_ht/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    if not op.exists(op.dirname(filE)):\n",
    "        os.makedirs(op.dirname(filE))\n",
    "    medvaldf = pd.DataFrame(medvals[pheno])\n",
    "    medvaldf.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno,'has',len(effect_snps[pheno]),'SNPs in intersection of top PIPs and top effects (alpha+beta)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariance among top 0.999 model average effects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_dfs[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = 'model_averaged'\n",
    "modfx = {}\n",
    "for pheno in combined_dfs:\n",
    "    modfx[pheno] = {}\n",
    "    d = combined_dfs[pheno]\n",
    "    for q in [0.999,0.995]:\n",
    "        cutoff = d[x].quantile(q)\n",
    "        xvals = d[x][d[x]>=cutoff]\n",
    "        modfx[pheno][q] = xvals.index\n",
    "        print pheno,q,len(modfx[pheno][q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariance_modfx_999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popsizeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "globs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glob['NODE_1000013_length_91_cov_1.802198_37']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in modfx:\n",
    "    print pheno,len(modfx[pheno][0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise to get Dij\n",
    "for pheno in modfx:\n",
    "    print pheno\n",
    "    outliersnps = modfx[pheno][0.999]\n",
    "    popDict = OrderedDict(popsizeDict)\n",
    "\n",
    "    dijDict = OrderedDict() \n",
    "    dijDict[pheno] = OrderedDict() \n",
    "    for i,locusi in enumerate(outliersnps):\n",
    "        dijDict[pheno][locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(outliersnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popDict.keys():\n",
    "                    qik = mafDict[locusi][pop] #get pop maf\n",
    "                    qjk = mafDict[locusj][pop] #get pop maf\n",
    "                    nk = popDict[pop]\n",
    "\n",
    "                    sums += (float(nk)/float(88))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[pheno][locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[pheno][locusi][locusj] = np.nan\n",
    "    #write out the file\n",
    "    for pheno in dijDict.keys(): #indented by accident, would just overwrite and take longer than necessary\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_modfx_999/dvals/%s_dvals.txt' % pheno\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            key0 = dijDict[pheno].keys()[0]\n",
    "            line = '\\t'.join(dijDict[pheno][key0].keys()) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "            for locusi in dijDict[pheno].keys():\n",
    "                line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[pheno][locusi].values()]) + str('\\n')\n",
    "                o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariance_modfx_999/dvals'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "dijDict = OrderedDict()\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    dijDict[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "dijDict[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(dijDict[pheno].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataframe with the outlier loci and their bins\n",
    "outlierdata = OrderedDict()\n",
    "for pheno in dijDict.keys():\n",
    "    outliersnps = modfx[pheno][0.999]\n",
    "\n",
    "    outlierdata[pheno] = pd.DataFrame(H[H.index.isin(outliersnps)])\n",
    "    print pheno,len(outlierdata[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get sets of SNPs to make random draws\n",
    "nonsigsnps = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    nonsigs = list(set(H.index.tolist()) - set(outlierdata[pheno].index.tolist()))\n",
    "    nonsigsnps[pheno] = pd.DataFrame(H[H.index.isin(nonsigs)])\n",
    "    print len(outlierdata[pheno].index.tolist())+len(nonsigsnps[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many random snps to draw from each bin?\n",
    "binCounter = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    binCounter[pheno] = Counter()\n",
    "    for row in outlierdata[pheno].index:\n",
    "        binCounter[pheno][outlierdata[pheno].loc[row,'bin']] += 1\n",
    "    for b in binCounter[pheno].keys():\n",
    "        print pheno,b,binCounter[pheno][b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices')\n",
    "os.makedirs('/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/randsnps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in outlierdata.keys():\n",
    "    print pheno,len(outlierdata[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each pheno make 1000 dataframes with a set of snps == len(outliersnps[pheno])\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    for i in range(1000):  \n",
    "        snps = []        \n",
    "        for binn in binCounter[pheno].keys():\n",
    "            data = nonsigsnps[pheno][nonsigsnps[pheno]['bin'] == binn]\n",
    "                \n",
    "            [snps.append(snp) for snp in random.sample(data.index,binCounter[pheno][binn])]\n",
    "        \n",
    "        assert len(np.unique(snps).tolist()) == sum(binCounter[pheno].values())\n",
    "\n",
    "        DIR = '/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/randsnps/%s' % pheno\n",
    "        if not op.exists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = op.join(DIR,\"%s_%s_randsnps.txt\" % (pheno,str(i).zfill(3)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quicker to do it this way\n",
    "#get dvals for the sets of random snps for each pheno\n",
    "for pheno in outlierdata.keys():\n",
    "    for i in range(1000):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "#population sizes (only for individuals in pops with phenotypic measurements)\n",
    "popDict = {'Dicks_Pass':15,\n",
    "'Freel_Peak':19,\n",
    "'Little_Round_Top':14,\n",
    "'Mt_Rose_Ophir':11,\n",
    "'Rifle_Peak':15,\n",
    "'Snow_Valley_Peak':14,}\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/gemma/6pop_pop_maf.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/randsnps/%s/%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/88)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/randDVALS/%s/%s_%s_randDVALS.txt'\n",
    "if not op.exists('/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/randDVALS/'):\n",
    "    os.makedirs('/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/randDVALS/')\n",
    "DIR = op.dirname(filE)\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "        \n",
    "''' % (pheno,pheno,str(i).zfill(3),\n",
    "       pheno,pheno,str(i).zfill(3))\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/randpyfiles/get_rand_dvals_%s_%s.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(3)\n",
    "                                                                                                     )\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the python scripts\n",
    "DIR = '/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/randpyfiles/'\n",
    "pyfiles = [op.join(DIR,f) for f in ls(DIR)]\n",
    "len(pyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spread out the jobs\n",
    "count = 0\n",
    "fcount = 0\n",
    "for f in sorted(pyfiles):\n",
    "    if count == 0:\n",
    "        text = 'python %s\\n' % f\n",
    "    else:\n",
    "        newtext = 'python %s\\n' % f\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    if count == 100:\n",
    "        count = 0\n",
    "        fcount += 1\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/runpyfiles/%s_cmds.txt' % str(fcount).zfill(3)\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make qsub files\n",
    "for i in range(50):\n",
    "    j = i+1\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N pyfile%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cat %s_cmds.txt | parallel --progress --\n",
    "\n",
    "''' % (str(j).zfill(3),\n",
    "       str(j).zfill(3)\n",
    "      )\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/runpyfiles/%s_run.sh' % str(j).zfill(3)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/randDVALS/'\n",
    "ds = [op.join(DIR,d) for d in ls(DIR)]\n",
    "count = 0\n",
    "for DIR in ds:\n",
    "    count = count + len(ls(DIR))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "dvals = OrderedDict()\n",
    "medvals = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariance_modfx_999/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "\n",
    "    DIR = '/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/randDVALS/%s/' % pheno\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "\n",
    "    fcount = 0\n",
    "    medvals[pheno] = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        rvals = [] #random dij values\n",
    "        for lst in df.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    rvals.append(x)\n",
    "\n",
    "\n",
    "        medvals[pheno].append(np.median([abs(val) for val in rvals]))\n",
    "\n",
    "        fcount += 1\n",
    "        if fcount % 100 == 0:\n",
    "            print fcount\n",
    "\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_modfx_999/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    if not op.exists(op.dirname(filE)):\n",
    "        os.makedirs(op.dirname(filE))\n",
    "    medvaldf = pd.DataFrame(medvals[pheno])\n",
    "    medvaldf.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno,'has',len(modfx[pheno][0.999]),'SNPs in top 0.999 of model averaged effects (alpha+beta*gamma)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariance among top 0.995 model averaged effects (alpha + beta x gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in modfx:\n",
    "    print pheno,len(modfx[pheno][0.995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariance_modfx_995')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise to get Dij\n",
    "for pheno in snpdict995.keys():\n",
    "    print pheno\n",
    "    outliersnps = modfx[pheno][0.995]\n",
    "    popDict = OrderedDict(popsizeDict)\n",
    "\n",
    "    dijDict = OrderedDict() \n",
    "    dijDict[pheno] = OrderedDict() \n",
    "    for i,locusi in enumerate(outliersnps):\n",
    "        dijDict[pheno][locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(outliersnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popDict.keys():\n",
    "                    qik = mafDict[locusi][pop] #get pop maf\n",
    "                    qjk = mafDict[locusj][pop] #get pop maf\n",
    "                    nk = popDict[pop]\n",
    "\n",
    "                    sums += (float(nk)/float(88))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[pheno][locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[pheno][locusi][locusj] = np.nan\n",
    "    #write out the file\n",
    "    for pheno in dijDict.keys(): #indented by accident, would just overwrite and take longer than necessary\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_modfx_995/dvals/%s_dvals.txt' % pheno\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            key0 = dijDict[pheno].keys()[0]\n",
    "            line = '\\t'.join(dijDict[pheno][key0].keys()) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "            for locusi in dijDict[pheno].keys():\n",
    "                line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[pheno][locusi].values()]) + str('\\n')\n",
    "                o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariance_modfx_995/dvals'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "dijDict = OrderedDict()\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    dijDict[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "dijDict[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(dijDict[pheno].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataframe with the outlier loci and their bins\n",
    "outlierdata = OrderedDict()\n",
    "for pheno in dijDict.keys():\n",
    "    outliersnps = modfx[pheno][0.995]\n",
    "\n",
    "    outlierdata[pheno] = pd.DataFrame(H[H.index.isin(outliersnps)])\n",
    "    print pheno,len(outlierdata[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get sets of SNPs to make random draws\n",
    "nonsigsnps = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    nonsigs = list(set(H.index.tolist()) - set(outlierdata[pheno].index.tolist()))\n",
    "    nonsigsnps[pheno] = pd.DataFrame(H[H.index.isin(nonsigs)])\n",
    "    print len(outlierdata[pheno].index.tolist())+len(nonsigsnps[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many random snps to draw from each bin?\n",
    "binCounter = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    binCounter[pheno] = Counter()\n",
    "    for row in outlierdata[pheno].index:\n",
    "        binCounter[pheno][outlierdata[pheno].loc[row,'bin']] += 1\n",
    "    for b in binCounter[pheno].keys():\n",
    "        print pheno,b,binCounter[pheno][b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices')\n",
    "os.makedirs('/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/randsnps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in outlierdata.keys():\n",
    "    print pheno,len(outlierdata[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each pheno make 1000 dataframes with a set of snps == len(outliersnps[pheno])\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    for i in range(1000):  \n",
    "        snps = []        \n",
    "        for binn in binCounter[pheno].keys():\n",
    "            data = nonsigsnps[pheno][nonsigsnps[pheno]['bin'] == binn]\n",
    "                \n",
    "            [snps.append(snp) for snp in random.sample(data.index,binCounter[pheno][binn])]\n",
    "        \n",
    "        assert len(np.unique(snps).tolist()) == sum(binCounter[pheno].values())\n",
    "\n",
    "        DIR = '/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/randsnps/%s' % pheno\n",
    "        if not op.exists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = op.join(DIR,\"%s_%s_randsnps.txt\" % (pheno,str(i).zfill(3)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quicker to do it this way\n",
    "#get dvals for the sets of random snps for each pheno\n",
    "for pheno in outlierdata.keys():\n",
    "    for i in range(1000):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "#population sizes (only for individuals in pops with phenotypic measurements)\n",
    "popDict = {'Dicks_Pass':15,\n",
    "'Freel_Peak':19,\n",
    "'Little_Round_Top':14,\n",
    "'Mt_Rose_Ophir':11,\n",
    "'Rifle_Peak':15,\n",
    "'Snow_Valley_Peak':14,}\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/gemma/6pop_pop_maf.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/randsnps/%s/%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/88)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/randDVALS/%s/%s_%s_randDVALS.txt'\n",
    "if not op.exists('/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/randDVALS/'):\n",
    "    os.makedirs('/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/randDVALS/')\n",
    "DIR = op.dirname(filE)\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "        \n",
    "''' % (pheno,pheno,str(i).zfill(3),\n",
    "       pheno,pheno,str(i).zfill(3))\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/randpyfiles/get_rand_dvals_%s_%s.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(3)\n",
    "                                                                                                     )\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the python scripts\n",
    "DIR = '/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/randpyfiles/'\n",
    "pyfiles = [op.join(DIR,f) for f in ls(DIR)]\n",
    "len(pyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyfiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spread out the jobs \n",
    "count = 0\n",
    "fcount = 0\n",
    "for f in sorted(pyfiles):\n",
    "    if count == 0:\n",
    "        text = 'python %s\\n' % f\n",
    "    else:\n",
    "        newtext = 'python %s\\n' % f\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    if count == 100:\n",
    "        count = 0\n",
    "        fcount += 1\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/runpyfiles/%s_cmds.txt' % str(fcount).zfill(3)\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make qsub files\n",
    "for i in range(50):\n",
    "    j = i+1\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N pyfile%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cat %s_cmds.txt | parallel --progress --\n",
    "\n",
    "''' % (str(j).zfill(3),\n",
    "       str(j).zfill(3)\n",
    "      )\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/runpyfiles/%s_run.sh' % str(j).zfill(3)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/randDVALS/'\n",
    "ds = [op.join(DIR,d) for d in ls(DIR)]\n",
    "count = 0\n",
    "for DIR in ds:\n",
    "    count = count + len(ls(DIR))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outlierdata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "dvals = OrderedDict()\n",
    "medvals = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariance_modfx_995/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "\n",
    "    DIR = '/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/randDVALS/%s/' % pheno\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "\n",
    "    fcount = 0\n",
    "    medvals[pheno] = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        rvals = [] #random dij values\n",
    "        for lst in df.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    rvals.append(x)\n",
    "\n",
    "\n",
    "        medvals[pheno].append(np.median([abs(val) for val in rvals]))\n",
    "\n",
    "        fcount += 1\n",
    "        if fcount % 100 == 0:\n",
    "            print fcount\n",
    "\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_modfx_995/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    if not op.exists(op.dirname(filE)):\n",
    "        os.makedirs(op.dirname(filE))\n",
    "    medvaldf = pd.DataFrame(medvals[pheno])\n",
    "    medvaldf.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno,'has',len(modfx[pheno][0.995]),'SNPs in intersection of top PIPs and top effects (alpha+beta)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariance for top alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 'alpha_hmean'\n",
    "alphasnps = {}\n",
    "for pheno in combined_dfs.keys():\n",
    "    d = combined_dfs[pheno]\n",
    "    alphasnps[pheno] = {}\n",
    "    for q in [0.999,0.995]:\n",
    "        x99_cutoff = d[x].quantile(q)\n",
    "        xvals = d[x][d[x] >= x99_cutoff] \n",
    "        alphasnps[pheno][q] = xvals.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in sorted(alphasnps):\n",
    "    for q in alphasnps[pheno]:\n",
    "        print pheno,q, len(alphasnps[pheno][q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphasnps999 = {}\n",
    "for pheno in alphasnps:\n",
    "    alphasnps999[pheno] = alphasnps[pheno][0.999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write out the alphasnps999\n",
    "for pheno in alphasnps999:\n",
    "    filE = '/home/lindb/wbp/gemma/infiles/bslmm/output/%s_top_alphasnps999.txt' % pheno\n",
    "    df = pd.DataFrame(alphasnps999[pheno])\n",
    "    df.to_csv(filE,header=None,index=False,sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impMAF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mafDict['NODE_1000013_length_91_cov_1.802198_37']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popsizeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "globs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glob['NODE_1000013_length_91_cov_1.802198_37']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariances_top_alphas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise to get Dij\n",
    "for pheno in alphasnps.keys():\n",
    "    outliersnps = alphasnps999[pheno]\n",
    "    popDict = OrderedDict(popsizeDict)\n",
    "\n",
    "    dijDict = OrderedDict() \n",
    "    dijDict[pheno] = OrderedDict() \n",
    "    for i,locusi in enumerate(outliersnps):\n",
    "        dijDict[pheno][locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(outliersnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popDict.keys():\n",
    "                    qik = mafDict[locusi][pop] #get pop maf\n",
    "                    qjk = mafDict[locusj][pop] #get pop maf\n",
    "                    nk = popDict[pop]\n",
    "\n",
    "                    sums += (float(nk)/float(88))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[pheno][locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[pheno][locusi][locusj] = np.nan\n",
    "    #write out the file\n",
    "    for pheno in dijDict.keys():\n",
    "        filE = '/home/lindb/wbp/gemma/covariances_top_alphas/dvals/%s_dvals.txt' % pheno\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            key0 = dijDict[pheno].keys()[0]\n",
    "            line = '\\t'.join(dijDict[pheno][key0].keys()) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "            for locusi in dijDict[pheno].keys():\n",
    "                line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[pheno][locusi].values()]) + str('\\n')\n",
    "                o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariances_top_alphas/dvals'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "dijDict = OrderedDict()\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    dijDict[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "dijDict[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(dijDict[pheno].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "H.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataframe with the outlier loci and their bins\n",
    "outlierdata = OrderedDict()\n",
    "for pheno in dijDict.keys():\n",
    "    outliersnps = alphasnps999[pheno]\n",
    "\n",
    "    outlierdata[pheno] = pd.DataFrame(H[H.index.isin(outliersnps)])\n",
    "    print pheno,len(outlierdata[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/grid_imp_hexp_hist_by_pheno_top_alphas_999.pdf') as pdf:\n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "\n",
    "    a1 = plt.subplot(231)\n",
    "    a1.hist(outlierdata['bfpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a1.set_title('A',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a1.spines['right'].set_visible(False)\n",
    "    a1.spines['top'].set_visible(False)\n",
    "    a1.yaxis.set_ticks_position('left')\n",
    "    a1.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a2 = plt.subplot(232)\n",
    "    a2.hist(outlierdata['c13popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a2.set_title('B',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a2.spines['right'].set_visible(False)\n",
    "    a2.spines['top'].set_visible(False)\n",
    "    a2.yaxis.set_ticks_position('left')\n",
    "    a2.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a3 = plt.subplot(233)\n",
    "    a3.hist(outlierdata['htpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a3.set_title('C',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a3.spines['right'].set_visible(False)\n",
    "    a3.spines['top'].set_visible(False)\n",
    "    a3.yaxis.set_ticks_position('left')\n",
    "    a3.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a4 = plt.subplot(234)\n",
    "    a4.hist(outlierdata['n15popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a4.set_title('D',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a4.spines['right'].set_visible(False)\n",
    "    a4.spines['top'].set_visible(False)\n",
    "    a4.yaxis.set_ticks_position('left')\n",
    "    a4.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a5 = plt.subplot(235)\n",
    "    a5.hist(outlierdata['rspopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a5.set_title('E',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a5.spines['right'].set_visible(False)\n",
    "    a5.spines['top'].set_visible(False)\n",
    "    a5.yaxis.set_ticks_position('left')\n",
    "    a5.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a6.spines['right'].set_visible(False)\n",
    "    a6.spines['left'].set_visible(False)\n",
    "    a6.spines['top'].set_visible(False)\n",
    "    a6.spines['bottom'].set_visible(False)\n",
    "    a6.axes.get_yaxis().set_visible(False)\n",
    "    a6.axes.get_xaxis().set_ticks([])\n",
    "    a6.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    fig.set_size_inches(13,10)\n",
    "    pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get sets of SNPs to make random draws\n",
    "nonsigsnps = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    nonsigs = list(set(H.index.tolist()) - set(outlierdata[pheno].index.tolist()))\n",
    "    nonsigsnps[pheno] = pd.DataFrame(H[H.index.isin(nonsigs)])\n",
    "    print len(outlierdata[pheno].index.tolist())+len(nonsigsnps[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many random snps to draw from each bin?\n",
    "binCounter = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    binCounter[pheno] = Counter()\n",
    "    for row in outlierdata[pheno].index:\n",
    "        binCounter[pheno][outlierdata[pheno].loc[row,'bin']] += 1\n",
    "    for b in binCounter[pheno].keys():\n",
    "        print pheno,b,binCounter[pheno][b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices')\n",
    "os.makedirs('/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/randsnps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in outlierdata.keys():\n",
    "    print pheno,len(outlierdata[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each pheno make 1000 dataframes with a set of snps == len(outliersnps[pheno])\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    for i in range(1000):  \n",
    "        snps = []        \n",
    "        for binn in binCounter[pheno].keys():\n",
    "            data = nonsigsnps[pheno][nonsigsnps[pheno]['bin'] == binn]\n",
    "                \n",
    "            [snps.append(snp) for snp in random.sample(data.index,binCounter[pheno][binn])]\n",
    "        \n",
    "        assert len(np.unique(snps).tolist()) == sum(binCounter[pheno].values())\n",
    "\n",
    "        DIR = '/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/randsnps/%s' % pheno\n",
    "        if not op.exists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = op.join(DIR,\"%s_%s_randsnps.txt\" % (pheno,str(i).zfill(3)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do it all at once, ninjas (quicker to do it this way)\n",
    "#you found the easter egg!\n",
    "#get dvals for the sets of random snps for each pheno\n",
    "for pheno in outlierdata.keys():\n",
    "    for i in range(1000):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "#population sizes (only for individuals in pops with phenotypic measurements)\n",
    "popDict = {'Dicks_Pass':15,\n",
    "'Freel_Peak':19,\n",
    "'Little_Round_Top':14,\n",
    "'Mt_Rose_Ophir':11,\n",
    "'Rifle_Peak':15,\n",
    "'Snow_Valley_Peak':14,}\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/gemma/6pop_pop_maf.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/randsnps/%s/%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/88)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/randDVALS/%s/%s_%s_randDVALS.txt'\n",
    "if not op.exists('/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/randDVALS/'):\n",
    "    os.makedirs('/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/randDVALS/')\n",
    "DIR = op.dirname(filE)\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "        \n",
    "''' % (pheno,pheno,str(i).zfill(3),\n",
    "       pheno,pheno,str(i).zfill(3))\n",
    "        filE = '/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/randpyfiles/get_rand_dvals_%s_%s.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(3)\n",
    "                                                                                                     )\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the python scripts\n",
    "DIR = '/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/randpyfiles/'\n",
    "pyfiles = [op.join(DIR,f) for f in ls(DIR)]\n",
    "len(pyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spread out the jobs\n",
    "count = 0\n",
    "fcount = 0\n",
    "tcount = 0\n",
    "for f in sorted(pyfiles):\n",
    "    if count == 0:\n",
    "        text = 'python %s\\n' % f\n",
    "    else:\n",
    "        newtext = 'python %s\\n' % f\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    tcount += 1\n",
    "    if (count == 125) or (tcount == 5000):\n",
    "        count = 0\n",
    "        fcount += 1\n",
    "        filE = '/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/runpyfiles/%s_cmds.txt' % str(fcount).zfill(3)\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make qsub files\n",
    "for i in range(40):\n",
    "    j = i+1\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N pyfile%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cat %s_cmds.txt | parallel --progress --\n",
    "\n",
    "''' % (str(j).zfill(3),\n",
    "       str(j).zfill(3)\n",
    "      )\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/runpyfiles/%s_run.sh' % str(j).zfill(3)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#qsub the qsubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/randDVALS/'\n",
    "ds = [op.join(DIR,d) for d in ls(DIR)]\n",
    "count = 0\n",
    "for DIR in ds:\n",
    "    count = count + len(ls(DIR))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get observed dvals and median vals from random snps\n",
    "dvals = OrderedDict()\n",
    "medvals = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariances_top_alphas/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "\n",
    "    DIR = '/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/randDVALS/%s/' % pheno\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "\n",
    "    fcount = 0\n",
    "    medvals[pheno] = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        rvals = [] #random dij values\n",
    "        for lst in df.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    rvals.append(x)\n",
    "\n",
    "\n",
    "        medvals[pheno].append(np.median([abs(val) for val in rvals]))\n",
    "\n",
    "        fcount += 1\n",
    "        if fcount % 100 == 0:\n",
    "            print fcount\n",
    "\n",
    "    filE = '/home/lindb/wbp/gemma/covariances_top_alphas/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    if not op.exists(op.dirname(filE)):\n",
    "        os.makedirs(op.dirname(filE))\n",
    "    medvaldf = pd.DataFrame(medvals[pheno])\n",
    "    medvaldf.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno,'has',len(alphasnps999[pheno]),'SNPs in intersection of top PIPs and top effects (alpha+beta)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# intersections of SNPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                         Intersections with top PIPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### intersection of top PIPs and OutFLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in toppips:\n",
    "    print pheno,len(toppips[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outflank = '/home/lindb/wbp/OutFLANK/OutFLANK_snps.txt'\n",
    "outdf = pd.read_csv(outflank,header=0,sep='\\t')\n",
    "outflanksnps = outdf['x'].tolist()\n",
    "len(outflanksnps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#intersections\n",
    "for pheno in toppips:\n",
    "    snps = set(toppips[pheno]).intersection(set(outflanksnps))\n",
    "    if len(snps)>0:\n",
    "        print pheno,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### intersection of top PIPs and bayenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get bayenv2 snps\n",
    "DIR = '/home/lindb/wbp/bayenv2/results/sigsnps'\n",
    "bayfs = [op.join(DIR,f) for f in ls(DIR)]\n",
    "baydict = {}\n",
    "for f in bayfs:\n",
    "    env = op.basename(f).split(\"_\")[0]\n",
    "    df = pd.read_csv(f,header=0,sep='\\t')\n",
    "    baydict[env] = df[env].tolist()\n",
    "    print env,len(baydict[env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#intersection\n",
    "for pheno in toppips:\n",
    "    for evn in baydict:\n",
    "        snps = set(toppips[pheno]).intersection(set(baydict[env]))\n",
    "        if len(snps) > 0:\n",
    "            print pheno,env,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### intersection of top PIPs across phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,phenoi in enumerate(toppips):\n",
    "    for j,phenoj in enumerate(toppips):\n",
    "        if i>j:\n",
    "            snps = set(toppips[phenoi]).intersection(toppips[phenoj])\n",
    "            if len(snps) > 0:\n",
    "                print phenoi,phenoj,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                  Intersections with top 0.998 PIPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in pip998:\n",
    "    print pheno,len(pip998[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#large effect snps from LMM (-ln(p_wald) >= 10)\n",
    "len(outloci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with OutFLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in pip998:\n",
    "    snps = set(pip998[pheno]).intersection(set(outflanksnps))\n",
    "    if len(snps) > 0:\n",
    "        print pheno,len(snps)\n",
    "        for snp in snps:\n",
    "            if snp in outloci:\n",
    "                print snp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with bayenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in sorted(pip998):\n",
    "    for env in sorted(baydict):\n",
    "        snps = set(pip998[pheno]).intersection(baydict[env])\n",
    "        if len(snps) > 0:\n",
    "            print pheno,env,len(snps)\n",
    "#             for snp in snps:\n",
    "#                 if snp in outloci:\n",
    "#                     print snp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### across phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,phenoi in enumerate(sorted(pip998)):\n",
    "    for j,phenoj in enumerate(sorted(pip998)):\n",
    "        if i > j:\n",
    "            snps = set(pip998[phenoi]).intersection(set(pip998[phenoj]))\n",
    "            if len(snps) > 0 :\n",
    "                print phenoi,phenoj,len(snps)\n",
    "                for snp in snps:\n",
    "                    if snp in outloci:\n",
    "                        print snp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bayenv with outloci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in baydict:\n",
    "    snps = set(baydict[env]).intersection(set(outloci))\n",
    "    if len(snps) > 0:\n",
    "        print env,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### outflank with outloci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(outflanksnps).intersection(set(outloci)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bayenv2 with bayenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,envi in enumerate(sorted(baydict)):\n",
    "    for j,envj in enumerate(sorted(baydict)):\n",
    "        if i > j:\n",
    "            snps = set(baydict[envi]).intersection(set(baydict[envj]))\n",
    "            if len(snps) > 0:\n",
    "                print envi,envj,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                  Intersections with top 0.999 total effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.999 total effect snps\n",
    "for pheno in snpdict999.keys():\n",
    "    print pheno,len(snpdict999[pheno])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection with OutFLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#intersection\n",
    "for pheno in snpdict999:\n",
    "    snps = set(snpdict999[pheno]).intersection(set(outflanksnps))\n",
    "    if len(snps)>0:\n",
    "        print pheno,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inersection with bayenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pheno in snpdict999.keys():\n",
    "    for env in baydict.keys():\n",
    "        snps = set(snpdict999[pheno]).intersection(set(baydict[env]))\n",
    "        if len(snps)>0:\n",
    "            print pheno,env,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection across phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,phenoi in enumerate(snpdict999.keys()):\n",
    "    for j,phenoj in enumerate(snpdict999.keys()):\n",
    "        if i>j:\n",
    "            snps = set(snpdict999[phenoi]).intersection(set(snpdict999[phenoj]))\n",
    "            if len(snps) > 0:\n",
    "                print phenoi,phenoj,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                  Intersections with top 0.995 total effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in snpdict995.keys():\n",
    "    print pheno,len(snpdict995[pheno])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection with OutFLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pheno in snpdict995.keys():\n",
    "    snps = set(snpdict995[pheno]).intersection(set(outflanksnps))\n",
    "    if len(snps) > 0:\n",
    "        print pheno,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection with bayenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in snpdict995.keys():\n",
    "    for env in baydict.keys():\n",
    "        snps = set(snpdict995[pheno]).intersection(set(baydict[env]))\n",
    "        if len(snps) > 0:\n",
    "            print pheno,env,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection across phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,phenoi in enumerate(snpdict995.keys()):\n",
    "    for j,phenoj in enumerate(snpdict995.keys()):\n",
    "        if i>j:\n",
    "            snps = set(snpdict995[phenoi]).intersection(set(snpdict995[phenoj]))\n",
    "            if len(snps) > 0:\n",
    "                print phenoi,phenoj,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                  Intersections with top 0.990 total effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#snps\n",
    "for pheno in effect_snps:\n",
    "    print pheno,len(effect_snps[pheno])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection with OutFLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#intersection\n",
    "for pheno in effect_snps.keys():\n",
    "    snps = set(effect_snps[pheno]).intersection(set(outflanksnps))\n",
    "    if len(snps) > 0:\n",
    "        print pheno,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection with bayenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in effect_snps.keys():\n",
    "    for env in baydict.keys():\n",
    "        snps = set(effect_snps[pheno]).intersection(set(baydict[env]))\n",
    "        if len(snps) > 0:\n",
    "            print pheno,env,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection across phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,phenoi in enumerate(effect_snps.keys()):\n",
    "    for j,phenoj in enumerate(effect_snps.keys()):\n",
    "        if i>j:\n",
    "            snps = set(effect_snps[phenoi]).intersection(set(effect_snps[phenoj]))\n",
    "            if len(snps) > 0:\n",
    "                print phenoi,phenoj,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                  Intersections with top 0.999 and 0.995 alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in sorted(alphasnps):\n",
    "    for q in alphasnps[pheno]:\n",
    "        print pheno,q,len(alphasnps[pheno][q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#snps\n",
    "for pheno in alphasnps999:\n",
    "    print pheno,len(alphasnps999[pheno])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection with OutFLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.999\n",
    "for pheno in alphasnps:\n",
    "    snps = set(alphasnps[pheno][q]).intersection(outflanksnps)\n",
    "    if len(snps) > 0:\n",
    "        print pheno,len(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.995\n",
    "for pheno in alphasnps:\n",
    "    snps = set(alphasnps[pheno][q]).intersection(outflanksnps)\n",
    "    if len(snps) > 0:\n",
    "        print pheno,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection with bayenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = 0.999\n",
    "for pheno in alphasnps:\n",
    "    for env in baydict:\n",
    "        snps = set(alphasnps[pheno][q]).intersection(baydict[env])\n",
    "        if len(snps) > 0:\n",
    "            print pheno,env,len(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.995\n",
    "for pheno in alphasnps:\n",
    "    for env in baydict:\n",
    "        snps = set(alphasnps[pheno][q]).intersection(baydict[env])\n",
    "        if len(snps) > 0:\n",
    "            print pheno,env,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection across phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.999\n",
    "for i,phenoi in enumerate(alphasnps):\n",
    "    for j,phenoj in enumerate(alphasnps):\n",
    "        if i>j:\n",
    "            snps = set(alphasnps[phenoi][q]).intersection(set(alphasnps[phenoj][q]))\n",
    "            if len(snps) > 0:\n",
    "                print phenoi,phenoj,len(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.995\n",
    "for i,phenoi in enumerate(alphasnps):\n",
    "    for j,phenoj in enumerate(alphasnps):\n",
    "        if i>j:\n",
    "            snps = set(alphasnps[phenoi][q]).intersection(set(alphasnps[phenoj][q]))\n",
    "            if len(snps) > 0:\n",
    "                print phenoi,phenoj,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                     Intersections with top 0.999 and 0.995 beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the betasnps\n",
    "betasnps = {}\n",
    "for pheno in combined_dfs:\n",
    "    betasnps[pheno] = {}\n",
    "    d = combined_dfs[pheno]\n",
    "    x = 'beta_hmean'\n",
    "    for q in [0.999,0.995]:\n",
    "        cutoff = d[x].quantile(q)\n",
    "        xvals = d[x][d[x]>=cutoff]\n",
    "        betasnps[pheno][q] = xvals.index\n",
    "        print pheno,q,len(xvals.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection with OutFLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in betasnps:\n",
    "    for q in betasnps[pheno]:\n",
    "        snps = set(betasnps[pheno][q]).intersection(set(outflanksnps))\n",
    "        if len(snps) > 0:\n",
    "            print pheno,q,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection with bayenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in betasnps:\n",
    "    for q in betasnps[pheno]:\n",
    "        for env in baydict:\n",
    "            snps = set(betasnps[pheno][q]).intersection(set(baydict[env]))\n",
    "            if len(snps) > 0:\n",
    "                print pheno,q,env,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection across phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(betasnps.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qi = 0.999\n",
    "for i,phenoi in enumerate(betasnps):\n",
    "    for j,phenoj in enumerate(betasnps):\n",
    "        if i > j:\n",
    "            snps = set(betasnps[phenoi][qi]).intersection(set(betasnps[phenoj][qi]))\n",
    "            if len(snps) > 0:\n",
    "                print phenoi,phenoj,qi,len(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qi = 0.995\n",
    "for i,phenoi in enumerate(betasnps):\n",
    "    for j,phenoj in enumerate(betasnps):\n",
    "        if i > j:\n",
    "            snps = set(betasnps[phenoi][qi]).intersection(set(betasnps[phenoj][qi]))\n",
    "            if len(snps) > 0:\n",
    "                print phenoi,phenoj,qi,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                Intersections with top 0.999 and 0.995 total effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fxsnps = {}\n",
    "for pheno in combined_dfs:\n",
    "    fxsnps[pheno] = {}\n",
    "    d = combined_dfs[pheno]\n",
    "    x = 'total_effect'\n",
    "    for q in [0.995,0.999]:\n",
    "        cutoff = d[x].quantile(q)\n",
    "        xvals = d[x][d[x]>=cutoff]\n",
    "        fxsnps[pheno][q] = xvals.index.tolist()\n",
    "        print pheno,q,len(xvals.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection with OutFLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.999\n",
    "for pheno in fxsnps:\n",
    "    snps = set(fxsnps[pheno][q]).intersection(set(outflanksnps))\n",
    "    if len(snps) > 0:\n",
    "        print pheno,len(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.999\n",
    "for pheno in fxsnps:\n",
    "    snps = set(fxsnps[pheno][q]).intersection(set(outflanksnps))\n",
    "    if len(snps) > 0:\n",
    "        print pheno,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection with bayenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.999\n",
    "for pheno in fxsnps:\n",
    "    for env in baydict:\n",
    "        snps = set(fxsnps[pheno][q]).intersection(set(baydict[env]))\n",
    "        if len(snps) > 0:\n",
    "            print pheno,env,len(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.995\n",
    "for pheno in fxsnps:\n",
    "    for env in baydict:\n",
    "        snps = set(fxsnps[pheno][q]).intersection(set(baydict[env]))\n",
    "        if len(snps) > 0:\n",
    "            print pheno,env,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                     Intersections among criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alpha and beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.999\n",
    "for i,phenoi in enumerate(alphasnps):\n",
    "    for j,phenoj in enumerate(betasnps):\n",
    "        snps = set(alphasnps[pheno]).intersection(betasnps[phenoj][q])\n",
    "        if len(snps) > 0:\n",
    "            print phenoi,phenoj,len(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = 0.995\n",
    "for i,phenoi in enumerate(alphasnps):\n",
    "    for j,phenoj in enumerate(betasnps):\n",
    "        snps = set(alphasnps[pheno]).intersection(betasnps[phenoj][q])\n",
    "        if len(snps) > 0:\n",
    "            print phenoi,phenoj,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alpha and pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.999\n",
    "for i,phenoi in enumerate(alphasnps):\n",
    "    for j,phenoj in enumerate(toppips):\n",
    "        snps = set(alphasnps[phenoi][q]).intersection(toppips[phenoj ])\n",
    "        if len(snps) > 0:\n",
    "            print phenoi,phenoj,len(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.995\n",
    "for i,phenoi in enumerate(alphasnps):\n",
    "    for j,phenoj in enumerate(toppips):\n",
    "        snps = set(alphasnps[phenoi][q]).intersection(toppips[phenoj ])\n",
    "        if len(snps) > 0:\n",
    "            print phenoi,phenoj,len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### beta and pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.999\n",
    "for i,phenoi in enumerate(betasnps):\n",
    "    for j,phenoj in enumerate(toppips):\n",
    "        snps = set(betasnps[phenoi][q]).intersection(set(toppips[phenoj]))\n",
    "        if len(snps) > 0:\n",
    "            print phenoi,phenoj, len(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = 0.995\n",
    "for i,phenoi in enumerate(betasnps):\n",
    "    for j,phenoj in enumerate(toppips):\n",
    "        snps = set(betasnps[phenoi][q]).intersection(set(toppips[phenoj]))\n",
    "        if len(snps) > 0:\n",
    "            print phenoi,phenoj, len(snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top fx and alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = 0.999\n",
    "for i,phenoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in snpdict999:\n",
    "    print pheno,len(snpdict999[pheno])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection across bayenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uniqs = {}\n",
    "for i,envi in enumerate(sorted(baydict)):\n",
    "    for j,envj in enumerate(sorted(baydict)):\n",
    "        if i>j:\n",
    "            combo = \"%s %s\" % (envi,envj)\n",
    "            uniqs[combo] = []\n",
    "            [uniqs[combo].append(snp) for snp in baydict[envi] if not snp in uniqs[combo]]\n",
    "            [uniqs[combo].append(snp) for snp in baydict[envj] if not snp in uniqs[combo]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bayint = {}\n",
    "bayintl = {}\n",
    "for i,envi in enumerate(sorted(baydict)):\n",
    "    for j,envj in enumerate(sorted(baydict)):\n",
    "        if i >j:\n",
    "            snps = set(baydict[envi]).intersection(set(baydict[envj]))\n",
    "            if len(snps) > 0:\n",
    "                bayint[\"%s %s\" % (envi,envj)] = snps\n",
    "                bayintl[\"%s %s\" % (envi,envj)] = len(snps)\n",
    "for combo,snps in sorted(bayintl.items(),key=lambda x: x[1],reverse=True):\n",
    "    print combo,bayintl[combo],len(uniqs[combo]),round((bayintl[combo]/len(uniqs[combo]))*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection between OutFLANK and bayenv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in sorted(baydict):\n",
    "    snps = set(baydict[env]).intersection(set(outflanksnps))\n",
    "    if len(snps) > 0:\n",
    "        print env,len(snps)\n",
    "        for snp in snps:\n",
    "            if snp in outloci:\n",
    "                print snp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outbay = []\n",
    "outbayo = {}\n",
    "for env in baydict:\n",
    "    outbayo[env] = set(baydict[env]).intersection(set(outflanksnps))\n",
    "    if len(outbayo[env]) > 0:\n",
    "        print env,len(outbayo[env])\n",
    "        [outbay.append(snp) for snp in outbayo[env] if snp not in outbay]\n",
    "len(outbay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "osnps = []\n",
    "for i,envi in enumerate(sorted(outbayo)):\n",
    "    for j,envj in enumerate(sorted(outbayo)):\n",
    "        if i > j:\n",
    "            snps = set(outbayo[envi]).intersection(outbayo[envj])\n",
    "            if len(snps) > 0:\n",
    "                print envi,envj,len(snps)\n",
    "                [osnps.append(snp) for snp in snps if snp not in osnps]\n",
    "len(osnps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covaraince of intersection of top PIPs and top model averaged effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topfx = {}\n",
    "for pheno in combined_dfs:\n",
    "    topfx[pheno] = {}\n",
    "    d = combined_dfs[pheno]\n",
    "    x = 'model_averaged'\n",
    "    for q in [0.999,0.995]:\n",
    "        cutoff = d[x].quantile(q)\n",
    "        xvals = d[x][d[x] >= cutoff]\n",
    "        topfx[pheno][q] = xvals.index\n",
    "        print pheno,q,len(xvals.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topint = {}\n",
    "for pheno in topfx:\n",
    "    topint[pheno] = {}\n",
    "    for q in topfx[pheno]:\n",
    "        snps = set(tops[pheno][q]).intersection(set(topfx[pheno][q]))\n",
    "        topint[pheno][q] = snps\n",
    "        print pheno,q,len(snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in snpdict995:\n",
    "    print pheno,len(snpdict995[pheno]),len(topint[pheno][0.995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in topint:\n",
    "    for q in [0.999]:\n",
    "        snps = set(topint[pheno][q]).intersection(set(snpdict999[pheno]))\n",
    "        print pheno,len(snps),len(topint[pheno][q]),len(snpdict999[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in topint:\n",
    "    for q in [0.995]:\n",
    "        snps = set(topint[pheno][q]).intersection(set(snpdict995[pheno]))\n",
    "        print pheno,len(snps),len(topint[pheno][q]),len(snpdict995[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mafs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mafDict['NODE_1000013_length_91_cov_1.802198_37']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariance_int_pips_modfx_999')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "globs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glob['NODE_1000013_length_91_cov_1.802198_37']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popsizeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise to get Dij\n",
    "for pheno in topint.keys():\n",
    "    print pheno\n",
    "    outliersnps = topint[pheno][0.999]\n",
    "\n",
    "    dijDict = OrderedDict() \n",
    "    dijDict[pheno] = OrderedDict() \n",
    "    for i,locusi in enumerate(outliersnps):\n",
    "        dijDict[pheno][locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(outliersnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popsizeDict.keys():\n",
    "                    qik = mafDict[locusi][pop] #get pop maf\n",
    "                    qjk = mafDict[locusj][pop] #get pop maf\n",
    "                    nk = popsizeDict[pop]\n",
    "\n",
    "                    sums += (float(nk)/float(88))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[pheno][locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[pheno][locusi][locusj] = np.nan\n",
    "    #write out the file\n",
    "    for pheno in dijDict.keys():\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/dvals/%s_dvals.txt' % pheno\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            key0 = dijDict[pheno].keys()[0]\n",
    "            line = '\\t'.join(dijDict[pheno][key0].keys()) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "            for locusi in dijDict[pheno].keys():\n",
    "                line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[pheno][locusi].values()]) + str('\\n')\n",
    "                o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/dvals'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "dijDict = OrderedDict()\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    dijDict[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "dijDict['n15popx'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in dijDict.keys():\n",
    "    print pheno,len(dijDict[pheno].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataframe with the outlier loci and their bins\n",
    "outlierdata = OrderedDict()\n",
    "for pheno in dijDict.keys():\n",
    "    outliersnps = topint[pheno][0.999]\n",
    "\n",
    "    outlierdata[pheno] = pd.DataFrame(H[H.index.isin(outliersnps)])\n",
    "    print pheno,len(outlierdata[pheno]),len(topint[pheno][0.999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/grid_imp_hexp_hist_by_pheno_int_pips_modfx_999.pdf') as pdf:\n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "\n",
    "    a1 = plt.subplot(231)\n",
    "    a1.hist(outlierdata['bfpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a1.set_title('A',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a1.spines['right'].set_visible(False)\n",
    "    a1.spines['top'].set_visible(False)\n",
    "    a1.yaxis.set_ticks_position('left')\n",
    "    a1.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a2 = plt.subplot(232)\n",
    "    a2.hist(outlierdata['c13popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a2.set_title('B',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a2.spines['right'].set_visible(False)\n",
    "    a2.spines['top'].set_visible(False)\n",
    "    a2.yaxis.set_ticks_position('left')\n",
    "    a2.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a3 = plt.subplot(233)\n",
    "    a3.hist(outlierdata['htpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a3.set_title('C',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a3.spines['right'].set_visible(False)\n",
    "    a3.spines['top'].set_visible(False)\n",
    "    a3.yaxis.set_ticks_position('left')\n",
    "    a3.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a4 = plt.subplot(234)\n",
    "    a4.hist(outlierdata['n15popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a4.set_title('D',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a4.spines['right'].set_visible(False)\n",
    "    a4.spines['top'].set_visible(False)\n",
    "    a4.yaxis.set_ticks_position('left')\n",
    "    a4.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a5 = plt.subplot(235)\n",
    "    a5.hist(outlierdata['rspopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a5.set_title('E',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a5.spines['right'].set_visible(False)\n",
    "    a5.spines['top'].set_visible(False)\n",
    "    a5.yaxis.set_ticks_position('left')\n",
    "    a5.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a6.spines['right'].set_visible(False)\n",
    "    a6.spines['left'].set_visible(False)\n",
    "    a6.spines['top'].set_visible(False)\n",
    "    a6.spines['bottom'].set_visible(False)\n",
    "    a6.axes.get_yaxis().set_visible(False)\n",
    "    a6.axes.get_xaxis().set_ticks([])\n",
    "    a6.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    fig.set_size_inches(13,10)\n",
    "    pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get sets of SNPs to make random draws\n",
    "nonsigsnps = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    nonsigs = list(set(H.index.tolist()) - set(outlierdata[pheno].index.tolist()))\n",
    "    nonsigsnps[pheno] = pd.DataFrame(H[H.index.isin(nonsigs)])\n",
    "    print len(outlierdata[pheno].index.tolist())+len(nonsigsnps[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many random snps to draw from each bin?\n",
    "binCounter = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    binCounter[pheno] = Counter()\n",
    "    for row in outlierdata[pheno].index:\n",
    "        binCounter[pheno][outlierdata[pheno].loc[row,'bin']] += 1\n",
    "    for b in binCounter[pheno].keys():\n",
    "        print pheno,b,binCounter[pheno][b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices')\n",
    "os.makedirs('/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/randsnps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each pheno make 1000 dataframes with a set of snps == len(outliersnps[pheno])\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    for i in range(1000):  \n",
    "        snps = []        \n",
    "        for binn in binCounter[pheno].keys():\n",
    "            data = nonsigsnps[pheno][nonsigsnps[pheno]['bin'] == binn]\n",
    "                \n",
    "            [snps.append(snp) for snp in random.sample(data.index,binCounter[pheno][binn])]\n",
    "        \n",
    "        assert len(np.unique(snps).tolist()) == sum(binCounter[pheno].values())\n",
    "\n",
    "        DIR = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/randsnps/%s' % pheno\n",
    "        if not op.exists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = op.join(DIR,\"%s_%s_randsnps.txt\" % (pheno,str(i).zfill(3)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quicker to do it this way\n",
    "#get dvals for the sets of random snps for each pheno\n",
    "for pheno in outlierdata.keys():\n",
    "    for i in range(1000):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "#population sizes (only for individuals in pops with phenotypic measurements)\n",
    "popDict = {'Dicks_Pass':15,\n",
    "'Freel_Peak':19,\n",
    "'Little_Round_Top':14,\n",
    "'Mt_Rose_Ophir':11,\n",
    "'Rifle_Peak':15,\n",
    "'Snow_Valley_Peak':14,}\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/gemma/6pop_pop_maf.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/randsnps/%s/%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/88)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/randDVALS/%s/%s_%s_randDVALS.txt'\n",
    "if not op.exists('/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/randDVALS/'):\n",
    "    os.makedirs('/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/randDVALS/')\n",
    "DIR = op.dirname(filE)\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "\n",
    "\n",
    "''' % (pheno,pheno,str(i).zfill(3),\n",
    "       pheno,pheno,str(i).zfill(3))\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/randpyfiles/get_rand_dvals_%s_%s.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(3)\n",
    "                                                                                                     )\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/randpyfiles/'\n",
    "pyfiles = [op.join(DIR,f) for f in ls(DIR)]\n",
    "len(pyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spread out the jobs\n",
    "count = 0\n",
    "fcount = 0\n",
    "tcount = 0\n",
    "for f in sorted(pyfiles):\n",
    "    if count == 0:\n",
    "        text = 'python %s\\n' % f\n",
    "    else:\n",
    "        newtext = 'python %s\\n' % f\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    tcount += 1\n",
    "    if (count == 100) or (tcount == 5000):\n",
    "        count = 0\n",
    "        fcount += 1\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/runpyfiles/%s_cmds.txt' % str(fcount).zfill(3)\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make qsub files\n",
    "for i in range(50):\n",
    "    j = i+1\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N pyfile%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cat %s_cmds.txt | parallel --progress --\n",
    "\n",
    "''' % (str(j).zfill(3),\n",
    "       str(j).zfill(3)\n",
    "      )\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/runpyfiles/%s_run.sh' % str(j).zfill(3)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/randDVALS/'\n",
    "ds = [op.join(DIR,d) for d in ls(DIR)]\n",
    "count = 0\n",
    "for DIR in ds:\n",
    "    count = count + len(ls(DIR))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "dvals = OrderedDict()\n",
    "medvals = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "\n",
    "    DIR = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/randDVALS/%s/' % pheno\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "\n",
    "    fcount = 0\n",
    "    medvals[pheno] = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        rvals = [] #random dij values\n",
    "        for lst in df.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    rvals.append(x)\n",
    "\n",
    "        medvals[pheno].append(np.median([abs(val) for val in rvals]))\n",
    "\n",
    "        fcount += 1\n",
    "        if fcount % 100 == 0:\n",
    "            print fcount\n",
    "\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    if not op.exists(op.dirname(filE)):\n",
    "        os.makedirs(op.dirname(filE))\n",
    "    medvaldf = pd.DataFrame(medvals[pheno])\n",
    "    medvaldf.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outlierdata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#in case I restart my notebook\n",
    "medvals = {}\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_999/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    df = pd.read_csv(filE,header=None,sep='\\t')\n",
    "    medvals[pheno] = df[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno,'has',len(topint[pheno][0.999]),'SNPs in intersection of top PIPs and top effects (alpha+beta*gamma)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "        if i+1 == len(dvals[pheno]):\n",
    "            print '%s bmax maxed out' % pheno\n",
    "            bmax[pheno] = (i+1,(i+1)/len(dvals[pheno]))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# covariance among top 99.5th pips and 99.5th model effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in topint:\n",
    "    for q in [0.995]:\n",
    "        print pheno,len(topint[pheno][q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariance_int_pips_modfx_995')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in effect_snps.keys():\n",
    "    print pheno\n",
    "    outliersnps = topint[pheno][0.995]\n",
    "\n",
    "    dijDict = OrderedDict() \n",
    "    dijDict[pheno] = OrderedDict() \n",
    "    for i,locusi in enumerate(outliersnps):\n",
    "        dijDict[pheno][locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(outliersnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popsizeDict.keys():\n",
    "                    qik = mafDict[locusi][pop] #get pop maf\n",
    "                    qjk = mafDict[locusj][pop] #get pop maf\n",
    "                    nk = popsizeDict[pop]\n",
    "\n",
    "                    sums += (float(nk)/float(88))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[pheno][locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[pheno][locusi][locusj] = np.nan\n",
    "    #write out the file\n",
    "    for pheno in dijDict.keys():\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/dvals/%s_dvals.txt' % pheno\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            key0 = dijDict[pheno].keys()[0]\n",
    "            line = '\\t'.join(dijDict[pheno][key0].keys()) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "            for locusi in dijDict[pheno].keys():\n",
    "                line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[pheno][locusi].values()]) + str('\\n')\n",
    "                o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/dvals'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "dijDict = OrderedDict()\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    dijDict[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "dijDict['n15popx'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dataframe with the outlier loci and their bins\n",
    "outlierdata = OrderedDict()\n",
    "for pheno in dijDict.keys():\n",
    "    outliersnps = topint[pheno][0.995]\n",
    "\n",
    "    outlierdata[pheno] = pd.DataFrame(H[H.index.isin(outliersnps)])\n",
    "    print pheno,len(outlierdata[pheno]),len(topint[pheno][0.995])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages('/home/lindb/wbp/figures/grid_imp_hexp_hist_by_pheno_modfx_pips_995.pdf') as pdf:\n",
    "    plt.close('all')\n",
    "    fig, ((a1,a2,a3),(a4,a5,a6)) = plt.subplots(2, 3, figsize=(5,5),dpi=400)\n",
    "\n",
    "    a1 = plt.subplot(231)\n",
    "    a1.hist(outlierdata['bfpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a1.set_title('A',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a1.spines['right'].set_visible(False)\n",
    "    a1.spines['top'].set_visible(False)\n",
    "    a1.yaxis.set_ticks_position('left')\n",
    "    a1.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a2 = plt.subplot(232)\n",
    "    a2.hist(outlierdata['c13popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a2.set_title('B',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a2.spines['right'].set_visible(False)\n",
    "    a2.spines['top'].set_visible(False)\n",
    "    a2.yaxis.set_ticks_position('left')\n",
    "    a2.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a3 = plt.subplot(233)\n",
    "    a3.hist(outlierdata['htpopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a3.set_title('C',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a3.spines['right'].set_visible(False)\n",
    "    a3.spines['top'].set_visible(False)\n",
    "    a3.yaxis.set_ticks_position('left')\n",
    "    a3.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a4 = plt.subplot(234)\n",
    "    a4.hist(outlierdata['n15popx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a4.set_title('D',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a4.spines['right'].set_visible(False)\n",
    "    a4.spines['top'].set_visible(False)\n",
    "    a4.yaxis.set_ticks_position('left')\n",
    "    a4.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a5 = plt.subplot(235)\n",
    "    a5.hist(outlierdata['rspopx']['H_exp'].tolist(),bins = [binn for binn in np.arange(0,0.51,0.01)])[2]\n",
    "    a5.set_title('E',y=.9,loc='left',fontsize=20,fontweight='bold')\n",
    "    a5.spines['right'].set_visible(False)\n",
    "    a5.spines['top'].set_visible(False)\n",
    "    a5.yaxis.set_ticks_position('left')\n",
    "    a5.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    a6.spines['right'].set_visible(False)\n",
    "    a6.spines['left'].set_visible(False)\n",
    "    a6.spines['top'].set_visible(False)\n",
    "    a6.spines['bottom'].set_visible(False)\n",
    "    a6.axes.get_yaxis().set_visible(False)\n",
    "    a6.axes.get_xaxis().set_ticks([])\n",
    "    a6.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    fig.set_size_inches(13,10)\n",
    "    pdf.savefig(fig,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get sets of SNPs to make random draws\n",
    "nonsigsnps = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    nonsigs = list(set(H.index.tolist()) - set(outlierdata[pheno].index.tolist()))\n",
    "    nonsigsnps[pheno] = pd.DataFrame(H[H.index.isin(nonsigs)])\n",
    "    print len(outlierdata[pheno].index.tolist())+len(nonsigsnps[pheno].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many random snps to draw from each bin?\n",
    "binCounter = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    binCounter[pheno] = Counter()\n",
    "    for row in outlierdata[pheno].index:\n",
    "        binCounter[pheno][outlierdata[pheno].loc[row,'bin']] += 1\n",
    "    for b in binCounter[pheno].keys():\n",
    "        print pheno,b,binCounter[pheno][b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices')\n",
    "os.makedirs('/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/randsnps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each pheno make 1000 dataframes with a set of snps == len(outliersnps[pheno])\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    for i in range(1000):  \n",
    "        snps = []        \n",
    "        for binn in binCounter[pheno].keys():\n",
    "            data = nonsigsnps[pheno][nonsigsnps[pheno]['bin'] == binn]\n",
    "                \n",
    "            [snps.append(snp) for snp in random.sample(data.index,binCounter[pheno][binn])]\n",
    "        \n",
    "        assert len(np.unique(snps).tolist()) == sum(binCounter[pheno].values())\n",
    "\n",
    "        DIR = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/randsnps/%s' % pheno\n",
    "        if not op.exists(DIR):\n",
    "            os.makedirs(DIR)\n",
    "        filE = op.join(DIR,\"%s_%s_randsnps.txt\" % (pheno,str(i).zfill(3)))\n",
    "        df = pd.DataFrame(snps)\n",
    "        df.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quicker to do it this way\n",
    "#get dvals for the sets of random snps for each pheno\n",
    "for pheno in outlierdata.keys():\n",
    "    for i in range(1000):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "#population sizes (only for individuals in pops with phenotypic measurements)\n",
    "popDict = {'Dicks_Pass':15,\n",
    "'Freel_Peak':19,\n",
    "'Little_Round_Top':14,\n",
    "'Mt_Rose_Ophir':11,\n",
    "'Rifle_Peak':15,\n",
    "'Snow_Valley_Peak':14,}\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/gemma/6pop_pop_maf.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/randsnps/%s/%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/88)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/randDVALS/%s/%s_%s_randDVALS.txt'\n",
    "if not op.exists('/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/randDVALS/'):\n",
    "    os.makedirs('/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/randDVALS/')\n",
    "DIR = op.dirname(filE)\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "\n",
    "\n",
    "''' % (pheno,pheno,str(i).zfill(3),\n",
    "       pheno,pheno,str(i).zfill(3))\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/randpyfiles/get_rand_dvals_%s_%s.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(3)\n",
    "                                                                                                     )\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/randpyfiles/'\n",
    "pyfiles = [op.join(DIR,f) for f in ls(DIR)]\n",
    "len(pyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spread out the jobs\n",
    "count = 0\n",
    "fcount = 0\n",
    "tcount = 0\n",
    "for f in sorted(pyfiles):\n",
    "    if count == 0:\n",
    "        text = 'python %s\\n' % f\n",
    "    else:\n",
    "        newtext = 'python %s\\n' % f\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    tcount += 1\n",
    "    if (count == 100) or (tcount == 5000):\n",
    "        count = 0\n",
    "        fcount += 1\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/runpyfiles/%s_cmds.txt' % str(fcount).zfill(3)\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make qsub files\n",
    "for i in range(50):\n",
    "    j = i+1\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N pyfile%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cat %s_cmds.txt | parallel --progress --\n",
    "\n",
    "''' % (str(j).zfill(3),\n",
    "       str(j).zfill(3)\n",
    "      )\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/runpyfiles/%s_run.sh' % str(j).zfill(3)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/randDVALS/'\n",
    "ds = [op.join(DIR,d) for d in ls(DIR)]\n",
    "count = 0\n",
    "for DIR in ds:\n",
    "    count = count + len(ls(DIR))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "dvals = OrderedDict()\n",
    "medvals = OrderedDict()\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "\n",
    "    DIR = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/randDVALS/%s/' % pheno\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "\n",
    "    fcount = 0\n",
    "    medvals[pheno] = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        rvals = [] #random dij values\n",
    "        for lst in df.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    rvals.append(x)\n",
    "\n",
    "        medvals[pheno].append(np.median([abs(val) for val in rvals]))\n",
    "\n",
    "        fcount += 1\n",
    "        if fcount % 100 == 0:\n",
    "            print fcount\n",
    "\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_int_pips_modfx_995/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    if not op.exists(op.dirname(filE)):\n",
    "        os.makedirs(op.dirname(filE))\n",
    "    medvaldf = pd.DataFrame(medvals[pheno])\n",
    "    medvaldf.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in outlierdata.keys():\n",
    "    print pheno,'has',len(topint[pheno][0.995]),'SNPs in intersection of top PIPs and top effects (alpha+beta)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# covariance among top 998 pips and mod fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in phenos:\n",
    "    d = combined_dfs[pheno]\n",
    "    cut = d[y].quantile(q)\n",
    "    xvals = d[y][d[y]>=cut]\n",
    "    modfx[pheno] = xvals.index\n",
    "    print pheno,len(modfx[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do pairwise to get Dij\n",
    "def get_dijdict(tokens):\n",
    "    from collections import OrderedDict\n",
    "    from os import path as op\n",
    "    from os import listdir as ls\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    pheno,outliersnps,glob,mafDict,popDict = tokens\n",
    "    dijDict = OrderedDict() \n",
    "    dijDict[pheno] = OrderedDict() \n",
    "    for i,locusi in enumerate(outliersnps):\n",
    "        dijDict[pheno][locusi] = OrderedDict()\n",
    "        qi = glob[locusi] #global maf\n",
    "\n",
    "        for j,locusj in enumerate(outliersnps):\n",
    "            if i > j: #i=row, j=col : lower triangle \n",
    "                qj = glob[locusj] #global maf\n",
    "\n",
    "                sums = 0\n",
    "                for pop in popDict.keys():\n",
    "                    qik = mafDict[locusi][pop] #get pop maf\n",
    "                    qjk = mafDict[locusj][pop] #get pop maf\n",
    "                    nk = popDict[pop]\n",
    "\n",
    "                    sums += (float(nk)/float(88))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                dijDict[pheno][locusi][locusj] = sums\n",
    "            else:\n",
    "                dijDict[pheno][locusi][locusj] = np.nan\n",
    "    #write out the file\n",
    "    for pheno in dijDict.keys():\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_pip998_by_modfx/dvals/%s_dvals.txt' % pheno\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            key0 = dijDict[pheno].keys()[0]\n",
    "            line = '\\t'.join(dijDict[pheno][key0].keys()) + str('\\n')\n",
    "            o.write(\"%s\" % line)\n",
    "            for locusi in dijDict[pheno].keys():\n",
    "                line = str(locusi)+'\\t'+'\\t'.join([str(x) for x in dijDict[pheno][locusi].values()]) + str('\\n')\n",
    "                o.write(\"%s\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jobs = []\n",
    "for pheno in phenos:\n",
    "    print pheno\n",
    "    jobs.append(lview.apply_async(get_dijdict,[pheno,modfx[pheno],glob,mafDict,popsizeDict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in jobs:\n",
    "    if j.ready():\n",
    "        print j.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariance_pip998_by_modfx/dvals'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "dijDict = OrderedDict()\n",
    "for f in sorted(files):\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    dijDict[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "dijDict[pheno].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quicker to do it this way\n",
    "#get dvals for the sets of random snps for each pheno\n",
    "for pheno in phenos:\n",
    "    for i in range(1000):\n",
    "        text = '''from __future__ import division\n",
    "import os\n",
    "from collections import OrderedDict,Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from os import path as op\n",
    "from os import listdir as ls\n",
    "\n",
    "\n",
    "#get a dict with num indiv in pop\n",
    "#population sizes (only for individuals in pops with phenotypic measurements)\n",
    "popDict = {'Dicks_Pass':15,\n",
    "'Freel_Peak':19,\n",
    "'Little_Round_Top':14,\n",
    "'Mt_Rose_Ophir':11,\n",
    "'Rifle_Peak':15,\n",
    "'Snow_Valley_Peak':14,}\n",
    "\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/6pop_glob_mafs.txt'\n",
    "globs = pd.read_csv(filE,header=0,sep='\\\\t')\n",
    "glob = OrderedDict()\n",
    "for row in globs.index:\n",
    "    snp = globs.loc[row,'locus']\n",
    "    maf = globs.loc[row,'maf']\n",
    "    glob[snp] = maf\n",
    "\n",
    "impMAF = pd.read_csv('/home/lindb/wbp/gemma/6pop_pop_maf.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "\n",
    "filE= '/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/randsnps/%s/%s_%s_randsnps.txt' \n",
    "df = pd.read_csv(filE,header=None,sep=\"\\\\t\")\n",
    "randomsnps = df[0].tolist()\n",
    "\n",
    "dijDict = OrderedDict() \n",
    "for i,locusi in enumerate(randomsnps):\n",
    "    dijDict[locusi] = OrderedDict()\n",
    "    qi = glob[locusi] #global maf\n",
    "\n",
    "    for j,locusj in enumerate(randomsnps):\n",
    "        if i > j: #i=row, j=col : lower triangle \n",
    "            qj = glob[locusj] #global maf\n",
    "\n",
    "            sums = 0\n",
    "            for pop in impMAF.columns:\n",
    "                qik = impMAF.loc[locusi,pop] #get pop maf\n",
    "                qjk = impMAF.loc[locusj,pop] #get pop maf\n",
    "                nk = popDict[pop]\n",
    "\n",
    "                sums += (nk/88)*((qik*qjk)-(qi*qj))\n",
    "\n",
    "            dijDict[locusi][locusj] = sums\n",
    "        else:\n",
    "            dijDict[locusi][locusj] = np.nan\n",
    "\n",
    "filE = '/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/randDVALS/%s/%s_%s_randDVALS.txt'\n",
    "if not op.exists('/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/randDVALS/'):\n",
    "    os.makedirs('/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/randDVALS/')\n",
    "DIR = op.dirname(filE)\n",
    "if not op.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "\n",
    "with open(filE,'w') as o:\n",
    "    key0 = dijDict.keys()[0]\n",
    "    line = '\\\\t'.join(dijDict[key0].keys()) + str('\\\\n')\n",
    "    o.write(\"%%s\" %% line)\n",
    "    for locusi in dijDict.keys():\n",
    "        line = str(locusi)+'\\\\t'+'\\\\t'.join([str(x) for x in dijDict[locusi].values()]) + str('\\\\n')\n",
    "        o.write(\"%%s\" %% line)\n",
    "\n",
    "\n",
    "''' % (pheno,pheno,str(i).zfill(3),\n",
    "       pheno,pheno,str(i).zfill(3))\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/randpyfiles/get_rand_dvals_%s_%s.py' % (pheno,\n",
    "                                                                                                      str(i).zfill(3)\n",
    "                                                                                                     )\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DIR = '/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/randpyfiles/'\n",
    "pyfiles = [op.join(DIR,f) for f in ls(DIR)]\n",
    "len(pyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spread out the jobs across 25 files\n",
    "count = 0\n",
    "fcount = 0\n",
    "for f in sorted(pyfiles):\n",
    "    if count == 0:\n",
    "        text = 'python %s\\n' % f\n",
    "    else:\n",
    "        newtext = 'python %s\\n' % f\n",
    "        text = text + newtext\n",
    "    count += 1\n",
    "    if count == 200:\n",
    "        count = 0\n",
    "        fcount += 1\n",
    "        filE = '/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/runpyfiles/%s_cmds.txt' % str(fcount).zfill(3)\n",
    "        if not op.exists(op.dirname(filE)):\n",
    "            os.makedirs(op.dirname(filE))\n",
    "        with open(filE,'w') as o:\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make 125 qsub files\n",
    "for i in range(25):\n",
    "    j = i+1\n",
    "    text = '''#!/bin/bash\n",
    "#$ -N pyfile%s\n",
    "#$ -V\n",
    "#$ -j y\n",
    "#$ -cwd\n",
    "\n",
    "cat %s_cmds.txt | parallel --progress --\n",
    "\n",
    "''' % (str(j).zfill(3),\n",
    "       str(j).zfill(3)\n",
    "      )\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/runpyfiles/%s_run.sh' % str(j).zfill(3)\n",
    "    with open(filE,'w') as o:\n",
    "        o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check on progress\n",
    "DIR = '/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/randDVALS'\n",
    "ds = [op.join(DIR,d) for d in ls(DIR)]\n",
    "count = 0\n",
    "for DIR in ds:\n",
    "    count = count + len(ls(DIR))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get observed dvals\n",
    "dvals = OrderedDict()\n",
    "medvals = OrderedDict()\n",
    "for pheno in phenos:\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariance_pip998_by_modfx/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))\n",
    "\n",
    "    DIR = '/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/randDVALS/%s/' % pheno\n",
    "    files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "\n",
    "    fcount = 0\n",
    "    medvals[pheno] = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f,header=0,index_col=0,sep=\"\\t\")\n",
    "        rvals = [] #random dij values\n",
    "        for lst in df.values.tolist():\n",
    "            for x in lst:\n",
    "                if math.isnan(x) == False:\n",
    "                    rvals.append(x)\n",
    "\n",
    "        medvals[pheno].append(np.median([abs(val) for val in rvals]))\n",
    "\n",
    "        fcount += 1\n",
    "        if fcount % 100 == 0:\n",
    "            print fcount\n",
    "\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    if not op.exists(op.dirname(filE)):\n",
    "        os.makedirs(op.dirname(filE))\n",
    "    medvaldf = pd.DataFrame(medvals[pheno])\n",
    "    medvaldf.to_csv(filE,header=False,index=False,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = ls('/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/randmedvals/')\n",
    "phenos = []\n",
    "for f in files:\n",
    "    pheno = f.split(\"_\")[0]\n",
    "    print pheno\n",
    "    phenos.append(pheno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "medvals = {}\n",
    "for pheno in phenos:\n",
    "    filE = '/home/lindb/wbp/gemma/covariance_pip998_by_modfx/randmatrices/randmedvals/%s_randmedvalues.txt' % pheno\n",
    "    df = pd.read_csv(filE,header=None,sep='\\t')\n",
    "    medvals[pheno] = df[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvals = {}\n",
    "for pheno in phenos:\n",
    "    print pheno\n",
    "    DF = pd.read_csv('/home/lindb/wbp/gemma/covariance_pip998_by_modfx/dvals/%s_dvals.txt' % pheno,\n",
    "                     header=0,index_col=0,sep=\"\\t\")\n",
    "    dvals[pheno] = []\n",
    "    for i,row in enumerate(DF.index):\n",
    "        for j,col in enumerate(DF.columns):\n",
    "            if i > j:\n",
    "                dvals[pheno].append(abs(DF.loc[row,col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phenos = ls('as')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in phenos:\n",
    "    print pheno,'has',\"234\",'SNPs'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in phenos:\n",
    "    print pheno,'has',\"234\",'SNPs in intersection of top PIPs and top effects (alpha+beta)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n5th = OrderedDict()    #get 95th percentile\n",
    "gt_hth = OrderedDict()  #how much greater is the empirical median dvalue than the 100th percentile of random SNPs?\n",
    "gt_n5th = OrderedDict() #how much greater is the empirical median dvalue than the 95th percentile of random SNPs?\n",
    "bmax=OrderedDict()      #below what percentile of the observed dvals are the values less than the maximum randdvals\n",
    "bn5th = OrderedDict()   #below what percentile of the observed dvals are the values less than the 95th perc randdvals\n",
    "for pheno in phenos:\n",
    "    print pheno,'has',\"234\",'SNPs in intersection of top PIPs and top effects (alpha+beta)'\n",
    "    sorts        = sorted(medvals[pheno])\n",
    "    n5th[pheno]    = sorts[949]\n",
    "    gt_hth[pheno]  = np.median(dvals[pheno])/max(medvals[pheno])\n",
    "    print 'emp median is %sx greater than 100th percentile of random'%str(round(gt_hth[pheno],3))\n",
    "    gt_n5th[pheno] = np.median(dvals[pheno])/n5th[pheno]\n",
    "    print 'emp median is %sx greater than 95th percentile of random'%str(round(gt_n5th[pheno],3))\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < max(sorts): #the max(randDval) corresponds to the xth percentile of empirical d-values\n",
    "            bmax[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    for i,dval in enumerate(sorted(dvals[pheno])):\n",
    "        if not dval < sorts[949]:\n",
    "            bn5th[pheno]=(i+1,(i+1)/len(dvals[pheno]))\n",
    "            break\n",
    "    print 'the max random value corresponds to the %s precentile' % str(round(bmax[pheno][1],3)*100),\"(%s/%s)\" % (bmax[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'the 95th perc rand val corresponds to the %s percentile',str(round(bn5th[pheno][1],3)*100),\"(%s/%s)\" % (bn5th[pheno][0],len(dvals[pheno])),'of emp values'\n",
    "    print 'emp med dval =',\"{:.4}\".format(np.median(dvals[pheno]))\n",
    "    print 'max rand dval =',\"{:.4}\".format(max(medvals[pheno]))\n",
    "    print '95th rand dval =',\"{:.4}\".format(n5th[pheno])\n",
    "    print '\\n' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allele frequency shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.999 intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impMAF = pd.read_csv('/home/lindb/wbp/gemma/6pop_pop_maf.txt',header=0,index_col=0,sep=\"\\\\t\")\n",
    "impMAF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(impMAF.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popsizeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in snpdict999.keys():\n",
    "    print pheno,len(snpdict999[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise pops for empirical SNPs and calculate median Dij for each pop pair\n",
    "shiftDict = OrderedDict() \n",
    "for pheno in snpdict999.keys():\n",
    "    print pheno\n",
    "    kcount = 0\n",
    "    shiftDict[pheno] = OrderedDict()\n",
    "    outliersnps = snpdict999[pheno]\n",
    "    for m,popm in enumerate(impMAF.columns):\n",
    "        print popm\n",
    "        shiftDict[pheno][popm] = OrderedDict()\n",
    "        for l,popl in enumerate(impMAF.columns):\n",
    "            if m>l: #only need to do the lower triangle\n",
    "                dijlist = []\n",
    "                for i,locusi in enumerate(outliersnps):\n",
    "                    for j,locusj in enumerate(outliersnps):\n",
    "                        if i > j: #i=row, j=col : lower triangle \n",
    "                            sums =0\n",
    "                            kcount += 1\n",
    "                            for popk in [popm,popl]:\n",
    "                                qik = impMAF.loc[locusi,popk]        #get locusi maf for pop k\n",
    "                                qjk = impMAF.loc[locusj,popk]        #get locusj maf for pop k\n",
    "                                nk = popsizeDict[popk]                   #N  individuals  in pop k\n",
    "\n",
    "                                globN = 2*(popsizeDict[popm]+popsizeDict[popl]) # number of alleles across 2 pops\n",
    "\n",
    "                                #get global mafs\n",
    "                                fqim = impMAF.loc[locusi,popm]        #minor allele locusi freq  in popm\n",
    "                                nqim = round(fqim*2*popsizeDict[popm])    #minor allele locusi count in popm\n",
    "                                fqil = impMAF.loc[locusi,popl]        #minor allele locusi freq  in popl\n",
    "                                nqil = round(fqil*2*popsizeDict[popl])    #minor allele locusi count in popl\n",
    "\n",
    "                                fqjm = impMAF.loc[locusj,popm]        #minor allele locusj freq  in popm\n",
    "                                nqjm = round(fqjm*2*popsizeDict[popm])    #minor allele locusj count in popm\n",
    "                                fqjl = impMAF.loc[locusj,popl]        #minor allele locusj freq  in popl\n",
    "                                nqjl = round(fqjl*2*popsizeDict[popl])    #minor allele locusj count in popl\n",
    "\n",
    "                                qi = (nqim+nqil)/((2*popsizeDict[popm])+(2*popsizeDict[popl])) #pairwise global maf locusi\n",
    "                                qj = (nqjm+nqjl)/((2*popsizeDict[popm])+(2*popsizeDict[popl])) #pariwise global maf locusj\n",
    "\n",
    "                                sums += (nk/(popsizeDict[popm]+popsizeDict[popl]))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                            dijlist.append(sums) #each pairwise pop comparison has a matrix of Dij\n",
    "                        else:\n",
    "                            pass #no redundancies, no diagonal. \n",
    "                shiftDict[pheno][popm][popl] = np.median([abs(d) for d in dijlist])\n",
    "            else:\n",
    "                shiftDict[pheno][popm][popl] = np.nan #no redundancies,no diagonal. will be faster to reflect across diag later\n",
    "    #    if kcount > 1:\n",
    "    #        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/freqshifts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write out the dfs\n",
    "for pheno in shiftDict.keys():\n",
    "    filE = '/home/lindb/wbp/gemma/freqshifts/%s_pop_pairwise_dij_999.text' % pheno\n",
    "    with open(filE,'w') as o:\n",
    "        key0 = shiftDict[pheno].keys()[0]\n",
    "        line = '\\t'.join(shiftDict[pheno][key0].keys()) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "        for popk in shiftDict[pheno].keys():\n",
    "            text = str(popk)+'\\t'+'\\t'.join([str(d) for d in shiftDict[pheno][popk].values()])+'\\n'\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in the DFs\n",
    "shiftDF = OrderedDict()\n",
    "DIR = '/home/lindb/wbp/gemma/freqshifts/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'dij_999' in f]\n",
    "for f in files:\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    shiftDF[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "shiftDF[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make DFs symmetrical\n",
    "for pheno in shiftDF.keys():\n",
    "    for i,popi in enumerate(shiftDF[pheno].index):\n",
    "        for j,popj in enumerate(shiftDF[pheno].columns):\n",
    "            if i == j:\n",
    "                shiftDF[pheno].loc[popi,popj] =0\n",
    "            elif math.isnan(shiftDF[pheno].loc[popi,popj]) == True:\n",
    "                shiftDF[pheno].loc[popi,popj] = shiftDF[pheno].loc[popj,popi]\n",
    "shiftDF[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shift999 = OrderedDict(shiftDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get Vincenty geographic distances\n",
    "#made in 09_OutFLANK\n",
    "filE = '/home/lindb/wbp/bayenv2/distance_matrices/geographic_distances.txt'\n",
    "geodist = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "geodist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#limit to pops with phenotypes\n",
    "geodist = geodist[[pop for pop in impMAF.columns]]\n",
    "geodist = geodist[geodist.index.isin(impMAF.columns)]\n",
    "geodist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get total environmental distance\n",
    "filE = '/home/lindb/wbp/bayenv2/distance_matrices/environmental_distances.txt'\n",
    "envdist = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "envdist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#limit to pops with phenotypes\n",
    "envdist = envdist[[pop for pop in impMAF.columns]]\n",
    "envdist = envdist[envdist.index.isin(impMAF.columns)]\n",
    "envdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get environmental distances for each environment, limit to pops with phenotypes\n",
    "envdDict = OrderedDict()\n",
    "for env in sorted(baydict.keys()):\n",
    "    filE = '/home/lindb/wbp/distance_matrices/%s_dist_symm.txt' % env\n",
    "    df = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "    df = df[[pop for pop in impMAF.columns]]\n",
    "    df = df[df.index.isin(impMAF.columns)]\n",
    "    envdDict[env] = df\n",
    "envdDict[env]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skbio.stats.distance import mantel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test geographic distance\n",
    "for pheno in shift999.keys():\n",
    "    mant = mantel(shift999[pheno],geodist,permutations=9999)\n",
    "    if mant[1] <= 0.05:\n",
    "        print pheno,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test total environmental distance\n",
    "for pheno in shift999:\n",
    "    mant = mantel(shift999[pheno],envdist,permutations=9999)\n",
    "    if mant[1] <= 0.05:\n",
    "        print pheno,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test individual environmental distances\n",
    "for pheno in shift999.keys():\n",
    "    for env in envdDict.keys():\n",
    "        mant = mantel(shift999[pheno],envdDict[env],permutations=9999)\n",
    "        if mant[1] <= 0.05:\n",
    "            print pheno,env,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get phenotypic distances\n",
    "#get population assignments\n",
    "popDict = OrderedDict()\n",
    "for samp in pheno_data.index:\n",
    "    pop = pheno_data.loc[samp,'Population_ID']\n",
    "    if not pop in popDict.keys():\n",
    "        popDict[pop] = []\n",
    "    popDict[pop].append(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get phenotypic distances\n",
    "#get average phenotypic score for each pop\n",
    "popavg = OrderedDict()\n",
    "for pheno in sorted(pheno_data.columns[1:]):\n",
    "    phen = \"%sx\" % \"\".join(pheno.split(\"_\"))\n",
    "    popavg[phen] = OrderedDict()\n",
    "    for pop in popDict.keys():\n",
    "        data = pheno_data[pheno_data.index.isin(popDict[pop])]\n",
    "        sums = sum([val for val in data[pheno].tolist() if not math.isnan(val)])\n",
    "        \n",
    "        popavg[phen][pop] = sums/len(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get phenotypic distances\n",
    "#get standardized phenotypic values\n",
    "popstd = OrderedDict()\n",
    "for pheno in popavg.keys():\n",
    "    popstd[pheno] = OrderedDict()\n",
    "    mean = np.mean(popavg[pheno].values())\n",
    "    std  = np.std(popavg[pheno].values())\n",
    "    for pop in popavg[pheno].keys():\n",
    "        popstd[pheno][pop] = ((popavg[pheno][pop] - mean)/std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get phenotypic distances\n",
    "#get pairwise phenotypic distances\n",
    "phendist = OrderedDict()\n",
    "for pheno in popstd.keys():\n",
    "    phendist[pheno] = OrderedDict()\n",
    "    for i,popi in enumerate(popstd[pheno].keys()):\n",
    "        phendist[pheno][popi] = OrderedDict()\n",
    "        for i,popj in enumerate(popstd[pheno].keys()):\n",
    "            if popi == popj:\n",
    "                phendist[pheno][popi][popj] = 0\n",
    "            else:\n",
    "                phendist[pheno][popi][popj] = abs(popstd[pheno][popi]-popstd[pheno][popj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('/home/lindb/wbp/gemma/distance_matrices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get phenotypic distances\n",
    "#write out pairwise phenotypic distance matrices\n",
    "for pheno in phendist.keys():\n",
    "    filE = '/home/lindb/wbp/gemma/distance_matrices/%s_phenotypic_distances.txt' % pheno\n",
    "    with open(filE,'w') as o:\n",
    "        text = '\\t'.join(phendist[pheno].keys())+'\\n'\n",
    "        o.write(\"%s\" % text)\n",
    "        for pop in phendist[pheno].keys():\n",
    "            text = str(pop)+'\\t'+'\\t'.join([str(x) for x in phendist[pheno][pop].values()])+'\\n'\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(filE,header=0,index_col=0,sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phendist[pheno]['Dicks_Pass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get phenotypic distances\n",
    "#get pairwise phenotypic distance matrices\n",
    "phendf = OrderedDict()\n",
    "DIR = '/home/lindb/wbp/gemma/distance_matrices/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR)]\n",
    "for f in files:\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    phendf[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phendf[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test against phenotypic distances\n",
    "for i,phenoi in enumerate(shift999.keys()):\n",
    "    for j,phenoj in enumerate(phendf.keys()):\n",
    "        if i>j:\n",
    "            mant = mantel(shift999[phenoi],phendf[phenoj],permutations=9999)\n",
    "            if mant[1] <= 0.05:\n",
    "                print phenoi,phenoj,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test phenotypic distance vs environmental distance\n",
    "for pheno in phendf.keys():\n",
    "    for env in envdDict.keys():\n",
    "        mant = mantel(phendf[pheno],envdDict[env],permutations = 9999)\n",
    "        if mant[1] <= 0.05:\n",
    "            print pheno,env,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mantel(phendf['c13popx'],envdDict['Ann-ppt'],permutations=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#phenotypic distance vs phenotypic distance\n",
    "for i,phenoi in enumerate(phendf):\n",
    "    for j,phenoj in enumerate(phendf):\n",
    "        if i > j:\n",
    "            mant = mantel(phendf[phenoi],phendf[phenoj],permutations=9999)\n",
    "            if mant[1] <= 0.05:\n",
    "                print phenoi,phenoj,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#phenotypic distance vs geographic distance\n",
    "for pheno in phendf:\n",
    "    mant = mantel(phendf[pheno],geodist,permutations=9999)\n",
    "    if mant[1] <= 0.05:\n",
    "        print pheno,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pDij vs pDij\n",
    "for i,phenoi in enumerate(shift999):\n",
    "    for j,phenoj in enumerate(shift999):\n",
    "        if i>j:\n",
    "            mant = mantel(shift999[phenoi],shift999[phenoj],permutations=9999)\n",
    "            if mant[1] <= 0.05:\n",
    "                print phenoi,phenoj,mant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 998 pips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in pip998:\n",
    "    print pheno,len(pip998[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impMAF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mafDict['NODE_1000013_length_91_cov_1.802198_37']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise pops for empirical SNPs and calculate median Dij for each pop pair\n",
    "shiftDict = OrderedDict() \n",
    "for pheno in pip998.keys():\n",
    "    print pheno\n",
    "    kcount = 0\n",
    "    shiftDict[pheno] = OrderedDict()\n",
    "    outliersnps = pip998[pheno]\n",
    "    for m,popm in enumerate(impMAF.columns):\n",
    "        print popm\n",
    "        shiftDict[pheno][popm] = OrderedDict()\n",
    "        for l,popl in enumerate(impMAF.columns):\n",
    "            if m>l: #only need to do the lower triangle\n",
    "                dijlist = []\n",
    "                for i,locusi in enumerate(outliersnps):\n",
    "                    for j,locusj in enumerate(outliersnps):\n",
    "                        if i > j: #i=row, j=col : lower triangle \n",
    "                            sums =0\n",
    "                            kcount += 1\n",
    "                            for popk in [popm,popl]:\n",
    "                                qik = impMAF.loc[locusi,popk]        #get locusi maf for pop k\n",
    "                                qjk = impMAF.loc[locusj,popk]        #get locusj maf for pop k\n",
    "                                nk = popsizeDict[popk]                   #N  individuals  in pop k\n",
    "\n",
    "                                globN = 2*(popsizeDict[popm]+popsizeDict[popl]) # number of alleles across 2 pops\n",
    "\n",
    "                                #get global mafs\n",
    "                                fqim = impMAF.loc[locusi,popm]        #minor allele locusi freq  in popm\n",
    "                                nqim = round(fqim*2*popsizeDict[popm])    #minor allele locusi count in popm\n",
    "                                fqil = impMAF.loc[locusi,popl]        #minor allele locusi freq  in popl\n",
    "                                nqil = round(fqil*2*popsizeDict[popl])    #minor allele locusi count in popl\n",
    "\n",
    "                                fqjm = impMAF.loc[locusj,popm]        #minor allele locusj freq  in popm\n",
    "                                nqjm = round(fqjm*2*popsizeDict[popm])    #minor allele locusj count in popm\n",
    "                                fqjl = impMAF.loc[locusj,popl]        #minor allele locusj freq  in popl\n",
    "                                nqjl = round(fqjl*2*popsizeDict[popl])    #minor allele locusj count in popl\n",
    "\n",
    "                                qi = (nqim+nqil)/((2*popsizeDict[popm])+(2*popsizeDict[popl])) #pairwise global maf locusi\n",
    "                                qj = (nqjm+nqjl)/((2*popsizeDict[popm])+(2*popsizeDict[popl])) #pariwise global maf locusj\n",
    "\n",
    "                                sums += (nk/(popsizeDict[popm]+popsizeDict[popl]))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                            dijlist.append(sums) #each pairwise pop comparison has a matrix of Dij\n",
    "                        else:\n",
    "                            pass #no redundancies, no diagonal. \n",
    "                shiftDict[pheno][popm][popl] = np.median([abs(d) for d in dijlist])\n",
    "            else:\n",
    "                shiftDict[pheno][popm][popl] = np.nan #no redundancies,no diagonal. will be faster to reflect across diag later\n",
    "    #    if kcount > 1:\n",
    "    #        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write out the dfs\n",
    "for pheno in shiftDict.keys():\n",
    "    filE = '/home/lindb/wbp/gemma/freqshifts/%s_pop_pairwise_dij_pip998.text' % pheno\n",
    "    with open(filE,'w') as o:\n",
    "        key0 = shiftDict[pheno].keys()[0]\n",
    "        line = '\\t'.join(shiftDict[pheno][key0].keys()) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "        for popk in shiftDict[pheno].keys():\n",
    "            text = str(popk)+'\\t'+'\\t'.join([str(d) for d in shiftDict[pheno][popk].values()])+'\\n'\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in the DFs\n",
    "shiftDF = OrderedDict()\n",
    "DIR = '/home/lindb/wbp/gemma/freqshifts/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'pip998' in f]\n",
    "for f in files:\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    shiftDF[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "shiftDF[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make DFs symmetrical\n",
    "for pheno in shiftDF.keys():\n",
    "    for i,popi in enumerate(shiftDF[pheno].index):\n",
    "        for j,popj in enumerate(shiftDF[pheno].columns):\n",
    "            if i == j:\n",
    "                shiftDF[pheno].loc[popi,popj] =0\n",
    "            elif math.isnan(shiftDF[pheno].loc[popi,popj]) == True:\n",
    "                shiftDF[pheno].loc[popi,popj] = shiftDF[pheno].loc[popj,popi]\n",
    "shiftDF[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#redo\n",
    "shiftpip998 = OrderedDict(shiftDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geodist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envdist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(envdDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test geographic distance\n",
    "for pheno in shiftpip998.keys():\n",
    "    mant = mantel(shiftpip998[pheno],geodist,permutations=9999)\n",
    "    if mant[1] <= 0.05:\n",
    "        print pheno,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test total environmental distance\n",
    "for pheno in shiftpip998.keys():\n",
    "    mant = mantel(shiftpip998[pheno],envdist,permutations=9999)\n",
    "    if mant[1] <= 0.05:\n",
    "        print pheno,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "godzilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shiftpip998[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for env in envdDict:\n",
    "    df = envdDict[env]\n",
    "    envdDict[env] = df.loc[[row for row in shiftpip998[pheno].columns],[col for col in df.columns if col in shiftpip998[pheno].columns]]\n",
    "envdDict[env]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test individual environmental distances\n",
    "for pheno in shiftpip998.keys():\n",
    "    for env in envdDict.keys():\n",
    "        mant = mantel(shiftpip998[pheno],envdDict[env],permutations=9999)\n",
    "        if mant[1] <= 0.05:\n",
    "            print pheno,env,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test against other pDij matrices\n",
    "for i,phenoi in enumerate(shiftpip998):\n",
    "    for j,phenoj in enumerate(shiftpip998):\n",
    "        if i>j:\n",
    "            mant = mantel(shiftpip998[phenoi],shiftpip998[phenoj],permutations=9999)\n",
    "            if mant[1] <= 0.05:\n",
    "                print phenoi,phenoj,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test against phenotypic distances\n",
    "for i,phenoi in enumerate(shiftpip998.keys()):\n",
    "    for j,phenoj in enumerate(phendf.keys()):\n",
    "        if i>j:\n",
    "            mant = mantel(shiftpip998[phenoi],phendf[phenoj],permutations=9999)\n",
    "            if mant[1] <= 0.05:\n",
    "                print phenoi,phenoj,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,phenoi in enumerate(phendf):\n",
    "    for j,phenoj in enumerate(phendf):\n",
    "        if i>j:\n",
    "            mant = mantel(phendf[phenoi],phendf[phenoj],permutations=9999)\n",
    "            if mant[1] <= 0.05:\n",
    "                print phenoi,phenoj,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alpha snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impMAF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "globs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glob['NODE_1000013_length_91_cov_1.802198_37']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise pops for empirical SNPs and calculate median Dij for each pop pair\n",
    "shiftDict = OrderedDict() \n",
    "for pheno in alphasnps999.keys():\n",
    "    print pheno\n",
    "    kcount = 0\n",
    "    shiftDict[pheno] = OrderedDict()\n",
    "    outliersnps = alphasnps999[pheno]\n",
    "    for m,popm in enumerate(impMAF.columns):\n",
    "        print popm\n",
    "        shiftDict[pheno][popm] = OrderedDict()\n",
    "        for l,popl in enumerate(impMAF.columns):\n",
    "            if m>l: #only need to do the lower triangle\n",
    "                dijlist = []\n",
    "                for i,locusi in enumerate(outliersnps):\n",
    "                    for j,locusj in enumerate(outliersnps):\n",
    "                        if i > j: #i=row, j=col : lower triangle \n",
    "                            sums =0\n",
    "                            kcount += 1\n",
    "                            for popk in [popm,popl]:\n",
    "                                qik = impMAF.loc[locusi,popk]        #get locusi maf for pop k\n",
    "                                qjk = impMAF.loc[locusj,popk]        #get locusj maf for pop k\n",
    "                                nk = popsizeDict[popk]                   #N  individuals  in pop k\n",
    "\n",
    "                                globN = 2*(popsizeDict[popm]+popsizeDict[popl]) # number of alleles across 2 pops\n",
    "\n",
    "                                #get global mafs\n",
    "                                fqim = impMAF.loc[locusi,popm]        #minor allele locusi freq  in popm\n",
    "                                nqim = round(fqim*2*popsizeDict[popm])    #minor allele locusi count in popm\n",
    "                                fqil = impMAF.loc[locusi,popl]        #minor allele locusi freq  in popl\n",
    "                                nqil = round(fqil*2*popsizeDict[popl])    #minor allele locusi count in popl\n",
    "\n",
    "                                fqjm = impMAF.loc[locusj,popm]        #minor allele locusj freq  in popm\n",
    "                                nqjm = round(fqjm*2*popsizeDict[popm])    #minor allele locusj count in popm\n",
    "                                fqjl = impMAF.loc[locusj,popl]        #minor allele locusj freq  in popl\n",
    "                                nqjl = round(fqjl*2*popsizeDict[popl])    #minor allele locusj count in popl\n",
    "\n",
    "                                qi = (nqim+nqil)/((2*popsizeDict[popm])+(2*popsizeDict[popl])) #pairwise global maf locusi\n",
    "                                qj = (nqjm+nqjl)/((2*popsizeDict[popm])+(2*popsizeDict[popl])) #pariwise global maf locusj\n",
    "\n",
    "                                sums += (nk/(popsizeDict[popm]+popsizeDict[popl]))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                            dijlist.append(sums) #each pairwise pop comparison has a matrix of Dij\n",
    "                        else:\n",
    "                            pass #no redundancies, no diagonal. \n",
    "                shiftDict[pheno][popm][popl] = np.median([abs(d) for d in dijlist])\n",
    "            else:\n",
    "                shiftDict[pheno][popm][popl] = np.nan #no redundancies,no diagonal. will be faster to reflect across diag later\n",
    "    #    if kcount > 1:\n",
    "    #        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write out the dfs\n",
    "for pheno in shiftDict.keys():\n",
    "    filE = '/home/lindb/wbp/gemma/freqshifts/%s_pop_pairwise_dij_alphasnps999.text' % pheno\n",
    "    with open(filE,'w') as o:\n",
    "        key0 = shiftDict[pheno].keys()[0]\n",
    "        line = '\\t'.join(shiftDict[pheno][key0].keys()) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "        for popk in shiftDict[pheno].keys():\n",
    "            text = str(popk)+'\\t'+'\\t'.join([str(d) for d in shiftDict[pheno][popk].values()])+'\\n'\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in the DFs\n",
    "shiftDF = OrderedDict()\n",
    "DIR = '/home/lindb/wbp/gemma/freqshifts/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'alphasnps' in f]\n",
    "for f in files:\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    shiftDF[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "shiftDF[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make DFs symmetrical\n",
    "for pheno in shiftDF.keys():\n",
    "    for i,popi in enumerate(shiftDF[pheno].index):\n",
    "        for j,popj in enumerate(shiftDF[pheno].columns):\n",
    "            if i == j:\n",
    "                shiftDF[pheno].loc[popi,popj] =0\n",
    "            elif math.isnan(shiftDF[pheno].loc[popi,popj]) == True:\n",
    "                shiftDF[pheno].loc[popi,popj] = shiftDF[pheno].loc[popj,popi]\n",
    "shiftDF[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shiftalphas = OrderedDict(shiftDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geodist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(envdDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test geographic distance\n",
    "for pheno in shiftalphas.keys():\n",
    "    mant = mantel(shiftalphas[pheno],geodist,permutations=9999)\n",
    "    if mant[1] <= 0.05:\n",
    "        print pheno,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test total environmental distance\n",
    "for pheno in shiftalphas.keys():\n",
    "    mant = mantel(shiftalphas[pheno],envdist,permutations=9999)\n",
    "    if mant[1] <= 0.05:\n",
    "        print pheno,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test individual environmental distances\n",
    "for pheno in shiftalphas.keys():\n",
    "    for env in envdDict.keys():\n",
    "        mant = mantel(shiftalphas[pheno],envdDict[env],permutations=9999)\n",
    "        if mant[1] <= 0.05:\n",
    "            print pheno,env,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test against other pDij matrices\n",
    "for i,phenoi in enumerate(shiftalphas):\n",
    "    for j,phenoj in enumerate(shiftalphas):\n",
    "        if i>j:\n",
    "            mant = mantel(shiftalphas[phenoi],shiftalphas[phenoj],permutations=9999)\n",
    "            if mant[1] <= 0.05:\n",
    "                print phenoi,phenoj,mant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top PIPs (gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pheno in toppips:\n",
    "    print pheno,len(toppips[pheno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do pairwise pops for empirical SNPs and calculate median Dij for each pop pair\n",
    "shiftDict = OrderedDict() \n",
    "for pheno in toppips.keys():\n",
    "    print pheno\n",
    "    kcount = 0\n",
    "    shiftDict[pheno] = OrderedDict()\n",
    "    outliersnps = toppips[pheno]\n",
    "    for m,popm in enumerate(impMAF.columns):\n",
    "        print popm\n",
    "        shiftDict[pheno][popm] = OrderedDict()\n",
    "        for l,popl in enumerate(impMAF.columns):\n",
    "            if m>l: #only need to do the lower triangle\n",
    "                dijlist = []\n",
    "                for i,locusi in enumerate(outliersnps):\n",
    "                    for j,locusj in enumerate(outliersnps):\n",
    "                        if i > j: #i=row, j=col : lower triangle \n",
    "                            sums =0\n",
    "                            kcount += 1\n",
    "                            for popk in [popm,popl]:\n",
    "                                qik = impMAF.loc[locusi,popk]        #get locusi maf for pop k\n",
    "                                qjk = impMAF.loc[locusj,popk]        #get locusj maf for pop k\n",
    "                                nk = popsizeDict[popk]                   #N  individuals  in pop k\n",
    "\n",
    "                                globN = 2*(popsizeDict[popm]+popsizeDict[popl]) # number of alleles across 2 pops\n",
    "\n",
    "                                #get global mafs\n",
    "                                fqim = impMAF.loc[locusi,popm]        #minor allele locusi freq  in popm\n",
    "                                nqim = round(fqim*2*popsizeDict[popm])    #minor allele locusi count in popm\n",
    "                                fqil = impMAF.loc[locusi,popl]        #minor allele locusi freq  in popl\n",
    "                                nqil = round(fqil*2*popsizeDict[popl])    #minor allele locusi count in popl\n",
    "\n",
    "                                fqjm = impMAF.loc[locusj,popm]        #minor allele locusj freq  in popm\n",
    "                                nqjm = round(fqjm*2*popsizeDict[popm])    #minor allele locusj count in popm\n",
    "                                fqjl = impMAF.loc[locusj,popl]        #minor allele locusj freq  in popl\n",
    "                                nqjl = round(fqjl*2*popsizeDict[popl])    #minor allele locusj count in popl\n",
    "\n",
    "                                qi = (nqim+nqil)/((2*popsizeDict[popm])+(2*popsizeDict[popl])) #pairwise global maf locusi\n",
    "                                qj = (nqjm+nqjl)/((2*popsizeDict[popm])+(2*popsizeDict[popl])) #pariwise global maf locusj\n",
    "\n",
    "                                sums += (nk/(popsizeDict[popm]+popsizeDict[popl]))*((qik*qjk)-(qi*qj))\n",
    "\n",
    "                            dijlist.append(sums) #each pairwise pop comparison has a matrix of Dij\n",
    "                        else:\n",
    "                            pass #no redundancies, no diagonal. \n",
    "                shiftDict[pheno][popm][popl] = np.median([abs(d) for d in dijlist])\n",
    "            else:\n",
    "                shiftDict[pheno][popm][popl] = np.nan #no redundancies,no diagonal. will be faster to reflect across diag later\n",
    "    #    if kcount > 1:\n",
    "    #        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#waiting for the above to finish!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write out the dfs\n",
    "for pheno in shiftDict.keys():\n",
    "    filE = '/home/lindb/wbp/gemma/freqshifts/%s_pop_pairwise_dij_top_PIPs.text' % pheno\n",
    "    with open(filE,'w') as o:\n",
    "        key0 = shiftDict[pheno].keys()[0]\n",
    "        line = '\\t'.join(shiftDict[pheno][key0].keys()) + str('\\n')\n",
    "        o.write(\"%s\" % line)\n",
    "        for popk in shiftDict[pheno].keys():\n",
    "            text = str(popk)+'\\t'+'\\t'.join([str(d) for d in shiftDict[pheno][popk].values()])+'\\n'\n",
    "            o.write(\"%s\" % text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in the DFs\n",
    "shiftDF = OrderedDict()\n",
    "DIR = '/home/lindb/wbp/gemma/freqshifts/'\n",
    "files = [op.join(DIR,f) for f in ls(DIR) if 'top_PIPs' in f]\n",
    "for f in files:\n",
    "    pheno = op.basename(f).split(\"_\")[0]\n",
    "    shiftDF[pheno] = pd.read_csv(f,header=0,index_col=0,sep='\\t')\n",
    "shiftDF[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make DFs symmetrical\n",
    "for pheno in shiftDF.keys():\n",
    "    for i,popi in enumerate(shiftDF[pheno].index):\n",
    "        for j,popj in enumerate(shiftDF[pheno].columns):\n",
    "            if i == j:\n",
    "                shiftDF[pheno].loc[popi,popj] =0\n",
    "            elif math.isnan(shiftDF[pheno].loc[popi,popj]) == True:\n",
    "                shiftDF[pheno].loc[popi,popj] = shiftDF[pheno].loc[popj,popi]\n",
    "shiftDF[pheno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shiftpips = OrderedDict(shiftDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test geographic distance\n",
    "for pheno in shiftpips.keys():\n",
    "    mant = mantel(shiftpips[pheno],geodist,permutations=9999)\n",
    "    if mant[1] <= 0.05:\n",
    "        print pheno,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test individual environmental distances\n",
    "for pheno in shiftpips.keys():\n",
    "    for env in envdDict.keys():\n",
    "        mant = mantel(shiftpips[pheno],envdDict[env],permutations=9999)\n",
    "        if mant[1] <= 0.05:\n",
    "            print pheno,env,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test against phenotypic distances\n",
    "for i,phenoi in enumerate(shiftpips.keys()):\n",
    "    for j,phenoj in enumerate(phendf.keys()):\n",
    "        if i>j:\n",
    "            mant = mantel(shiftpips[phenoi],phendf[phenoj],permutations=9999)\n",
    "            if mant[1] <= 0.05:\n",
    "                print phenoi,phenoj,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test against other pDij matrices\n",
    "for i,phenoi in enumerate(shiftpips):\n",
    "    for j,phenoj in enumerate(shiftpips):\n",
    "        if i>j:\n",
    "            mant = mantel(shiftpips[phenoi],shiftpips[phenoj],permutations=9999)\n",
    "            if mant[1] <= 0.05:\n",
    "                print phenoi,phenoj,mant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "envdDict['Ann-ppt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
